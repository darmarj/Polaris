{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"DevOps/Container/cgroups/","text":"Abstract \u00b6 A control group (cgroup) is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, and so on) of a collection of processes. Cgroups provide the following features: Resource limits \u2013 You can configure a cgroup to limit how much of a particular resource (memory or CPU, for example) a process can use. Prioritization \u2013 You can control how much of a resource (CPU, disk, or network) a process can use compared to processes in another cgroup when there is resource contention. Accounting \u2013 Resource limits are monitored and reported at the cgroup level. Control \u2013 You can change the status (frozen, stopped, or restarted) of all processes in a cgroup with a single command. Basically, you use cgroups to control how much of a given key resource (CPU, memory, network, and disk I/O) can be accessed or used by a process or set of processes. Cgroups are a key component of containers because there are often multiple processes running in a container that you need to control together. In a Kubernetes environment, cgroups can be used to implement resource requests and limits and corresponding QoS classes at the pod level. The following diagram illustrates how when you allocate a particular percentage of available system resources to a cgroup (in this case cgroup\u20111), the remaining percentage is available to other cgroups (and individual processes) on the system. Cgroup Versions \u00b6 According to Wikipedia, the first version of cgroups was merged into the Linux kernel mainline in late 2007 or early 2008, and \u201cthe documentation of cgroups\u2011v2 first appeared in [the] Linux kernel \u2026 [in] 2016\u201d. Among the many changes in version 2, the big ones are a much simplified tree architecture, new features and interfaces in the cgroup hierarchy, and better accommodation of \u201crootless\u201d containers (with non\u2011zero UIDs). The new interface in v2 is for pressure stall information (PSI) . It provides insight into per\u2011process memory use and allocation in a much more granular way than was previously possible (this is beyond the scope of this blog, but is a very cool topic). Conclusion \u00b6 Namespaces and cgroups are the building blocks for containers and modern applications. Having an understanding of how they work is important as we refactor applications to more modern architectures. Namespaces provide isolation of system resources, and cgroups allow for fine\u2011grained control and enforcement of limits for those resources. Containers are not the only way that you can use namespaces and cgroups. Namespaces and cgroup interfaces are built into the Linux kernel, which means that other applications can use them to provide separation and resource constraints.","title":"Cgroups"},{"location":"DevOps/Container/cgroups/#abstract","text":"A control group (cgroup) is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, and so on) of a collection of processes. Cgroups provide the following features: Resource limits \u2013 You can configure a cgroup to limit how much of a particular resource (memory or CPU, for example) a process can use. Prioritization \u2013 You can control how much of a resource (CPU, disk, or network) a process can use compared to processes in another cgroup when there is resource contention. Accounting \u2013 Resource limits are monitored and reported at the cgroup level. Control \u2013 You can change the status (frozen, stopped, or restarted) of all processes in a cgroup with a single command. Basically, you use cgroups to control how much of a given key resource (CPU, memory, network, and disk I/O) can be accessed or used by a process or set of processes. Cgroups are a key component of containers because there are often multiple processes running in a container that you need to control together. In a Kubernetes environment, cgroups can be used to implement resource requests and limits and corresponding QoS classes at the pod level. The following diagram illustrates how when you allocate a particular percentage of available system resources to a cgroup (in this case cgroup\u20111), the remaining percentage is available to other cgroups (and individual processes) on the system.","title":"Abstract"},{"location":"DevOps/Container/cgroups/#cgroup-versions","text":"According to Wikipedia, the first version of cgroups was merged into the Linux kernel mainline in late 2007 or early 2008, and \u201cthe documentation of cgroups\u2011v2 first appeared in [the] Linux kernel \u2026 [in] 2016\u201d. Among the many changes in version 2, the big ones are a much simplified tree architecture, new features and interfaces in the cgroup hierarchy, and better accommodation of \u201crootless\u201d containers (with non\u2011zero UIDs). The new interface in v2 is for pressure stall information (PSI) . It provides insight into per\u2011process memory use and allocation in a much more granular way than was previously possible (this is beyond the scope of this blog, but is a very cool topic).","title":"Cgroup Versions"},{"location":"DevOps/Container/cgroups/#conclusion","text":"Namespaces and cgroups are the building blocks for containers and modern applications. Having an understanding of how they work is important as we refactor applications to more modern architectures. Namespaces provide isolation of system resources, and cgroups allow for fine\u2011grained control and enforcement of limits for those resources. Containers are not the only way that you can use namespaces and cgroups. Namespaces and cgroup interfaces are built into the Linux kernel, which means that other applications can use them to provide separation and resource constraints.","title":"Conclusion"},{"location":"DevOps/Container/containerd/","text":"Abstract \u00b6 Containerd \u2013 An abstraction of kernel features that provides a relatively high level container interface. Other software projects can use this to run containers and manage container images. Ecosystem Container Runtime Interface (CRI) \u00b6 CRI is the API that Kubernetes uses to control the different runtimes that create and manage containers. CRI makes it easier for Kubernetes to use different container runtimes. Instead of the Kubernetes project having to manually add support for each runtime, the CRI API describes how Kubernetes interacts with each runtime. So it\u2019s up to the runtime how to actually manage containers, as long as it obeys the CRI API. So if you prefer to use containerd to run your containers, you can. Or, if you prefer to use CRI-O , then you can. This is because both of these runtimes implement the CRI spec. If you\u2019re an end user, the implementation mostly shouldn\u2019t matter. These CRI implementations are intended to be pluggable and seamlessly changeable. Your choice of runtime might be important if you pay to get support (security, bug fixes etc) from a vendor. For example, Red Hat\u2019s OpenShift uses CRI-O , and offers support for it. Docker provides support for their own containerd . How to check your container runtime in Kubernetes? In Kubernetes architecture, the kubelet (the agent that runs on each node) is responsible for sending instructions to the container runtime to start and run containers. You can check which container runtime you\u2019re using by looking at the kubelet parameters on each node. There\u2019s an option --container-runtime and --container-runtime-endpoint which are used to configure which runtime to use. Containerd \u00b6 containerd is a high-level container runtime that came from Docker, and implements the CRI spec. It pulls images from registries, manages them and then hands over to a lower-level runtime, which actually creates and runs the container processes. containerd was separated out of the Docker project, to make Docker more modular. So Docker uses containerd internally itself. When you install Docker, it will also install containerd. containerd implements the Kubernetes Container Runtime Interface (CRI), via its cri plugin. CRI-O \u00b6 CRI-O is another high-level container runtime which implements the Container Runtime Interface (CRI). It\u2019s an alternative to containerd. It pulls container images from registries, manages them on disk, and launches a lower-level runtime to run container processes. Yes, CRI-O is another container runtime. It was born out of Red Hat, IBM, Intel, SUSE and others. It was specifically created from the ground up as a container runtime for Kubernetes. It provides the ability to start, stop and restart containers, just like containerd. Open Container Initiative (OCI) \u00b6 The OCI is a group of tech companies who maintain a specification for the container image format, and how containers should be run. The idea behind the OCI is that you can choose between different runtimes which conform to the spec. Each of these runtimes have different lower-level implementations. For example, you might have one OCI-compliant runtime for your Linux hosts, and one for your Windows hosts. This is the benefit of having one standard that can be implemented by many different projects. This same \u201cone standard, many implementations\u201d approach is in use everywhere, from Bluetooth devices to Java APIs. runc \u00b6 runc is an OCI-compatible container runtime. It implements the OCI specification and runs the container processes. runc is called the reference implementation of OCI. What is a reference implementation? A reference implementation is a piece of software that has implemented all the requirements of a specification or standard. It\u2019s usually the first piece of software which is developed from the specification. In the case of OCI, runc provides all the features expected of an OCI-compliant runtime, although anyone can implement their own OCI runtime if they like. runc provides all of the low-level functionality for containers, interacting with existing low-level Linux features, like namespaces and control groups. It uses these features to create and run container processes. A couple of alternatives to runc are: crun a container runtime written in C (by contrast, runc is written in Go.) kata-runtime from the Katacontainers project, which implements the OCI specification as individual lightweight VMs (hardware virtualisation gVisor from Google, which creates containers that have their own kernel. It implements OCI in its runtime called runsc. What's the equivalent of runc on Windows? runc is a tool for running containers on Linux. So that means it runs on Linux, on bare metal or inside a VM. On Windows, it\u2019s slightly different. The equivalent of runc is Microsoft\u2019s Host Compute Service (HCS). It includes a tool called runhcs, which itself is a fork of runc, and also implements the Open Container Initiative specification.","title":"Containerd"},{"location":"DevOps/Container/containerd/#abstract","text":"Containerd \u2013 An abstraction of kernel features that provides a relatively high level container interface. Other software projects can use this to run containers and manage container images. Ecosystem","title":"Abstract"},{"location":"DevOps/Container/containerd/#container-runtime-interface-cri","text":"CRI is the API that Kubernetes uses to control the different runtimes that create and manage containers. CRI makes it easier for Kubernetes to use different container runtimes. Instead of the Kubernetes project having to manually add support for each runtime, the CRI API describes how Kubernetes interacts with each runtime. So it\u2019s up to the runtime how to actually manage containers, as long as it obeys the CRI API. So if you prefer to use containerd to run your containers, you can. Or, if you prefer to use CRI-O , then you can. This is because both of these runtimes implement the CRI spec. If you\u2019re an end user, the implementation mostly shouldn\u2019t matter. These CRI implementations are intended to be pluggable and seamlessly changeable. Your choice of runtime might be important if you pay to get support (security, bug fixes etc) from a vendor. For example, Red Hat\u2019s OpenShift uses CRI-O , and offers support for it. Docker provides support for their own containerd . How to check your container runtime in Kubernetes? In Kubernetes architecture, the kubelet (the agent that runs on each node) is responsible for sending instructions to the container runtime to start and run containers. You can check which container runtime you\u2019re using by looking at the kubelet parameters on each node. There\u2019s an option --container-runtime and --container-runtime-endpoint which are used to configure which runtime to use.","title":"Container Runtime Interface (CRI)"},{"location":"DevOps/Container/containerd/#containerd","text":"containerd is a high-level container runtime that came from Docker, and implements the CRI spec. It pulls images from registries, manages them and then hands over to a lower-level runtime, which actually creates and runs the container processes. containerd was separated out of the Docker project, to make Docker more modular. So Docker uses containerd internally itself. When you install Docker, it will also install containerd. containerd implements the Kubernetes Container Runtime Interface (CRI), via its cri plugin.","title":"Containerd"},{"location":"DevOps/Container/containerd/#cri-o","text":"CRI-O is another high-level container runtime which implements the Container Runtime Interface (CRI). It\u2019s an alternative to containerd. It pulls container images from registries, manages them on disk, and launches a lower-level runtime to run container processes. Yes, CRI-O is another container runtime. It was born out of Red Hat, IBM, Intel, SUSE and others. It was specifically created from the ground up as a container runtime for Kubernetes. It provides the ability to start, stop and restart containers, just like containerd.","title":"CRI-O"},{"location":"DevOps/Container/containerd/#open-container-initiative-oci","text":"The OCI is a group of tech companies who maintain a specification for the container image format, and how containers should be run. The idea behind the OCI is that you can choose between different runtimes which conform to the spec. Each of these runtimes have different lower-level implementations. For example, you might have one OCI-compliant runtime for your Linux hosts, and one for your Windows hosts. This is the benefit of having one standard that can be implemented by many different projects. This same \u201cone standard, many implementations\u201d approach is in use everywhere, from Bluetooth devices to Java APIs.","title":"Open Container Initiative (OCI)"},{"location":"DevOps/Container/containerd/#runc","text":"runc is an OCI-compatible container runtime. It implements the OCI specification and runs the container processes. runc is called the reference implementation of OCI. What is a reference implementation? A reference implementation is a piece of software that has implemented all the requirements of a specification or standard. It\u2019s usually the first piece of software which is developed from the specification. In the case of OCI, runc provides all the features expected of an OCI-compliant runtime, although anyone can implement their own OCI runtime if they like. runc provides all of the low-level functionality for containers, interacting with existing low-level Linux features, like namespaces and control groups. It uses these features to create and run container processes. A couple of alternatives to runc are: crun a container runtime written in C (by contrast, runc is written in Go.) kata-runtime from the Katacontainers project, which implements the OCI specification as individual lightweight VMs (hardware virtualisation gVisor from Google, which creates containers that have their own kernel. It implements OCI in its runtime called runsc. What's the equivalent of runc on Windows? runc is a tool for running containers on Linux. So that means it runs on Linux, on bare metal or inside a VM. On Windows, it\u2019s slightly different. The equivalent of runc is Microsoft\u2019s Host Compute Service (HCS). It includes a tool called runhcs, which itself is a fork of runc, and also implements the Open Container Initiative specification.","title":"runc"},{"location":"DevOps/Container/namespace/","text":"Abstract \u00b6 Namespaces are a feature of the Linux kernel that partitions kernel resources such that one set of processes sees one set of resources while another set of processes sees a different set of resources. Types of Namespaces \u00b6 Within the Linux kernel, there are different types of namespaces. Each namespace has its own unique properties: user namespace has its own set of user IDs and group IDs for assignment to processes. In particular, this means that a process can have root privilege within its user namespace without having it in other user namespaces. process ID (PID) namespace assigns a set of PIDs to processes that are independent from the set of PIDs in other namespaces. The first process created in a new namespace has PID 1 and child processes are assigned subsequent PIDs. If a child process is created with its own PID namespace, it has PID 1 in that namespace as well as its PID in the parent process\u2019 namespace. See below for an example. network namespace has an independent network stack: its own private routing table, set of IP addresses, socket listing, connection tracking table, firewall, and other network\u2011related resources. mount namespace has an independent list of mount points seen by the processes in the namespace. This means that you can mount and unmount filesystems in a mount namespace without affecting the host filesystem. An interprocess communication (IPC) namespace has its own IPC resources, for example POSIX message queues. A UNIX Time\u2011Sharing (UTS) namespace allows a single system to appear to have different host and domain names to different processes. An Example of Parent and Child PID Namespaces \u00b6 In the diagram above, there are three PID namespaces \u2013 a parent namespace and two child namespaces. Within the parent namespace, there are four processes, named PID1 through PID4. These are normal processes which can all see each other and share resources. The child processes with PID2 and PID3 in the parent namespace also belong to their own PID namespaces in which their PID is 1. From within a child namespace, the PID1 process cannot see anything outside. For example, PID1 in both child namespaces cannot see PID4 in the parent namespace. This provides isolation between (in this case) processes within different namespaces. Conclusion \u00b6 Namespaces and cgroups are the building blocks for containers and modern applications. Having an understanding of how they work is important as we refactor applications to more modern architectures. Namespaces provide isolation of system resources, and cgroups allow for fine\u2011grained control and enforcement of limits for those resources. Containers are not the only way that you can use namespaces and cgroups. Namespaces and cgroup interfaces are built into the Linux kernel, which means that other applications can use them to provide separation and resource constraints.","title":"Namespace"},{"location":"DevOps/Container/namespace/#abstract","text":"Namespaces are a feature of the Linux kernel that partitions kernel resources such that one set of processes sees one set of resources while another set of processes sees a different set of resources.","title":"Abstract"},{"location":"DevOps/Container/namespace/#types-of-namespaces","text":"Within the Linux kernel, there are different types of namespaces. Each namespace has its own unique properties: user namespace has its own set of user IDs and group IDs for assignment to processes. In particular, this means that a process can have root privilege within its user namespace without having it in other user namespaces. process ID (PID) namespace assigns a set of PIDs to processes that are independent from the set of PIDs in other namespaces. The first process created in a new namespace has PID 1 and child processes are assigned subsequent PIDs. If a child process is created with its own PID namespace, it has PID 1 in that namespace as well as its PID in the parent process\u2019 namespace. See below for an example. network namespace has an independent network stack: its own private routing table, set of IP addresses, socket listing, connection tracking table, firewall, and other network\u2011related resources. mount namespace has an independent list of mount points seen by the processes in the namespace. This means that you can mount and unmount filesystems in a mount namespace without affecting the host filesystem. An interprocess communication (IPC) namespace has its own IPC resources, for example POSIX message queues. A UNIX Time\u2011Sharing (UTS) namespace allows a single system to appear to have different host and domain names to different processes.","title":"Types of Namespaces"},{"location":"DevOps/Container/namespace/#an-example-of-parent-and-child-pid-namespaces","text":"In the diagram above, there are three PID namespaces \u2013 a parent namespace and two child namespaces. Within the parent namespace, there are four processes, named PID1 through PID4. These are normal processes which can all see each other and share resources. The child processes with PID2 and PID3 in the parent namespace also belong to their own PID namespaces in which their PID is 1. From within a child namespace, the PID1 process cannot see anything outside. For example, PID1 in both child namespaces cannot see PID4 in the parent namespace. This provides isolation between (in this case) processes within different namespaces.","title":"An Example of Parent and Child PID Namespaces"},{"location":"DevOps/Container/namespace/#conclusion","text":"Namespaces and cgroups are the building blocks for containers and modern applications. Having an understanding of how they work is important as we refactor applications to more modern architectures. Namespaces provide isolation of system resources, and cgroups allow for fine\u2011grained control and enforcement of limits for those resources. Containers are not the only way that you can use namespaces and cgroups. Namespaces and cgroup interfaces are built into the Linux kernel, which means that other applications can use them to provide separation and resource constraints.","title":"Conclusion"},{"location":"DevOps/Container/nerdctl/","text":"Abstract \u00b6 Nerdctl: Docker-compatible CLI for containerd Why another CLI? \u00b6 containerd already has its own CLI called ctr . However, ctr was made only for work very low-level functionality of containerd, and hence its CLI design is not friendly to humans. So we had to create another CLI with high-level functionalities and with human-friendly UI/UX. Notably, ctr lacks the equivalents of the following Docker CLI commands: docker run -p docker run --restart=always docker pull with ~/.docker/config.json and credential helper binaries such as docker-credential-ecr-login docker logs Getting started with nerdctl \u00b6 The latest binary release of nerdctl can be downloaded . Two types of distributions are available: nerdctl- -linux-amd64.tar.gz : nerdctl only. Should be extracted under /usr/local/bin . nerdctl-full- -linux-amd64.tar.gz : nerdctl with dependencies (containerd, runc, CNI, \u2026). Should be extracted under /usr/local . If you already have containerd, you should use the former one. Otherwise the latter one is the best choice. Installation \u00b6 \u279c wget https://download.fastgit.org/containerd/nerdctl/releases/download/v0.12.1/nerdctl-0.12.1-linux-amd64.tar.gz \u279c mkdir -p /usr/local/containerd/bin/ && tar -zxvf nerdctl-0.12.1-linux-amd64.tar.gz nerdctl && mv nerdctl /usr/local/containerd/bin/ \u279c ln -s /usr/local/containerd/bin/nerdctl /usr/local/bin/nerdctl \u279c nerdctl run -d -p 80:80 --name=nginx --restart=always nginx:alpine \u279c nerdctl exec -it nginx /bin/sh / # date Thu Aug 19 06:43:19 UTC 2021 / # Nerdctl \u00b6 nerdctl inspect : Return low-level information on objects. nerdctl tag : Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE \u279c nerdctl images REPOSITORY TAG IMAGE ID CREATED SIZE busybox latest 0f354ec1728d 6 minutes ago 1 .3 MiB nginx alpine bead42240255 41 minutes ago 16 .0 KiB \u279c nerdctl tag nginx:alpine harbor.k8s.local/course/nginx:alpine \u279c nerdctl images REPOSITORY TAG IMAGE ID CREATED SIZE busybox latest 0f354ec1728d 7 minutes ago 1 .3 MiB nginx alpine bead42240255 41 minutes ago 16 .0 KiB harbor.k8s.local/course/nginx alpine bead42240255 2 seconds ago 16 .0 KiB nerdctl save : Save one or more images to a tar archive (streamed to STDOUT by default) \u279c nerdctl save -o busybox.tar.gz busybox:latest \u279c ls -lh busybox.tar.gz -rw-r--r-- 1 root root 761K Aug 19 15 :19 busybox.tar.gz nerdctl load : Load an image from a tar archive or STDIN \u279c nerdctl load -i busybox.tar.gz unpacking docker.io/library/busybox:latest ( sha256:0f354ec1728d9ff32edcd7d1b8bbdfc798277ad36120dc3dc683be44524c8b60 ) ...done nerdctl build : Build an image from a Dockerfile. Needs buildkitd to be running. Tip BuildKit is turns a Dockerfile into a Docker image . And it doesn\u2019t just build Docker images; it can build OCI images and several other output formats . BuildKit is composed of the buildkitd daemon and the buildctl client . While the buildctl client is available for Linux, macOS, and Windows, the buildkitd daemon is only available for Linux currently. The buildkitd daemon requires the following components to be installed: runc or crun containerd (if you want to use containerd worker) Github \u279c nerdctl build -t nginx:nerdctl -f Dockerfile . FATA [ 0000 ] ` buildctl ` needs to be installed and ` buildkitd ` needs to be running, see https://github.com/moby/buildkit: exec: \"buildctl\" : executable file not found in $PATH BuildKit \u279c wget https://github.com/moby/buildkit/releases/download/v0.9.1/buildkit-v0.9.1.linux-amd64.tar.gz # download.fastgit.org for speedup # wget https://download.fastgit.org/moby/buildkit/releases/download/v0.9.1/buildkit-v0.9.1.linux-amd64.tar.gz \u279c tar -zxvf buildkit-v0.9.1.linux-amd64.tar.gz -C /usr/local/containerd/ bin/ bin/buildctl bin/buildkit-qemu-aarch64 bin/buildkit-qemu-arm bin/buildkit-qemu-i386 bin/buildkit-qemu-mips64 bin/buildkit-qemu-mips64el bin/buildkit-qemu-ppc64le bin/buildkit-qemu-riscv64 bin/buildkit-qemu-s390x bin/buildkit-runc bin/buildkitd \u279c ln -s /usr/local/containerd/bin/buildkitd /usr/local/bin/buildkitd \u279c ln -s /usr/local/containerd/bin/buildctl /usr/local/bin/buildctl Systemd \u279c cat /etc/systemd/system/buildkit.service [ Unit ] Description = BuildKit Documentation = https://github.com/moby/buildkit [ Service ] ExecStart = /usr/local/bin/buildkitd --oci-worker = false --containerd-worker = true [ Install ] WantedBy = multi-user.target \u279c systemctl daemon-reload \u279c systemctl enable buildkit --now Created symlink /etc/systemd/system/multi-user.target.wants/buildkit.service \u2192 /etc/systemd/system/buildkit.service. \u279c systemctl status buildkit \u25cf buildkit.service - BuildKit Loaded: loaded ( /etc/systemd/system/buildkit.service ; enabled ; vendor preset: enabled ) Memory: 8 .6M CGroup: /system.slice/buildkit.service \u2514\u25005779 /usr/local/bin/buildkitd --oci-worker = false --containerd-worker = true Aug 19 16 :03:10 ydzsio systemd [ 1 ] : Started BuildKit. Aug 19 16 :03:10 ydzsio buildkitd [ 5779 ] : time = \"2021-08-19T16:03:10+08:00\" level = warning msg = \"using host network as the default\" Aug 19 16 :03:10 ydzsio buildkitd [ 5779 ] : time = \"2021-08-19T16:03:10+08:00\" level = info msg = \"found worker \\\"euznuelxhxb689bc5of7pxmbc\\\", labels> Aug 19 16:03:10 ydzsio buildkitd[5779]: time=\" 2021 -08-19T16:03:10+08:00 \" level=info msg=\" found 1 workers, default = \\\" euznuelxhxb689bc5of7pxm> Aug 19 16 :03:10 ydzsio buildkitd [ 5779 ] : time = \"2021-08-19T16:03:10+08:00\" level = warning msg = \"currently, only the default worker can be used.\" Aug 19 16 :03:10 ydzsio buildkitd [ 5779 ] : time = \"2021-08-19T16:03:10+08:00\" level = info msg = \"running server on /run/buildkit/buildkitd.sock\" ~ Nerdctl Compose \u00b6 nerdctl compose up : Create and start containers nerdctl compose build : Build or rebuild services nerdctl compose config : Validate and view the Compose file","title":"Nerdctl"},{"location":"DevOps/Container/nerdctl/#abstract","text":"Nerdctl: Docker-compatible CLI for containerd","title":"Abstract"},{"location":"DevOps/Container/nerdctl/#why-another-cli","text":"containerd already has its own CLI called ctr . However, ctr was made only for work very low-level functionality of containerd, and hence its CLI design is not friendly to humans. So we had to create another CLI with high-level functionalities and with human-friendly UI/UX. Notably, ctr lacks the equivalents of the following Docker CLI commands: docker run -p docker run --restart=always docker pull with ~/.docker/config.json and credential helper binaries such as docker-credential-ecr-login docker logs","title":"Why another CLI?"},{"location":"DevOps/Container/nerdctl/#getting-started-with-nerdctl","text":"The latest binary release of nerdctl can be downloaded . Two types of distributions are available: nerdctl- -linux-amd64.tar.gz : nerdctl only. Should be extracted under /usr/local/bin . nerdctl-full- -linux-amd64.tar.gz : nerdctl with dependencies (containerd, runc, CNI, \u2026). Should be extracted under /usr/local . If you already have containerd, you should use the former one. Otherwise the latter one is the best choice.","title":"Getting started with nerdctl"},{"location":"DevOps/Container/nerdctl/#installation","text":"\u279c wget https://download.fastgit.org/containerd/nerdctl/releases/download/v0.12.1/nerdctl-0.12.1-linux-amd64.tar.gz \u279c mkdir -p /usr/local/containerd/bin/ && tar -zxvf nerdctl-0.12.1-linux-amd64.tar.gz nerdctl && mv nerdctl /usr/local/containerd/bin/ \u279c ln -s /usr/local/containerd/bin/nerdctl /usr/local/bin/nerdctl \u279c nerdctl run -d -p 80:80 --name=nginx --restart=always nginx:alpine \u279c nerdctl exec -it nginx /bin/sh / # date Thu Aug 19 06:43:19 UTC 2021 / #","title":"Installation"},{"location":"DevOps/Container/nerdctl/#nerdctl","text":"nerdctl inspect : Return low-level information on objects. nerdctl tag : Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE \u279c nerdctl images REPOSITORY TAG IMAGE ID CREATED SIZE busybox latest 0f354ec1728d 6 minutes ago 1 .3 MiB nginx alpine bead42240255 41 minutes ago 16 .0 KiB \u279c nerdctl tag nginx:alpine harbor.k8s.local/course/nginx:alpine \u279c nerdctl images REPOSITORY TAG IMAGE ID CREATED SIZE busybox latest 0f354ec1728d 7 minutes ago 1 .3 MiB nginx alpine bead42240255 41 minutes ago 16 .0 KiB harbor.k8s.local/course/nginx alpine bead42240255 2 seconds ago 16 .0 KiB nerdctl save : Save one or more images to a tar archive (streamed to STDOUT by default) \u279c nerdctl save -o busybox.tar.gz busybox:latest \u279c ls -lh busybox.tar.gz -rw-r--r-- 1 root root 761K Aug 19 15 :19 busybox.tar.gz nerdctl load : Load an image from a tar archive or STDIN \u279c nerdctl load -i busybox.tar.gz unpacking docker.io/library/busybox:latest ( sha256:0f354ec1728d9ff32edcd7d1b8bbdfc798277ad36120dc3dc683be44524c8b60 ) ...done nerdctl build : Build an image from a Dockerfile. Needs buildkitd to be running. Tip BuildKit is turns a Dockerfile into a Docker image . And it doesn\u2019t just build Docker images; it can build OCI images and several other output formats . BuildKit is composed of the buildkitd daemon and the buildctl client . While the buildctl client is available for Linux, macOS, and Windows, the buildkitd daemon is only available for Linux currently. The buildkitd daemon requires the following components to be installed: runc or crun containerd (if you want to use containerd worker) Github \u279c nerdctl build -t nginx:nerdctl -f Dockerfile . FATA [ 0000 ] ` buildctl ` needs to be installed and ` buildkitd ` needs to be running, see https://github.com/moby/buildkit: exec: \"buildctl\" : executable file not found in $PATH BuildKit \u279c wget https://github.com/moby/buildkit/releases/download/v0.9.1/buildkit-v0.9.1.linux-amd64.tar.gz # download.fastgit.org for speedup # wget https://download.fastgit.org/moby/buildkit/releases/download/v0.9.1/buildkit-v0.9.1.linux-amd64.tar.gz \u279c tar -zxvf buildkit-v0.9.1.linux-amd64.tar.gz -C /usr/local/containerd/ bin/ bin/buildctl bin/buildkit-qemu-aarch64 bin/buildkit-qemu-arm bin/buildkit-qemu-i386 bin/buildkit-qemu-mips64 bin/buildkit-qemu-mips64el bin/buildkit-qemu-ppc64le bin/buildkit-qemu-riscv64 bin/buildkit-qemu-s390x bin/buildkit-runc bin/buildkitd \u279c ln -s /usr/local/containerd/bin/buildkitd /usr/local/bin/buildkitd \u279c ln -s /usr/local/containerd/bin/buildctl /usr/local/bin/buildctl Systemd \u279c cat /etc/systemd/system/buildkit.service [ Unit ] Description = BuildKit Documentation = https://github.com/moby/buildkit [ Service ] ExecStart = /usr/local/bin/buildkitd --oci-worker = false --containerd-worker = true [ Install ] WantedBy = multi-user.target \u279c systemctl daemon-reload \u279c systemctl enable buildkit --now Created symlink /etc/systemd/system/multi-user.target.wants/buildkit.service \u2192 /etc/systemd/system/buildkit.service. \u279c systemctl status buildkit \u25cf buildkit.service - BuildKit Loaded: loaded ( /etc/systemd/system/buildkit.service ; enabled ; vendor preset: enabled ) Memory: 8 .6M CGroup: /system.slice/buildkit.service \u2514\u25005779 /usr/local/bin/buildkitd --oci-worker = false --containerd-worker = true Aug 19 16 :03:10 ydzsio systemd [ 1 ] : Started BuildKit. Aug 19 16 :03:10 ydzsio buildkitd [ 5779 ] : time = \"2021-08-19T16:03:10+08:00\" level = warning msg = \"using host network as the default\" Aug 19 16 :03:10 ydzsio buildkitd [ 5779 ] : time = \"2021-08-19T16:03:10+08:00\" level = info msg = \"found worker \\\"euznuelxhxb689bc5of7pxmbc\\\", labels> Aug 19 16:03:10 ydzsio buildkitd[5779]: time=\" 2021 -08-19T16:03:10+08:00 \" level=info msg=\" found 1 workers, default = \\\" euznuelxhxb689bc5of7pxm> Aug 19 16 :03:10 ydzsio buildkitd [ 5779 ] : time = \"2021-08-19T16:03:10+08:00\" level = warning msg = \"currently, only the default worker can be used.\" Aug 19 16 :03:10 ydzsio buildkitd [ 5779 ] : time = \"2021-08-19T16:03:10+08:00\" level = info msg = \"running server on /run/buildkit/buildkitd.sock\" ~","title":"Nerdctl"},{"location":"DevOps/Container/nerdctl/#nerdctl-compose","text":"nerdctl compose up : Create and start containers nerdctl compose build : Build or rebuild services nerdctl compose config : Validate and view the Compose file","title":"Nerdctl Compose"},{"location":"DevOps/Docker/docker/","text":"Abstract \u00b6 Docker \u2013 A developer-oriented software with a high level interface that lets you easily build and run containers from your terminal. It now uses containerd as its container runtime. We have to start with Docker because it\u2019s the most popular developer tool for working with containers. And for a lot of people, the name \u201cDocker\u201d itself is synonymous with the word \u201ccontainer\u201d. Docker kick-started this whole revolution. Docker created a very ergonomic (nice-to-use) tool for working with containers \u2013 also called docker . Container in Docker \u00b6 docker is designed to be installed on a workstation or server and comes with a bunch of tools to make it easy to build and run containers as a developer, or DevOps person. The docker command line tool can build container images, pull them from registries, create, start and manage containers. To make all of this happen, the experience you know as docker is now comprised of these projects (there are others, but these are the main ones): docker-cli: This is the command-line utility that you interact with using docker ... commands. containerd: This is a daemon process that manages and runs containers. It pushes and pulls images, manages storage and networking, and supervises the running of containers. runc: This is the low-level container runtime (the thing that actually creates and runs containers). It includes libcontainer, a native Go-based implementation for creating containers. In reality, when you run a container with docker , you\u2019re actually running it through the Docker daemon, containerd, and then runc. Dockershim: Docker in Kubernetes \u00b6 Kubernetes includes a component called dockershim , which allows it to support Docker. Kubernetes prefers to run containers through any container runtime which supports its Container Runtime Interface (CRI) . But Docker, being older than Kubernetes, doesn\u2019t implement CRI. So that\u2019s why the dockershim exists, to basically bolt Docker onto Kubernetes. Or Kubernetes onto Docker, whichever way round you prefer to think of it. What is a shim? In tech terms, a shim is a component in a software system, which acts as a bridge between different APIs, or as a compatibility layer. A shim is sometimes added when you want to use a third-party component, but you need a little bit of glue code to make it work. Going forward, Kubernetes will remove support for Docker directly, and prefer to use only container runtimes that implement its Container Runtime Interface. This probably means using containerd or CRI-O. But this doesn\u2019t mean that Kubernetes won\u2019t be able to run Docker-formatted containers . Both containerd and CRI-O can run Docker-formatted (actually OCI-formatted) images, they just do it without having to use the docker command or the Docker daemon. Phew. Hope that cleared that up. Docker images \u00b6 What many people refer to as Docker images, are actually images packaged in the Open Container Initiative (OCI) format. So if you pull an image from Docker Hub, or another registry, you should be able to use it with the docker command, or on a Kubernetes cluster, or with the podman utility, or any other tool that supports the OCI image format spec.","title":"Docker"},{"location":"DevOps/Docker/docker/#abstract","text":"Docker \u2013 A developer-oriented software with a high level interface that lets you easily build and run containers from your terminal. It now uses containerd as its container runtime. We have to start with Docker because it\u2019s the most popular developer tool for working with containers. And for a lot of people, the name \u201cDocker\u201d itself is synonymous with the word \u201ccontainer\u201d. Docker kick-started this whole revolution. Docker created a very ergonomic (nice-to-use) tool for working with containers \u2013 also called docker .","title":"Abstract"},{"location":"DevOps/Docker/docker/#container-in-docker","text":"docker is designed to be installed on a workstation or server and comes with a bunch of tools to make it easy to build and run containers as a developer, or DevOps person. The docker command line tool can build container images, pull them from registries, create, start and manage containers. To make all of this happen, the experience you know as docker is now comprised of these projects (there are others, but these are the main ones): docker-cli: This is the command-line utility that you interact with using docker ... commands. containerd: This is a daemon process that manages and runs containers. It pushes and pulls images, manages storage and networking, and supervises the running of containers. runc: This is the low-level container runtime (the thing that actually creates and runs containers). It includes libcontainer, a native Go-based implementation for creating containers. In reality, when you run a container with docker , you\u2019re actually running it through the Docker daemon, containerd, and then runc.","title":"Container in Docker"},{"location":"DevOps/Docker/docker/#dockershim-docker-in-kubernetes","text":"Kubernetes includes a component called dockershim , which allows it to support Docker. Kubernetes prefers to run containers through any container runtime which supports its Container Runtime Interface (CRI) . But Docker, being older than Kubernetes, doesn\u2019t implement CRI. So that\u2019s why the dockershim exists, to basically bolt Docker onto Kubernetes. Or Kubernetes onto Docker, whichever way round you prefer to think of it. What is a shim? In tech terms, a shim is a component in a software system, which acts as a bridge between different APIs, or as a compatibility layer. A shim is sometimes added when you want to use a third-party component, but you need a little bit of glue code to make it work. Going forward, Kubernetes will remove support for Docker directly, and prefer to use only container runtimes that implement its Container Runtime Interface. This probably means using containerd or CRI-O. But this doesn\u2019t mean that Kubernetes won\u2019t be able to run Docker-formatted containers . Both containerd and CRI-O can run Docker-formatted (actually OCI-formatted) images, they just do it without having to use the docker command or the Docker daemon. Phew. Hope that cleared that up.","title":"Dockershim: Docker in Kubernetes"},{"location":"DevOps/Docker/docker/#docker-images","text":"What many people refer to as Docker images, are actually images packaged in the Open Container Initiative (OCI) format. So if you pull an image from Docker Hub, or another registry, you should be able to use it with the docker command, or on a Kubernetes cluster, or with the podman utility, or any other tool that supports the OCI image format spec.","title":"Docker images"},{"location":"DevOps/Kubernetes/architecture/","text":"Abstract \u00b6 A daemon for Linux and Windows. It manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments and beyond. Architecture \u00b6","title":"Architecture"},{"location":"DevOps/Kubernetes/architecture/#abstract","text":"A daemon for Linux and Windows. It manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments and beyond.","title":"Abstract"},{"location":"DevOps/Kubernetes/architecture/#architecture","text":"","title":"Architecture"},{"location":"DevOps/Kubernetes/kubernetes/","text":"Abstract \u00b6 Kubernetes \u2013 A container orchestrator that works with multiple container runtimes, including containerd. Kubernetes is focused on deploying containers in aggregate across one or more physical \u201cnodes.\u201d Historically, Kubernetes was tied to Docker. Architecture \u00b6 The Control plane \u00b6 On the left, you\u2019ve got the control plane , which lives across multiple nodes. \u201cControl plane\u201d is a pretty broad term to describe the components that manage the cluster. It includes things like the Kubernetes API Server (which you interact with when you use kubectl ), and Etcd (which is the data store that holds the desired state of the cluster). The nodes \u00b6 The nodes are the worker machines which run your Pods and their containers. On each node, the Kubelet is a clever daemon that manages the containers running on that node, and reports back their status to the control plane. You want to learn more about Kubernetes, check out the components page on the official Kubernetes on the website.","title":"Kubernetes"},{"location":"DevOps/Kubernetes/kubernetes/#abstract","text":"Kubernetes \u2013 A container orchestrator that works with multiple container runtimes, including containerd. Kubernetes is focused on deploying containers in aggregate across one or more physical \u201cnodes.\u201d Historically, Kubernetes was tied to Docker.","title":"Abstract"},{"location":"DevOps/Kubernetes/kubernetes/#architecture","text":"","title":"Architecture"},{"location":"DevOps/Kubernetes/kubernetes/#the-control-plane","text":"On the left, you\u2019ve got the control plane , which lives across multiple nodes. \u201cControl plane\u201d is a pretty broad term to describe the components that manage the cluster. It includes things like the Kubernetes API Server (which you interact with when you use kubectl ), and Etcd (which is the data store that holds the desired state of the cluster).","title":"The Control plane"},{"location":"DevOps/Kubernetes/kubernetes/#the-nodes","text":"The nodes are the worker machines which run your Pods and their containers. On each node, the Kubelet is a clever daemon that manages the containers running on that node, and reports back their status to the control plane. You want to learn more about Kubernetes, check out the components page on the official Kubernetes on the website.","title":"The nodes"},{"location":"DevOps/Rancher/K3s/","text":"K3s \u00b6 Abstract \u00b6 Lightweight Kubernetes. Production ready, easy to install, half the memory, all in a binary less than 100 MB. Great for: Edge IoT CI Development ARM Embedding k8s Situations where a PhD in k8s clusterology is infeasible Architecture \u00b6 How it works alpine Linux : \u00b6 mini requirement can be used by virtual systems which optimized by slimmed down the kernel. Warning In order to set up Alpine Linux, you have to go through the following preparation: Update /etc/update-extlinux.conf by adding: default_kernel_opts=\"... cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory\" update-extlinux reboot curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE = \"644\" sh -s","title":"K3s"},{"location":"DevOps/Rancher/K3s/#k3s","text":"","title":"K3s"},{"location":"DevOps/Rancher/K3s/#abstract","text":"Lightweight Kubernetes. Production ready, easy to install, half the memory, all in a binary less than 100 MB. Great for: Edge IoT CI Development ARM Embedding k8s Situations where a PhD in k8s clusterology is infeasible","title":"Abstract"},{"location":"DevOps/Rancher/K3s/#architecture","text":"How it works","title":"Architecture"},{"location":"DevOps/Rancher/K3s/#alpine-linux","text":"mini requirement can be used by virtual systems which optimized by slimmed down the kernel. Warning In order to set up Alpine Linux, you have to go through the following preparation: Update /etc/update-extlinux.conf by adding: default_kernel_opts=\"... cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory\" update-extlinux reboot curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE = \"644\" sh -s","title":"alpine Linux:"},{"location":"DevOps/Terraform/KVM/","text":"K3s cluster on Ubuntu using terraform and libvirt \u00b6 K3s is a lightweight Kubernetes deployment by Rancher that is fully compliant, yet also compact enough to run on development boxes and edge devices. In this article, I will show you how to deploy a three-node K3s cluster on Ubuntu nodes that are created using Terraform and a local KVM libvirt provider . Creating node VMs \u00b6 We will deploy this K3s cluster on three independent guests running Ubuntu. These Ubuntu VMs could actually be created using any hypervisor or hyperscaler, but for this article we will use Terraform and the local KVM libvirt provider to create guests named: k3s-1 , k3s-2 , k3s-3 . We will place them on the standard KVM default network, 192.168.122.0/24 . Install Terraform, its libvirt provider, and KVM as described in a previous article. Then use my github project to create the three Ubuntu guest OS. # required packages sudo apt install make git curl -y # github project with terraform to create guest OS git clone https://github.com/fabianlee/k3s-cluster-kvm.git cd k3s-cluster-kvm # run terraform init and apply cd tf-libvirt # make ssh keypair for login as 'ubuntu' ssh-keygen -t rsa -b 4096 -f id_rsa -C tf-libvirt -N \"\" -q # initialize terraform and its plugins terraform init # do creation terraform apply -auto-approve The KVM guests can now be listed using virsh. I have embedded the IP address in the libvirt domain name to make the address obvious. # should show three running k3s VMs $ virsh list Id Name State -------------------------------------------- ... 10 k3s-1-192.168.122.213 running 11 k3s-2-192.168.122.214 running 12 k3s-1-192.168.122.215 running cloud-init has been used to give the \u2018ubuntu\u2019 user an ssh keypair for login, which allows us to validate the login for each host using the command below. # accept key as known_hosts for octet in $( seq 213 215 ) ; do ssh-keyscan -H 192 .168.122. $octet >> ~/.ssh/known_hosts ; done # test ssh into remote host for octet in $( seq 213 215 ) ; do ssh -i id_rsa ubuntu@192.168.122. $octet \"hostname -f; uptime\" ; done Updating hosts files for K3s cluster \u00b6 To allow all the VMs participating in the K3s cluster to see each other, add entries to / etc / hosts . Do this either manually like below: # login to each host manually # ssh -i id_rsa ubuntu@192.168.122.[213-215] # add these entries to each /etc/hosts k3s-1 192.168.122.213 k3s-2 192.168.122.214 k3s-3 192.168.122.215 Or you can use this single command. for octet in $( seq 213 215 ) ; do ssh -i id_rsa ubuntu@192.168.122. $octet 'echo -e \"k3s-1 192.168.122.213\\nk3s-2 192.168.122.214\\nk3s-3 192.168.122.215\" | sudo tee -a /etc/hosts' ; done Install master K3s node \u00b6 Login manually to the K3s master node (.213) and run these commands. # login to master $ ssh -i id_rsa ubuntu@192.168.122.213 # install $ sudo curl -sfL https://get.k3s.io | sh - [ INFO ] Finding release for channel stable [ INFO ] Using v1.21.4+k3s1 as release [ INFO ] Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.21.4+k3s1/sha256sum-amd64.txt [ INFO ] Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.21.4+k3s1/k3s [ INFO ] Verifying binary download [ INFO ] Installing k3s to /usr/local/bin/k3s [ INFO ] Creating /usr/local/bin/kubectl symlink to k3s [ INFO ] Creating /usr/local/bin/crictl symlink to k3s [ INFO ] Creating /usr/local/bin/ctr symlink to k3s [ INFO ] Creating killall script /usr/local/bin/k3s-killall.sh [ INFO ] Creating uninstall script /usr/local/bin/k3s-uninstall.sh [ INFO ] env: Creating environment file /etc/systemd/system/k3s.service.env [ INFO ] systemd: Creating service file /etc/systemd/system/k3s.service [ INFO ] systemd: Enabling k3s unit Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service \u2192 /etc/systemd/system/k3s.service. [ INFO ] systemd: Starting k3s And then do a validation to ensure the service is started and kubectl reports back a single master node. # validate service, status should say 'active (running)' $ sudo systemctl status k3s --no-pager | head -n5 k3s.service - Lightweight Kubernetes Loaded : loaded (/etc/systemd/system/k3s.service; enabled; vendor preset : enabled) Active : active (running) since Sun 2021-09-12 20:39:32 UTC; 5min ago Docs : https://k3s.io Process : 3050 ExecStartPre=/bin/sh -xc ! /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service (code=exited, status=0/SUCCESS) # validate that kubectl returns single master $ sudo kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k3s-1 Ready control-plane,master 7m3s v1.21.4+k3s1 192.168.122.213 Ubuntu 20.04.3 LTS 5.4.0-84-generic containerd://1.4.9-k3s1 The master node has a file \u201c / var / lib / rancher / k3s / server / node-token \u201d which we will use in the next section to add nodes to the cluster. Join K3s nodes to the cluster \u00b6 In order to add K3s nodes to the master, we need to use the key found on the master at \u201c/var/lib/rancher/k3s/server/node-token\u201d . So let\u2019s capture that value. k3s_master = 192 .168.122.213 # grab node token from master node_token = $( ssh -i id_rsa ubuntu@ $k3s_master \"sudo cat /var/lib/rancher/k3s/server/node-token\" ) echo node_token from master is $node_token To create a cluster run the get.k3s.io script just like the master, but this time add parameters for the master node IP and node token. # install on k3s-2, join cluster $ ssh -i id_rsa ubuntu@192.168.122.214 \"sudo curl -sfL http://get.k3s.io | K3S_URL=https:// ${ k3s_master } :6443 K3S_TOKEN= ${ node_token } sh -\" [ INFO ] Finding release for channel stable [ INFO ] Using v1.21.4+k3s1 as release [ INFO ] Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.21.4+k3s1/sha256sum-amd64.txt [ INFO ] Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.21.4+k3s1/k3s [ INFO ] Verifying binary download [ INFO ] Installing k3s to /usr/local/bin/k3s [ INFO ] Creating /usr/local/bin/kubectl symlink to k3s [ INFO ] Creating /usr/local/bin/crictl symlink to k3s [ INFO ] Creating /usr/local/bin/ctr symlink to k3s [ INFO ] Creating killall script /usr/local/bin/k3s-killall.sh [ INFO ] Creating uninstall script /usr/local/bin/k3s-agent-uninstall.sh [ INFO ] env: Creating environment file /etc/systemd/system/k3s-agent.service.env [ INFO ] systemd: Creating service file /etc/systemd/system/k3s-agent.service [ INFO ] systemd: Enabling k3s-agent unit Created symlink /etc/systemd/system/multi-user.target.wants/k3s-agent.service \u2192 /etc/systemd/system/k3s-agent.service. [ INFO ] systemd: Starting k3s-agent In the same way, join the k3s-3 node. # install on k3s-3, join cluster $ ssh -i id_rsa ubuntu@192.168.122.214 \"sudo curl -sfL http://get.k3s.io | K3S_URL=https:// ${ k3s_master } :6443 K3S_TOKEN= ${ node_token } sh -\" Validate Kubernetes Cluster \u00b6 A call to kubectl should now show all 3 nodes participating in the Kubernetes cluster. $ ssh -i id_rsa ubuntu@192.168.122.213 \"sudo kubectl get nodes\" NAME STATUS ROLES AGE VERSION k3s-2 Ready 10m v1.21.4+k3s1 k3s-3 Ready 4m7s v1.21.4+k3s1 k3s-1 Ready control-plane,master 32m v1.21.4+k3s1 Validate app deployment to cluster \u00b6 As a quick test of the Kubernetes cluster, create a test deployment of the whoami app which is exposed using the default K3s Traefik ingress . # copy deployment manifest to master scp -i id_rsa whoami_traefik_example.yml ubuntu@192.168.122.213:. # login to master ssh -i id_rsa ubuntu@192.168.122.213 # apply manifest $ sudo kubectl apply -f whoami_traefik_example.yml deployment.apps/whoami-ds created service/whoami-service created ingressroute.traefik.containo.us/whoami-ingressroute created # exposed services should now have 'whoami-service' $ sudo kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 443/TCP 49m whoami-service ClusterIP 10.43.210.80 80/TCP 40s # put ClusterIP of whoami service into variable cluster_ip=$(sudo kubectl get service/whoami-service -o=jsonpath=\"{.spec.clusterIP}\") # curl to service ClusterIP $ curl http://$cluster_ip/whoami Hostname : whoami-ds-78447d957f-jnfrw IP : 127.0.0.1 IP : ::1 IP : 10.42.1.3 IP : fe80::18:13ff:febc:c0fb RemoteAddr : 10.42.0.0:18326 GET /whoami HTTP/1.1 Host : 10.43.210.80 User-Agent : curl/7.68.0 Accept : * /* # also exposed by Traefik $ sudo kubectl get service traefik -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traefik LoadBalancer 10.43.214.196 192.168.122.213,192.168.122.214,192.168.122.215 80:30180/TCP,443:31528/TCP 59m # available here from master node $ curl http://localhost/whoami $ curl http://k3s-1.local/whoami # leave ssh session exit And Traefik also exposes this service externally. # available also outside cluster $ curl http://192.168.122.213/whoami Hostname : whoami-ds-78447d957f-jnfrw IP : 127.0.0.1 IP : ::1 IP : 10.42.1.3 IP : fe80::18:13ff:febc:c0fb RemoteAddr : 10.42.0.8:35890 GET /whoami HTTP/1.1 Host : 192.168.122.213 User-Agent : curl/7.68.0 Accept : * /* Accept-Encoding : gzip X-Forwarded-For : 10.42.0.1 X-Forwarded-Host : 192.168.122.213 X-Forwarded-Port : 80 X-Forwarded-Proto : http X-Forwarded-Server : traefik-97b44b794-k6bbx X-Real-Ip : 10.42.0.1 # available at each cluster node $ curl http://192.168.122.214/whoami $ curl http://192.168.122.215/whoami View traefik dashboard \u00b6 # http://127.0.0.1:9000/dashboard kubectl port-forward -n kube-system pod/traefik-97b44b794-4mpx5 9000 :9000 k3s config.toml for configuration of containerd sudo vi /var/lib/rancher/k3s/agent/etc/containerd/config.toml","title":"KVM"},{"location":"DevOps/Terraform/KVM/#k3s-cluster-on-ubuntu-using-terraform-and-libvirt","text":"K3s is a lightweight Kubernetes deployment by Rancher that is fully compliant, yet also compact enough to run on development boxes and edge devices. In this article, I will show you how to deploy a three-node K3s cluster on Ubuntu nodes that are created using Terraform and a local KVM libvirt provider .","title":"K3s cluster on Ubuntu using terraform and libvirt"},{"location":"DevOps/Terraform/KVM/#creating-node-vms","text":"We will deploy this K3s cluster on three independent guests running Ubuntu. These Ubuntu VMs could actually be created using any hypervisor or hyperscaler, but for this article we will use Terraform and the local KVM libvirt provider to create guests named: k3s-1 , k3s-2 , k3s-3 . We will place them on the standard KVM default network, 192.168.122.0/24 . Install Terraform, its libvirt provider, and KVM as described in a previous article. Then use my github project to create the three Ubuntu guest OS. # required packages sudo apt install make git curl -y # github project with terraform to create guest OS git clone https://github.com/fabianlee/k3s-cluster-kvm.git cd k3s-cluster-kvm # run terraform init and apply cd tf-libvirt # make ssh keypair for login as 'ubuntu' ssh-keygen -t rsa -b 4096 -f id_rsa -C tf-libvirt -N \"\" -q # initialize terraform and its plugins terraform init # do creation terraform apply -auto-approve The KVM guests can now be listed using virsh. I have embedded the IP address in the libvirt domain name to make the address obvious. # should show three running k3s VMs $ virsh list Id Name State -------------------------------------------- ... 10 k3s-1-192.168.122.213 running 11 k3s-2-192.168.122.214 running 12 k3s-1-192.168.122.215 running cloud-init has been used to give the \u2018ubuntu\u2019 user an ssh keypair for login, which allows us to validate the login for each host using the command below. # accept key as known_hosts for octet in $( seq 213 215 ) ; do ssh-keyscan -H 192 .168.122. $octet >> ~/.ssh/known_hosts ; done # test ssh into remote host for octet in $( seq 213 215 ) ; do ssh -i id_rsa ubuntu@192.168.122. $octet \"hostname -f; uptime\" ; done","title":"Creating node VMs"},{"location":"DevOps/Terraform/KVM/#updating-hosts-files-for-k3s-cluster","text":"To allow all the VMs participating in the K3s cluster to see each other, add entries to / etc / hosts . Do this either manually like below: # login to each host manually # ssh -i id_rsa ubuntu@192.168.122.[213-215] # add these entries to each /etc/hosts k3s-1 192.168.122.213 k3s-2 192.168.122.214 k3s-3 192.168.122.215 Or you can use this single command. for octet in $( seq 213 215 ) ; do ssh -i id_rsa ubuntu@192.168.122. $octet 'echo -e \"k3s-1 192.168.122.213\\nk3s-2 192.168.122.214\\nk3s-3 192.168.122.215\" | sudo tee -a /etc/hosts' ; done","title":"Updating hosts files for K3s cluster"},{"location":"DevOps/Terraform/KVM/#install-master-k3s-node","text":"Login manually to the K3s master node (.213) and run these commands. # login to master $ ssh -i id_rsa ubuntu@192.168.122.213 # install $ sudo curl -sfL https://get.k3s.io | sh - [ INFO ] Finding release for channel stable [ INFO ] Using v1.21.4+k3s1 as release [ INFO ] Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.21.4+k3s1/sha256sum-amd64.txt [ INFO ] Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.21.4+k3s1/k3s [ INFO ] Verifying binary download [ INFO ] Installing k3s to /usr/local/bin/k3s [ INFO ] Creating /usr/local/bin/kubectl symlink to k3s [ INFO ] Creating /usr/local/bin/crictl symlink to k3s [ INFO ] Creating /usr/local/bin/ctr symlink to k3s [ INFO ] Creating killall script /usr/local/bin/k3s-killall.sh [ INFO ] Creating uninstall script /usr/local/bin/k3s-uninstall.sh [ INFO ] env: Creating environment file /etc/systemd/system/k3s.service.env [ INFO ] systemd: Creating service file /etc/systemd/system/k3s.service [ INFO ] systemd: Enabling k3s unit Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service \u2192 /etc/systemd/system/k3s.service. [ INFO ] systemd: Starting k3s And then do a validation to ensure the service is started and kubectl reports back a single master node. # validate service, status should say 'active (running)' $ sudo systemctl status k3s --no-pager | head -n5 k3s.service - Lightweight Kubernetes Loaded : loaded (/etc/systemd/system/k3s.service; enabled; vendor preset : enabled) Active : active (running) since Sun 2021-09-12 20:39:32 UTC; 5min ago Docs : https://k3s.io Process : 3050 ExecStartPre=/bin/sh -xc ! /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service (code=exited, status=0/SUCCESS) # validate that kubectl returns single master $ sudo kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k3s-1 Ready control-plane,master 7m3s v1.21.4+k3s1 192.168.122.213 Ubuntu 20.04.3 LTS 5.4.0-84-generic containerd://1.4.9-k3s1 The master node has a file \u201c / var / lib / rancher / k3s / server / node-token \u201d which we will use in the next section to add nodes to the cluster.","title":"Install master K3s node"},{"location":"DevOps/Terraform/KVM/#join-k3s-nodes-to-the-cluster","text":"In order to add K3s nodes to the master, we need to use the key found on the master at \u201c/var/lib/rancher/k3s/server/node-token\u201d . So let\u2019s capture that value. k3s_master = 192 .168.122.213 # grab node token from master node_token = $( ssh -i id_rsa ubuntu@ $k3s_master \"sudo cat /var/lib/rancher/k3s/server/node-token\" ) echo node_token from master is $node_token To create a cluster run the get.k3s.io script just like the master, but this time add parameters for the master node IP and node token. # install on k3s-2, join cluster $ ssh -i id_rsa ubuntu@192.168.122.214 \"sudo curl -sfL http://get.k3s.io | K3S_URL=https:// ${ k3s_master } :6443 K3S_TOKEN= ${ node_token } sh -\" [ INFO ] Finding release for channel stable [ INFO ] Using v1.21.4+k3s1 as release [ INFO ] Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.21.4+k3s1/sha256sum-amd64.txt [ INFO ] Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.21.4+k3s1/k3s [ INFO ] Verifying binary download [ INFO ] Installing k3s to /usr/local/bin/k3s [ INFO ] Creating /usr/local/bin/kubectl symlink to k3s [ INFO ] Creating /usr/local/bin/crictl symlink to k3s [ INFO ] Creating /usr/local/bin/ctr symlink to k3s [ INFO ] Creating killall script /usr/local/bin/k3s-killall.sh [ INFO ] Creating uninstall script /usr/local/bin/k3s-agent-uninstall.sh [ INFO ] env: Creating environment file /etc/systemd/system/k3s-agent.service.env [ INFO ] systemd: Creating service file /etc/systemd/system/k3s-agent.service [ INFO ] systemd: Enabling k3s-agent unit Created symlink /etc/systemd/system/multi-user.target.wants/k3s-agent.service \u2192 /etc/systemd/system/k3s-agent.service. [ INFO ] systemd: Starting k3s-agent In the same way, join the k3s-3 node. # install on k3s-3, join cluster $ ssh -i id_rsa ubuntu@192.168.122.214 \"sudo curl -sfL http://get.k3s.io | K3S_URL=https:// ${ k3s_master } :6443 K3S_TOKEN= ${ node_token } sh -\"","title":"Join K3s nodes to the cluster"},{"location":"DevOps/Terraform/KVM/#validate-kubernetes-cluster","text":"A call to kubectl should now show all 3 nodes participating in the Kubernetes cluster. $ ssh -i id_rsa ubuntu@192.168.122.213 \"sudo kubectl get nodes\" NAME STATUS ROLES AGE VERSION k3s-2 Ready 10m v1.21.4+k3s1 k3s-3 Ready 4m7s v1.21.4+k3s1 k3s-1 Ready control-plane,master 32m v1.21.4+k3s1","title":"Validate Kubernetes Cluster"},{"location":"DevOps/Terraform/KVM/#validate-app-deployment-to-cluster","text":"As a quick test of the Kubernetes cluster, create a test deployment of the whoami app which is exposed using the default K3s Traefik ingress . # copy deployment manifest to master scp -i id_rsa whoami_traefik_example.yml ubuntu@192.168.122.213:. # login to master ssh -i id_rsa ubuntu@192.168.122.213 # apply manifest $ sudo kubectl apply -f whoami_traefik_example.yml deployment.apps/whoami-ds created service/whoami-service created ingressroute.traefik.containo.us/whoami-ingressroute created # exposed services should now have 'whoami-service' $ sudo kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 443/TCP 49m whoami-service ClusterIP 10.43.210.80 80/TCP 40s # put ClusterIP of whoami service into variable cluster_ip=$(sudo kubectl get service/whoami-service -o=jsonpath=\"{.spec.clusterIP}\") # curl to service ClusterIP $ curl http://$cluster_ip/whoami Hostname : whoami-ds-78447d957f-jnfrw IP : 127.0.0.1 IP : ::1 IP : 10.42.1.3 IP : fe80::18:13ff:febc:c0fb RemoteAddr : 10.42.0.0:18326 GET /whoami HTTP/1.1 Host : 10.43.210.80 User-Agent : curl/7.68.0 Accept : * /* # also exposed by Traefik $ sudo kubectl get service traefik -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traefik LoadBalancer 10.43.214.196 192.168.122.213,192.168.122.214,192.168.122.215 80:30180/TCP,443:31528/TCP 59m # available here from master node $ curl http://localhost/whoami $ curl http://k3s-1.local/whoami # leave ssh session exit And Traefik also exposes this service externally. # available also outside cluster $ curl http://192.168.122.213/whoami Hostname : whoami-ds-78447d957f-jnfrw IP : 127.0.0.1 IP : ::1 IP : 10.42.1.3 IP : fe80::18:13ff:febc:c0fb RemoteAddr : 10.42.0.8:35890 GET /whoami HTTP/1.1 Host : 192.168.122.213 User-Agent : curl/7.68.0 Accept : * /* Accept-Encoding : gzip X-Forwarded-For : 10.42.0.1 X-Forwarded-Host : 192.168.122.213 X-Forwarded-Port : 80 X-Forwarded-Proto : http X-Forwarded-Server : traefik-97b44b794-k6bbx X-Real-Ip : 10.42.0.1 # available at each cluster node $ curl http://192.168.122.214/whoami $ curl http://192.168.122.215/whoami","title":"Validate app deployment to cluster"},{"location":"DevOps/Terraform/KVM/#view-traefik-dashboard","text":"# http://127.0.0.1:9000/dashboard kubectl port-forward -n kube-system pod/traefik-97b44b794-4mpx5 9000 :9000 k3s config.toml for configuration of containerd sudo vi /var/lib/rancher/k3s/agent/etc/containerd/config.toml","title":"View traefik dashboard"},{"location":"DevOps/Terraform/Kubernetes/","text":"Deploying Kubernetes VMs in Proxmox with Terraform \u00b6 Kubernetes Proxmox Terraform Template \u00b6 Without further ado, below is the template I used to create my virtual machines. The main LAN network is 10.98.1.0/24, and the Kube internal network (on its own bridge) is 10.17.0.0/24. This template creates a Kube server, two agents, and a storage server. terraf orm { required_providers { proxmox = { source = \"telmate/proxmox\" versio n = \"2.7.4\" } } } provider \"proxmox\" { pm_api_url = \"https://prox-1u.home.fluffnet.net:8006/api2/json\" pm_api_ t oke n _id = [ secre t ] pm_api_ t oke n _secre t = [ secre t ] pm_ tls _i nse cure = true } resource \"proxmox_vm_qemu\" \"kube-server\" { cou nt = 1 na me = \"kube-server-0${count.index + 1}\" tar ge t _ n ode = \"prox-1u\" clo ne = \"ubuntu-2004-cloudinit-template\" age nt = 1 os_ t ype = \"cloud-init\" cores = 2 socke ts = 1 cpu = \"host\" memory = 4096 scsihw = \"virtio-scsi-pci\" boo t disk = \"scsi0\" disk { slo t = 0 size = \"10G\" t ype = \"scsi\" s t orage = \"local-zfs\" #s t orage_ t ype = \"zfspool\" io t hread = 1 } net work { model = \"virtio\" bridge = \"vmbr0\" } net work { model = \"virtio\" bridge = \"vmbr17\" } li fe cycle { ig n ore_cha n ges = [ net work , ] } ipco nf ig 0 = \"ip=10.98.1.4${count.index + 1}/24,gw=10.98.1.1\" ipco nf ig 1 = \"ip=10.17.0.4${count.index + 1}/24\" sshkeys = <<EOF $ { var.ssh_key } EOF } resource \"proxmox_vm_qemu\" \"kube-agent\" { cou nt = 2 na me = \"kube-agent-0${count.index + 1}\" tar ge t _ n ode = \"prox-1u\" clo ne = \"ubuntu-2004-cloudinit-template\" age nt = 1 os_ t ype = \"cloud-init\" cores = 2 socke ts = 1 cpu = \"host\" memory = 4096 scsihw = \"virtio-scsi-pci\" boo t disk = \"scsi0\" disk { slo t = 0 size = \"10G\" t ype = \"scsi\" s t orage = \"local-zfs\" #s t orage_ t ype = \"zfspool\" io t hread = 1 } net work { model = \"virtio\" bridge = \"vmbr0\" } net work { model = \"virtio\" bridge = \"vmbr17\" } li fe cycle { ig n ore_cha n ges = [ net work , ] } ipco nf ig 0 = \"ip=10.98.1.5${count.index + 1}/24,gw=10.98.1.1\" ipco nf ig 1 = \"ip=10.17.0.5${count.index + 1}/24\" sshkeys = <<EOF $ { var.ssh_key } EOF } resource \"proxmox_vm_qemu\" \"kube-storage\" { cou nt = 1 na me = \"kube-storage-0${count.index + 1}\" tar ge t _ n ode = \"prox-1u\" clo ne = \"ubuntu-2004-cloudinit-template\" age nt = 1 os_ t ype = \"cloud-init\" cores = 2 socke ts = 1 cpu = \"host\" memory = 4096 scsihw = \"virtio-scsi-pci\" boo t disk = \"scsi0\" disk { slo t = 0 size = \"20G\" t ype = \"scsi\" s t orage = \"local-zfs\" #s t orage_ t ype = \"zfspool\" io t hread = 1 } net work { model = \"virtio\" bridge = \"vmbr0\" } net work { model = \"virtio\" bridge = \"vmbr17\" } li fe cycle { ig n ore_cha n ges = [ net work , ] } ipco nf ig 0 = \"ip=10.98.1.6${count.index + 1}/24,gw=10.98.1.1\" ipco nf ig 1 = \"ip=10.17.0.6${count.index + 1}/24\" sshkeys = <<EOF $ { var.ssh_key } EOF } After running Terraform plan and apply, you should have 4 new VMs in your Proxmox cluster: Conclusion \u00b6 You now have 4 VMs ready for Kubernetes installation. The next post will show how to install Kubernetes with Ansible.","title":"Kubernetes"},{"location":"DevOps/Terraform/Kubernetes/#deploying-kubernetes-vms-in-proxmox-with-terraform","text":"","title":"Deploying Kubernetes VMs in Proxmox with Terraform"},{"location":"DevOps/Terraform/Kubernetes/#kubernetes-proxmox-terraform-template","text":"Without further ado, below is the template I used to create my virtual machines. The main LAN network is 10.98.1.0/24, and the Kube internal network (on its own bridge) is 10.17.0.0/24. This template creates a Kube server, two agents, and a storage server. terraf orm { required_providers { proxmox = { source = \"telmate/proxmox\" versio n = \"2.7.4\" } } } provider \"proxmox\" { pm_api_url = \"https://prox-1u.home.fluffnet.net:8006/api2/json\" pm_api_ t oke n _id = [ secre t ] pm_api_ t oke n _secre t = [ secre t ] pm_ tls _i nse cure = true } resource \"proxmox_vm_qemu\" \"kube-server\" { cou nt = 1 na me = \"kube-server-0${count.index + 1}\" tar ge t _ n ode = \"prox-1u\" clo ne = \"ubuntu-2004-cloudinit-template\" age nt = 1 os_ t ype = \"cloud-init\" cores = 2 socke ts = 1 cpu = \"host\" memory = 4096 scsihw = \"virtio-scsi-pci\" boo t disk = \"scsi0\" disk { slo t = 0 size = \"10G\" t ype = \"scsi\" s t orage = \"local-zfs\" #s t orage_ t ype = \"zfspool\" io t hread = 1 } net work { model = \"virtio\" bridge = \"vmbr0\" } net work { model = \"virtio\" bridge = \"vmbr17\" } li fe cycle { ig n ore_cha n ges = [ net work , ] } ipco nf ig 0 = \"ip=10.98.1.4${count.index + 1}/24,gw=10.98.1.1\" ipco nf ig 1 = \"ip=10.17.0.4${count.index + 1}/24\" sshkeys = <<EOF $ { var.ssh_key } EOF } resource \"proxmox_vm_qemu\" \"kube-agent\" { cou nt = 2 na me = \"kube-agent-0${count.index + 1}\" tar ge t _ n ode = \"prox-1u\" clo ne = \"ubuntu-2004-cloudinit-template\" age nt = 1 os_ t ype = \"cloud-init\" cores = 2 socke ts = 1 cpu = \"host\" memory = 4096 scsihw = \"virtio-scsi-pci\" boo t disk = \"scsi0\" disk { slo t = 0 size = \"10G\" t ype = \"scsi\" s t orage = \"local-zfs\" #s t orage_ t ype = \"zfspool\" io t hread = 1 } net work { model = \"virtio\" bridge = \"vmbr0\" } net work { model = \"virtio\" bridge = \"vmbr17\" } li fe cycle { ig n ore_cha n ges = [ net work , ] } ipco nf ig 0 = \"ip=10.98.1.5${count.index + 1}/24,gw=10.98.1.1\" ipco nf ig 1 = \"ip=10.17.0.5${count.index + 1}/24\" sshkeys = <<EOF $ { var.ssh_key } EOF } resource \"proxmox_vm_qemu\" \"kube-storage\" { cou nt = 1 na me = \"kube-storage-0${count.index + 1}\" tar ge t _ n ode = \"prox-1u\" clo ne = \"ubuntu-2004-cloudinit-template\" age nt = 1 os_ t ype = \"cloud-init\" cores = 2 socke ts = 1 cpu = \"host\" memory = 4096 scsihw = \"virtio-scsi-pci\" boo t disk = \"scsi0\" disk { slo t = 0 size = \"20G\" t ype = \"scsi\" s t orage = \"local-zfs\" #s t orage_ t ype = \"zfspool\" io t hread = 1 } net work { model = \"virtio\" bridge = \"vmbr0\" } net work { model = \"virtio\" bridge = \"vmbr17\" } li fe cycle { ig n ore_cha n ges = [ net work , ] } ipco nf ig 0 = \"ip=10.98.1.6${count.index + 1}/24,gw=10.98.1.1\" ipco nf ig 1 = \"ip=10.17.0.6${count.index + 1}/24\" sshkeys = <<EOF $ { var.ssh_key } EOF } After running Terraform plan and apply, you should have 4 new VMs in your Proxmox cluster:","title":"Kubernetes Proxmox Terraform Template"},{"location":"DevOps/Terraform/Kubernetes/#conclusion","text":"You now have 4 VMs ready for Kubernetes installation. The next post will show how to install Kubernetes with Ansible.","title":"Conclusion"},{"location":"DevOps/Terraform/Proxmox/","text":"How to deploy VMs in Proxmox with Terraform \u00b6 Background \u00b6 I\u2019d like to learn Kubernetes and DevOps. A Kubernetes cluster requires at least 3 VMs/bare metal machines. In my last post, I wrote about how to create a Ubuntu cloud-init template for Proxmox . In this post, we\u2019ll take that template and use it to deploy a couple VMs via automation using Terraform. If you don\u2019t have a template, you need one before proceeding. Overview \u00b6 Install Terraform Determine authentication method for Terraform to interact with Proxmox (user/pass vs API keys) Terraform basic initialization and provider installation Develop Terraform plan Terraform plan Run Terraform plan and watch the VMs appear! Install Terraform \u00b6 curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add - sudo apt-add-repository \"deb [arch= $( dpkg --print-architecture ) ] https://apt.releases.hashicorp.com $( lsb_release -cs ) main\" sudo apt update sudo apt install terraform Determine Authentication Method (use API keys) \u00b6 You have two options here: Username/password \u2013 you can use the existing default root user and root password here to make things easy\u2026 or API keys \u2013 this involves setting up a new user, giving that new user the required permissions, and then setting up API keys so that user doesn\u2019t have to type in a password to perform actions I went with the API key method since it is not desirable to have your root password sitting in Terraform files (even as an environment variable isn\u2019t a great idea). I didn\u2019t really know what I was doing and I basically gave the new user full admin permissions anyways. Should I lock it down? Surely. Do I know what the minimum required permissions are to do so? Nope. If someone in the comments or on Reddit could enlighten me, I\u2019d really appreciate it! So we need to create a new user. We\u2019ll name it \u2018blog_example\u2019. To add a new user go to Datacenter in the left tab, then Permissions -> Users -> Click add, name the user and click add. Adding \u2018blog_example\u2019 user to my proxmox datacenter (cluster) Next, we need to add API tokens. Click API tokens below users in the permissions category and click add. Select the user you just created and give the token an ID, and uncheck privilege separation (which means we want the token to have the same permissions as the user): Adding a new API token for user \u2018blog_example\u2019 When you click Add it will show you the key. Save this key. It will never be displayed again! Super secret API key secret Next we need to add a role to the new user. Permissions -> Add -> Path = \u2018/\u2019, User is the one you just made, role = \u2018PVEVMAdmin\u2019. This gives the user (and associated API token!) rights to all nodes (the / for path) to do VMAdmin activities: You also need to add permissions to the storage used by the VMs you want to deploy (both from and to), for me this is /storage/local-zfs (might be /storage/local-lvm for you). Add that too in the path section. Use Admin for the role here because the user also needs the ability to allocate space in the datastore (you could use PVEVMAdmin + a datastore role but I haven\u2019t dove into which one yet): At this point we are done with the permissions: Terraform basic information and provider installation \u00b6 Terraform has three main stages: init, plan, and apply. We will start with describing the plans, which can be thought of a a type of configuration file for what you want to do. Plans are files stored in directories. Make a new directory (terraform-blog), and create two files: main.tf and vars.tf: cd ~ mkdir terraform-blog && cd terraform-blog touch main.tf vars.tf The two files are hopefully reasonably named. The main content will be in main.tf and we will put a few variables in vars.tf. Everything could go in main.tf but it is a good practice to start splitting things out early. I actually don\u2019t have as much in vars.tf as I should but we all gotta start somewhere Ok so in main.tf let\u2019s add the bare minimum. We need to tell Terraform to use a provider, which is the term they use for the connector to the entity Terraform will be interacting with. Since we are using Proxmox, we need to use a Proxmox provider. This is actually super easy \u2013 we just need to specify the name and version and Terraform goes out and grabs it from github and installs it. I used the Telmate Proxmox provider . main.tf: terraf orm { required_providers { proxmox = { source = \"telmate/proxmox\" versio n = \"2.7.4\" } } } Save the file. Now we\u2019ll initialize Terraform with our barebones plan (terraform init), which will force it to go out and grab the provider. If all goes well, we will be informed that the provider was installed and that Terraform has been initialized. Terraform is also really nice in that it tells you the next step towards the bottom of the output (\u201ctry running \u2018terraform plan\u2019 next\u201d). austin@EARTH:/mnt/c/Users/Austin/terraform-blog$ terraform init Initializing the backend... Initializing provider plugins... - Finding telmate/proxmox versions matching \"2.7.4\"... - Installing telmate/proxmox v2.7.4... - Installed telmate/proxmox v2.7.4 (self-signed, key ID A9EBBE091B35AFCE) Partner and community providers are signed by their developers. If you'd like to know more about provider signing, you can read about it here: https://www.terraform.io/docs/cli/plugins/signing.html Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Develop Terraform plan \u00b6 Alright with the provider installed, it is time to use it to deploy a VM. We will use the template as below. Alter your main.tf file to be the following. I break it down inside the file with comments terraf orm { required_providers { proxmox = { source = \"telmate/proxmox\" versio n = \"2.7.4\" } } } provider \"proxmox\" { # url is t he hos tna me (FQDN i f you have o ne ) f or t he proxmox hos t you'd like t o co nne c t t o t o issue t he comma n ds. my proxmox hos t is 'prox -1 u'. Add /api 2 /jso n a t t he e n d f or t he API pm_api_url = \"https://prox-1u:8006/api2/json\" # api t oke n id is i n t he f orm o f : <user na me>@pam!< t oke n Id> pm_api_ t oke n _id = \"blog_example@pam!new_token_id\" # t his is t he full secre t wrapped i n quo tes . do n ' t worry , I've already dele te d t his fr om my proxmox clus ter by t he t ime you read t his pos t pm_api_ t oke n _secre t = \"9ec8e608-d834-4ce5-91d2-15dd59f9a8c1\" # leave tls _i nse cure se t t o true u nless you have your proxmox SSL cer t i f ica te si tuat io n full y sor te d ou t (i f you do , you will k n ow) pm_ tls _i nse cure = true } # resource is f orma tte d t o be \"[type]\" \"[entity_name]\" so i n t his case # we are looki n g t o crea te a proxmox_vm_qemu e nt i t y na med test _server resource \"proxmox_vm_qemu\" \"test_server\" { cou nt = 1 # jus t wa nt 1 f or n ow , se t t o 0 a n d apply t o des tr oy VM na me = \"test-vm-${count.index + 1}\" #cou nt .i n dex s tarts a t 0 , so + 1 mea ns t his VM will be na med test - vm -1 i n proxmox # t his n ow reaches ou t t o t he vars f ile. I could've also used t his var above i n t he pm_api_url se tt i n g bu t wa nte d t o spell i t ou t up t here. tar ge t _ n ode is di fferent t ha n api_url. tar ge t _ n ode is which n ode hos ts t he te mpla te a n d t hus also which n ode will hos t t he ne w VM. i t ca n be di fferent t ha n t he hos t you use t o commu n ica te wi t h t he API. t he variable co nta i ns t he co ntents \"prox-1u\" tar ge t _ n ode = var.proxmox_hos t # a n o t her variable wi t h co ntents \"ubuntu-2004-cloudinit-template\" clo ne = var. te mpla te _ na me # basic VM se tt i n gs here. age nt re fers t o gues t age nt age nt = 1 os_ t ype = \"cloud-init\" cores = 2 socke ts = 1 cpu = \"host\" memory = 2048 scsihw = \"virtio-scsi-pci\" boo t disk = \"scsi0\" disk { slo t = 0 # se t disk size here. leave i t small f or test i n g because expa n di n g t he disk ta kes t ime. size = \"10G\" t ype = \"scsi\" s t orage = \"local-zfs\" io t hread = 1 } # i f you wa nt t wo NICs , jus t copy t his whole net work sec t io n a n d duplica te i t net work { model = \"virtio\" bridge = \"vmbr0\" } # n o t sure exac tl y wha t t his is f or. presumably some t hi n g abou t MAC addresses a n d ig n ore net work cha n ges duri n g t he li fe o f t he VM li fe cycle { ig n ore_cha n ges = [ net work , ] } # t he $ { cou nt .i n dex + 1 } t hi n g appe n ds te x t t o t he e n d o f t he ip address # i n t his case , si n ce we are o nl y addi n g a si n gle VM , t he IP will # be 10.98.1.91 si n ce cou nt .i n dex s tarts a t 0. t his is how you ca n crea te # mul t iple VMs a n d have a n IP assig ne d t o each (. 91 , . 92 , . 93 , e t c.) ipco nf ig 0 = \"ip=10.98.1.9${count.index + 1}/24,gw=10.98.1.1\" # sshkeys se t usi n g variables. t he variable co nta i ns t he te x t o f t he key. sshkeys = <<EOF $ { var.ssh_key } EOF } There is a good amount going on in here. Hopefully the embedded comments explain everything. If not, let me know in the comments or on Reddit (u/Nerdy-Austin). Now for the vars.tf file. This is a bit easier to understand. Just declare a variable, give it a name, and a default value. That\u2019s all I know at this point and it works. variable \"ssh_key\" { de fault = \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDcwZAOfqf6E6p8IkrurF2vR3NccPbMlXFPaFe2+Eh/8QnQCJVTL6PKduXjXynuLziC9cubXIDzQA+4OpFYUV2u0fAkXLOXRIwgEmOrnsGAqJTqIsMC3XwGRhR9M84c4XPAX5sYpOsvZX/qwFE95GAdExCUkS3H39rpmSCnZG9AY4nPsVRlIIDP+/6YSy9KWp2YVYe5bDaMKRtwKSq3EOUhl3Mm8Ykzd35Z0Cysgm2hR2poN+EB7GD67fyi+6ohpdJHVhinHi7cQI4DUp+37nVZG4ofYFL9yRdULlHcFa9MocESvFVlVW0FCvwFKXDty6askpg9yf4FnM0OSbhgqXzD austin@EARTH\" } variable \"proxmox_host\" { de fault = \"prox-1u\" } variable \"template_name\" { de fault = \"ubuntu-2004-cloudinit-template\" } Terraform plan (official term for \u201cwhat will Terraform do next\u201d) \u00b6 Now with the .tf files completed, we can run the plan (terraform plan). We defined a count=1 resource, so we would expect Terraform to create a single VM. Let\u2019s have Terraform run through the plan and tell us what it intends to do. It tells us a lot. austin@EARTH : /mnt/c/Users/Austin/terraform-blog$ terraform plan Terraform used the selected providers to generate the following execution plan . Resource actions are indicated with the following symbols : + create Terraform will perform the following actions : # proxmox_vm_qemu.test_server[0] will be created + resource \"proxmox_vm_qemu\" \"test_server\" { + additional_wait = 15 + agent = 1 + balloon = 0 + bios = \"seabios\" + boot = \"cdn\" + bootdisk = \"scsi0\" + clone = \"ubuntu-2004-cloudinit-template\" + clone_wait = 15 + cores = 2 + cpu = \"host\" + default_ipv4_address = ( known after apply ) + define_connection_info = true + force_create = false + full_clone = true + guest_agent_ready_timeout = 600 + hotplug = \"network,disk,usb\" + id = ( known after apply ) + ipconfig0 = \"ip=10.98.1.91/24,gw=10.98.1.1\" + kvm = true + memory = 2048 + name = \"test-vm-1\" + nameserver = ( known after apply ) + numa = false + onboot = true + os_type = \"cloud-init\" + preprovision = true + reboot_required = ( known after apply ) + scsihw = \"virtio-scsi-pci\" + searchdomain = ( known after apply ) + sockets = 1 + ssh_host = ( known after apply ) + ssh_port = ( known after apply ) + sshkeys = <<- EOT ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDcwZAOfqf6E6p8IkrurF2vR3NccPbMlXFPaFe2+Eh/8QnQCJVTL6PKduXjXynuLziC9cubXIDzQA+4OpFYUV2u0fAkXLOXRIwgEmOrnsGAqJTqIsMC3XwGRhR9M84c4XPAX5sYpOsvZX/qwFE95GAdExCUkS3H39rpmSCnZG9AY4nPsVRlIIDP+/6YSy9KWp2YVYe5bDaMKRtwKSq3EOUhl3Mm8Ykzd35Z0Cysgm2hR2poN+EB7GD67fyi+6ohpdJHVhinHi7cQI4DUp+37nVZG4ofYFL9yRdULlHcFa9MocESvFVlVW0FCvwFKXDty6askpg9yf4FnM0OSbhgqXzD austin@EARTH EOT + target_node = \"prox-1u\" + unused_disk = ( known after apply ) + vcpus = 0 + vlan = - 1 + vmid = ( known after apply ) + disk { + backup = 0 + cache = \"none\" + file = ( known after apply ) + format = ( known after apply ) + iothread = 1 + mbps = 0 + mbps_rd = 0 + mbps_rd_max = 0 + mbps_wr = 0 + mbps_wr_max = 0 + media = ( known after apply ) + replicate = 0 + size = \"10G\" + slot = 0 + ssd = 0 + storage = \"local-zfs\" + storage_type = ( known after apply ) + type = \"scsi\" + volume = ( known after apply ) } + network { + bridge = \"vmbr0\" + firewall = false + link_down = false + macaddr = ( known after apply ) + model = \"virtio\" + queues = ( known after apply ) + rate = ( known after apply ) + tag = - 1 } } Plan : 1 to add , 0 to change , 0 to destroy . \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Note : You didn't use the -out option to save this plan , so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now . You can see the output of the planning phase of Terraform. It is telling us it will create proxmox_vm_qemu.test_server[0] with a list of parameters. You can double-check the IP address here, as well as the rest of the basic settings. At the bottom is the summary \u2013 \u201cPlan: 1 to add, 0 to change, 0 to destroy.\u201d Also note that it tells us again what step to run next \u2013 \u201cterraform apply\u201d. Execute the Terraform plan and watch the VMs appear! \u00b6 With the summary stating what we want, we can now apply the plan (terraform apply). Note that it prompts you to type in \u2018yes\u2019 to apply the changes after it determines what the changes are . It typically takes 1m15s \u00b1 15s for my VMs to get created. If all goes well, you will be informed that 1 resource was added! Command and full output: austin@EARTH : /mnt/c/Users/Austin/terraform-blog$ terraform apply Terraform used the selected providers to generate the following execution plan . Resource actions are indicated with the following symbols : + create Terraform will perform the following actions : # proxmox_vm_qemu.test_server[0] will be created + resource \"proxmox_vm_qemu\" \"test_server\" { + additional_wait = 15 + agent = 1 + balloon = 0 + bios = \"seabios\" + boot = \"cdn\" + bootdisk = \"scsi0\" + clone = \"ubuntu-2004-cloudinit-template\" + clone_wait = 15 + cores = 2 + cpu = \"host\" + default_ipv4_address = ( known after apply ) + define_connection_info = true + force_create = false + full_clone = true + guest_agent_ready_timeout = 600 + hotplug = \"network,disk,usb\" + id = ( known after apply ) + ipconfig0 = \"ip=10.98.1.91/24,gw=10.98.1.1\" + kvm = true + memory = 2048 + name = \"test-vm-1\" + nameserver = ( known after apply ) + numa = false + onboot = true + os_type = \"cloud-init\" + preprovision = true + reboot_required = ( known after apply ) + scsihw = \"virtio-scsi-pci\" + searchdomain = ( known after apply ) + sockets = 1 + ssh_host = ( known after apply ) + ssh_port = ( known after apply ) + sshkeys = <<- EOT ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDcwZAOfqf6E6p8IkrurF2vR3NccPbMlXFPaFe2+Eh/8QnQCJVTL6PKduXjXynuLziC9cubXIDzQA+4OpFYUV2u0fAkXLOXRIwgEmOrnsGAqJTqIsMC3XwGRhR9M84c4XPAX5sYpOsvZX/qwFE95GAdExCUkS3H39rpmSCnZG9AY4nPsVRlIIDP+/6YSy9KWp2YVYe5bDaMKRtwKSq3EOUhl3Mm8Ykzd35Z0Cysgm2hR2poN+EB7GD67fyi+6ohpdJHVhinHi7cQI4DUp+37nVZG4ofYFL9yRdULlHcFa9MocESvFVlVW0FCvwFKXDty6askpg9yf4FnM0OSbhgqXzD austin@EARTH EOT + target_node = \"prox-1u\" + unused_disk = ( known after apply ) + vcpus = 0 + vlan = - 1 + vmid = ( known after apply ) + disk { + backup = 0 + cache = \"none\" + file = ( known after apply ) + format = ( known after apply ) + iothread = 1 + mbps = 0 + mbps_rd = 0 + mbps_rd_max = 0 + mbps_wr = 0 + mbps_wr_max = 0 + media = ( known after apply ) + replicate = 0 + size = \"10G\" + slot = 0 + ssd = 0 + storage = \"local-zfs\" + storage_type = ( known after apply ) + type = \"scsi\" + volume = ( known after apply ) } + network { + bridge = \"vmbr0\" + firewall = false + link_down = false + macaddr = ( known after apply ) + model = \"virtio\" + queues = ( known after apply ) + rate = ( known after apply ) + tag = - 1 } } Plan : 1 to add , 0 to change , 0 to destroy . Do you want to perform these actions ? Terraform will perform the actions described above . Only 'yes' will be accepted to approve . Enter a value : yes proxmox_vm_qemu.test_server[0 ]: Creating ... proxmox_vm_qemu.test_server[0 ]: Still creating ... [ 10 s elapsed ] proxmox_vm_qemu.test_server[0 ]: Still creating ... [ 20 s elapsed ] proxmox_vm_qemu.test_server[0 ]: Still creating ... [ 30 s elapsed ] proxmox_vm_qemu.test_server[0 ]: Still creating ... [ 40 s elapsed ] proxmox_vm_qemu.test_server[0 ]: Still creating ... [ 50 s elapsed ] proxmox_vm_qemu.test_server[0 ]: Still creating ... [ 1 m 0 s elapsed ] proxmox_vm_qemu.test_server[0 ]: Creation complete after 1 m 9 s [ id = prox- 1 u/qemu/ 142 ] Apply complete ! Resources : 1 added , 0 changed , 0 destroyed . Now go check Proxmox and see if your VM was created: Success! You should now be able to SSH into the new VM with the key you already provided (note: the username will be \u2018ubuntu\u2019, not whatever you had set in your key) . Removing the test VM \u00b6 I just set the count to 0 for the resource in the main.tf file and apply and the VM is stopped and destroyed. resource \"proxmox_vm_qemu\" \"test_server\" { count = # just want 1 for now, set to 0 and apply to destroy } Conclusion \u00b6 This felt like a quick-n-dirty tutorial for how to use Terraform to deploy virtual machines in Proxmox but looking back, there is a decent amount of detail. It took me quite a while to work through permission issues, hostnames being invalid (turns out you can\u2019t have underscores (_) in hostnames, duh, that took an hour to find), assigning roles to users vs the associated API keys, etc. but I\u2019m glad I worked through everything and can pass it along. Check back soon for my next post on using Terraform to deploy a full set of Kubernetes machines to a Proxmox cluster (and thrilling sequel to that post, Using Ansible to bootstrap a Kubernetes Cluster)! References \u00b6 https://austinsnerdythings.com/2021/09/01/how-to-deploy-vms-in-proxmox-with-terraform/ https://registry.terraform.io/providers/Telmate/proxmox/latest/docs https://vectops.com/2020/05/provision-proxmox-vms-with-terraform-quick-and-easy/ https://norocketscience.at/provision-proxmox-virtual-machines-with-terraform/","title":"Proxmox"},{"location":"DevOps/Terraform/Proxmox/#how-to-deploy-vms-in-proxmox-with-terraform","text":"","title":"How to deploy VMs in Proxmox with Terraform"},{"location":"DevOps/Terraform/Proxmox/#background","text":"I\u2019d like to learn Kubernetes and DevOps. A Kubernetes cluster requires at least 3 VMs/bare metal machines. In my last post, I wrote about how to create a Ubuntu cloud-init template for Proxmox . In this post, we\u2019ll take that template and use it to deploy a couple VMs via automation using Terraform. If you don\u2019t have a template, you need one before proceeding.","title":"Background"},{"location":"DevOps/Terraform/Proxmox/#overview","text":"Install Terraform Determine authentication method for Terraform to interact with Proxmox (user/pass vs API keys) Terraform basic initialization and provider installation Develop Terraform plan Terraform plan Run Terraform plan and watch the VMs appear!","title":"Overview"},{"location":"DevOps/Terraform/Proxmox/#install-terraform","text":"curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add - sudo apt-add-repository \"deb [arch= $( dpkg --print-architecture ) ] https://apt.releases.hashicorp.com $( lsb_release -cs ) main\" sudo apt update sudo apt install terraform","title":"Install Terraform"},{"location":"DevOps/Terraform/Proxmox/#determine-authentication-method-use-api-keys","text":"You have two options here: Username/password \u2013 you can use the existing default root user and root password here to make things easy\u2026 or API keys \u2013 this involves setting up a new user, giving that new user the required permissions, and then setting up API keys so that user doesn\u2019t have to type in a password to perform actions I went with the API key method since it is not desirable to have your root password sitting in Terraform files (even as an environment variable isn\u2019t a great idea). I didn\u2019t really know what I was doing and I basically gave the new user full admin permissions anyways. Should I lock it down? Surely. Do I know what the minimum required permissions are to do so? Nope. If someone in the comments or on Reddit could enlighten me, I\u2019d really appreciate it! So we need to create a new user. We\u2019ll name it \u2018blog_example\u2019. To add a new user go to Datacenter in the left tab, then Permissions -> Users -> Click add, name the user and click add. Adding \u2018blog_example\u2019 user to my proxmox datacenter (cluster) Next, we need to add API tokens. Click API tokens below users in the permissions category and click add. Select the user you just created and give the token an ID, and uncheck privilege separation (which means we want the token to have the same permissions as the user): Adding a new API token for user \u2018blog_example\u2019 When you click Add it will show you the key. Save this key. It will never be displayed again! Super secret API key secret Next we need to add a role to the new user. Permissions -> Add -> Path = \u2018/\u2019, User is the one you just made, role = \u2018PVEVMAdmin\u2019. This gives the user (and associated API token!) rights to all nodes (the / for path) to do VMAdmin activities: You also need to add permissions to the storage used by the VMs you want to deploy (both from and to), for me this is /storage/local-zfs (might be /storage/local-lvm for you). Add that too in the path section. Use Admin for the role here because the user also needs the ability to allocate space in the datastore (you could use PVEVMAdmin + a datastore role but I haven\u2019t dove into which one yet): At this point we are done with the permissions:","title":"Determine Authentication Method (use API keys)"},{"location":"DevOps/Terraform/Proxmox/#terraform-basic-information-and-provider-installation","text":"Terraform has three main stages: init, plan, and apply. We will start with describing the plans, which can be thought of a a type of configuration file for what you want to do. Plans are files stored in directories. Make a new directory (terraform-blog), and create two files: main.tf and vars.tf: cd ~ mkdir terraform-blog && cd terraform-blog touch main.tf vars.tf The two files are hopefully reasonably named. The main content will be in main.tf and we will put a few variables in vars.tf. Everything could go in main.tf but it is a good practice to start splitting things out early. I actually don\u2019t have as much in vars.tf as I should but we all gotta start somewhere Ok so in main.tf let\u2019s add the bare minimum. We need to tell Terraform to use a provider, which is the term they use for the connector to the entity Terraform will be interacting with. Since we are using Proxmox, we need to use a Proxmox provider. This is actually super easy \u2013 we just need to specify the name and version and Terraform goes out and grabs it from github and installs it. I used the Telmate Proxmox provider . main.tf: terraf orm { required_providers { proxmox = { source = \"telmate/proxmox\" versio n = \"2.7.4\" } } } Save the file. Now we\u2019ll initialize Terraform with our barebones plan (terraform init), which will force it to go out and grab the provider. If all goes well, we will be informed that the provider was installed and that Terraform has been initialized. Terraform is also really nice in that it tells you the next step towards the bottom of the output (\u201ctry running \u2018terraform plan\u2019 next\u201d). austin@EARTH:/mnt/c/Users/Austin/terraform-blog$ terraform init Initializing the backend... Initializing provider plugins... - Finding telmate/proxmox versions matching \"2.7.4\"... - Installing telmate/proxmox v2.7.4... - Installed telmate/proxmox v2.7.4 (self-signed, key ID A9EBBE091B35AFCE) Partner and community providers are signed by their developers. If you'd like to know more about provider signing, you can read about it here: https://www.terraform.io/docs/cli/plugins/signing.html Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary.","title":"Terraform basic information and provider installation"},{"location":"DevOps/Terraform/Proxmox/#develop-terraform-plan","text":"Alright with the provider installed, it is time to use it to deploy a VM. We will use the template as below. Alter your main.tf file to be the following. I break it down inside the file with comments terraf orm { required_providers { proxmox = { source = \"telmate/proxmox\" versio n = \"2.7.4\" } } } provider \"proxmox\" { # url is t he hos tna me (FQDN i f you have o ne ) f or t he proxmox hos t you'd like t o co nne c t t o t o issue t he comma n ds. my proxmox hos t is 'prox -1 u'. Add /api 2 /jso n a t t he e n d f or t he API pm_api_url = \"https://prox-1u:8006/api2/json\" # api t oke n id is i n t he f orm o f : <user na me>@pam!< t oke n Id> pm_api_ t oke n _id = \"blog_example@pam!new_token_id\" # t his is t he full secre t wrapped i n quo tes . do n ' t worry , I've already dele te d t his fr om my proxmox clus ter by t he t ime you read t his pos t pm_api_ t oke n _secre t = \"9ec8e608-d834-4ce5-91d2-15dd59f9a8c1\" # leave tls _i nse cure se t t o true u nless you have your proxmox SSL cer t i f ica te si tuat io n full y sor te d ou t (i f you do , you will k n ow) pm_ tls _i nse cure = true } # resource is f orma tte d t o be \"[type]\" \"[entity_name]\" so i n t his case # we are looki n g t o crea te a proxmox_vm_qemu e nt i t y na med test _server resource \"proxmox_vm_qemu\" \"test_server\" { cou nt = 1 # jus t wa nt 1 f or n ow , se t t o 0 a n d apply t o des tr oy VM na me = \"test-vm-${count.index + 1}\" #cou nt .i n dex s tarts a t 0 , so + 1 mea ns t his VM will be na med test - vm -1 i n proxmox # t his n ow reaches ou t t o t he vars f ile. I could've also used t his var above i n t he pm_api_url se tt i n g bu t wa nte d t o spell i t ou t up t here. tar ge t _ n ode is di fferent t ha n api_url. tar ge t _ n ode is which n ode hos ts t he te mpla te a n d t hus also which n ode will hos t t he ne w VM. i t ca n be di fferent t ha n t he hos t you use t o commu n ica te wi t h t he API. t he variable co nta i ns t he co ntents \"prox-1u\" tar ge t _ n ode = var.proxmox_hos t # a n o t her variable wi t h co ntents \"ubuntu-2004-cloudinit-template\" clo ne = var. te mpla te _ na me # basic VM se tt i n gs here. age nt re fers t o gues t age nt age nt = 1 os_ t ype = \"cloud-init\" cores = 2 socke ts = 1 cpu = \"host\" memory = 2048 scsihw = \"virtio-scsi-pci\" boo t disk = \"scsi0\" disk { slo t = 0 # se t disk size here. leave i t small f or test i n g because expa n di n g t he disk ta kes t ime. size = \"10G\" t ype = \"scsi\" s t orage = \"local-zfs\" io t hread = 1 } # i f you wa nt t wo NICs , jus t copy t his whole net work sec t io n a n d duplica te i t net work { model = \"virtio\" bridge = \"vmbr0\" } # n o t sure exac tl y wha t t his is f or. presumably some t hi n g abou t MAC addresses a n d ig n ore net work cha n ges duri n g t he li fe o f t he VM li fe cycle { ig n ore_cha n ges = [ net work , ] } # t he $ { cou nt .i n dex + 1 } t hi n g appe n ds te x t t o t he e n d o f t he ip address # i n t his case , si n ce we are o nl y addi n g a si n gle VM , t he IP will # be 10.98.1.91 si n ce cou nt .i n dex s tarts a t 0. t his is how you ca n crea te # mul t iple VMs a n d have a n IP assig ne d t o each (. 91 , . 92 , . 93 , e t c.) ipco nf ig 0 = \"ip=10.98.1.9${count.index + 1}/24,gw=10.98.1.1\" # sshkeys se t usi n g variables. t he variable co nta i ns t he te x t o f t he key. sshkeys = <<EOF $ { var.ssh_key } EOF } There is a good amount going on in here. Hopefully the embedded comments explain everything. If not, let me know in the comments or on Reddit (u/Nerdy-Austin). Now for the vars.tf file. This is a bit easier to understand. Just declare a variable, give it a name, and a default value. That\u2019s all I know at this point and it works. variable \"ssh_key\" { de fault = \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDcwZAOfqf6E6p8IkrurF2vR3NccPbMlXFPaFe2+Eh/8QnQCJVTL6PKduXjXynuLziC9cubXIDzQA+4OpFYUV2u0fAkXLOXRIwgEmOrnsGAqJTqIsMC3XwGRhR9M84c4XPAX5sYpOsvZX/qwFE95GAdExCUkS3H39rpmSCnZG9AY4nPsVRlIIDP+/6YSy9KWp2YVYe5bDaMKRtwKSq3EOUhl3Mm8Ykzd35Z0Cysgm2hR2poN+EB7GD67fyi+6ohpdJHVhinHi7cQI4DUp+37nVZG4ofYFL9yRdULlHcFa9MocESvFVlVW0FCvwFKXDty6askpg9yf4FnM0OSbhgqXzD austin@EARTH\" } variable \"proxmox_host\" { de fault = \"prox-1u\" } variable \"template_name\" { de fault = \"ubuntu-2004-cloudinit-template\" }","title":"Develop Terraform plan"},{"location":"DevOps/Terraform/Proxmox/#terraform-plan-official-term-for-what-will-terraform-do-next","text":"Now with the .tf files completed, we can run the plan (terraform plan). We defined a count=1 resource, so we would expect Terraform to create a single VM. Let\u2019s have Terraform run through the plan and tell us what it intends to do. It tells us a lot. austin@EARTH : /mnt/c/Users/Austin/terraform-blog$ terraform plan Terraform used the selected providers to generate the following execution plan . Resource actions are indicated with the following symbols : + create Terraform will perform the following actions : # proxmox_vm_qemu.test_server[0] will be created + resource \"proxmox_vm_qemu\" \"test_server\" { + additional_wait = 15 + agent = 1 + balloon = 0 + bios = \"seabios\" + boot = \"cdn\" + bootdisk = \"scsi0\" + clone = \"ubuntu-2004-cloudinit-template\" + clone_wait = 15 + cores = 2 + cpu = \"host\" + default_ipv4_address = ( known after apply ) + define_connection_info = true + force_create = false + full_clone = true + guest_agent_ready_timeout = 600 + hotplug = \"network,disk,usb\" + id = ( known after apply ) + ipconfig0 = \"ip=10.98.1.91/24,gw=10.98.1.1\" + kvm = true + memory = 2048 + name = \"test-vm-1\" + nameserver = ( known after apply ) + numa = false + onboot = true + os_type = \"cloud-init\" + preprovision = true + reboot_required = ( known after apply ) + scsihw = \"virtio-scsi-pci\" + searchdomain = ( known after apply ) + sockets = 1 + ssh_host = ( known after apply ) + ssh_port = ( known after apply ) + sshkeys = <<- EOT ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDcwZAOfqf6E6p8IkrurF2vR3NccPbMlXFPaFe2+Eh/8QnQCJVTL6PKduXjXynuLziC9cubXIDzQA+4OpFYUV2u0fAkXLOXRIwgEmOrnsGAqJTqIsMC3XwGRhR9M84c4XPAX5sYpOsvZX/qwFE95GAdExCUkS3H39rpmSCnZG9AY4nPsVRlIIDP+/6YSy9KWp2YVYe5bDaMKRtwKSq3EOUhl3Mm8Ykzd35Z0Cysgm2hR2poN+EB7GD67fyi+6ohpdJHVhinHi7cQI4DUp+37nVZG4ofYFL9yRdULlHcFa9MocESvFVlVW0FCvwFKXDty6askpg9yf4FnM0OSbhgqXzD austin@EARTH EOT + target_node = \"prox-1u\" + unused_disk = ( known after apply ) + vcpus = 0 + vlan = - 1 + vmid = ( known after apply ) + disk { + backup = 0 + cache = \"none\" + file = ( known after apply ) + format = ( known after apply ) + iothread = 1 + mbps = 0 + mbps_rd = 0 + mbps_rd_max = 0 + mbps_wr = 0 + mbps_wr_max = 0 + media = ( known after apply ) + replicate = 0 + size = \"10G\" + slot = 0 + ssd = 0 + storage = \"local-zfs\" + storage_type = ( known after apply ) + type = \"scsi\" + volume = ( known after apply ) } + network { + bridge = \"vmbr0\" + firewall = false + link_down = false + macaddr = ( known after apply ) + model = \"virtio\" + queues = ( known after apply ) + rate = ( known after apply ) + tag = - 1 } } Plan : 1 to add , 0 to change , 0 to destroy . \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Note : You didn't use the -out option to save this plan , so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now . You can see the output of the planning phase of Terraform. It is telling us it will create proxmox_vm_qemu.test_server[0] with a list of parameters. You can double-check the IP address here, as well as the rest of the basic settings. At the bottom is the summary \u2013 \u201cPlan: 1 to add, 0 to change, 0 to destroy.\u201d Also note that it tells us again what step to run next \u2013 \u201cterraform apply\u201d.","title":"Terraform plan (official term for \u201cwhat will Terraform do next\u201d)"},{"location":"DevOps/Terraform/Proxmox/#execute-the-terraform-plan-and-watch-the-vms-appear","text":"With the summary stating what we want, we can now apply the plan (terraform apply). Note that it prompts you to type in \u2018yes\u2019 to apply the changes after it determines what the changes are . It typically takes 1m15s \u00b1 15s for my VMs to get created. If all goes well, you will be informed that 1 resource was added! Command and full output: austin@EARTH : /mnt/c/Users/Austin/terraform-blog$ terraform apply Terraform used the selected providers to generate the following execution plan . Resource actions are indicated with the following symbols : + create Terraform will perform the following actions : # proxmox_vm_qemu.test_server[0] will be created + resource \"proxmox_vm_qemu\" \"test_server\" { + additional_wait = 15 + agent = 1 + balloon = 0 + bios = \"seabios\" + boot = \"cdn\" + bootdisk = \"scsi0\" + clone = \"ubuntu-2004-cloudinit-template\" + clone_wait = 15 + cores = 2 + cpu = \"host\" + default_ipv4_address = ( known after apply ) + define_connection_info = true + force_create = false + full_clone = true + guest_agent_ready_timeout = 600 + hotplug = \"network,disk,usb\" + id = ( known after apply ) + ipconfig0 = \"ip=10.98.1.91/24,gw=10.98.1.1\" + kvm = true + memory = 2048 + name = \"test-vm-1\" + nameserver = ( known after apply ) + numa = false + onboot = true + os_type = \"cloud-init\" + preprovision = true + reboot_required = ( known after apply ) + scsihw = \"virtio-scsi-pci\" + searchdomain = ( known after apply ) + sockets = 1 + ssh_host = ( known after apply ) + ssh_port = ( known after apply ) + sshkeys = <<- EOT ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDcwZAOfqf6E6p8IkrurF2vR3NccPbMlXFPaFe2+Eh/8QnQCJVTL6PKduXjXynuLziC9cubXIDzQA+4OpFYUV2u0fAkXLOXRIwgEmOrnsGAqJTqIsMC3XwGRhR9M84c4XPAX5sYpOsvZX/qwFE95GAdExCUkS3H39rpmSCnZG9AY4nPsVRlIIDP+/6YSy9KWp2YVYe5bDaMKRtwKSq3EOUhl3Mm8Ykzd35Z0Cysgm2hR2poN+EB7GD67fyi+6ohpdJHVhinHi7cQI4DUp+37nVZG4ofYFL9yRdULlHcFa9MocESvFVlVW0FCvwFKXDty6askpg9yf4FnM0OSbhgqXzD austin@EARTH EOT + target_node = \"prox-1u\" + unused_disk = ( known after apply ) + vcpus = 0 + vlan = - 1 + vmid = ( known after apply ) + disk { + backup = 0 + cache = \"none\" + file = ( known after apply ) + format = ( known after apply ) + iothread = 1 + mbps = 0 + mbps_rd = 0 + mbps_rd_max = 0 + mbps_wr = 0 + mbps_wr_max = 0 + media = ( known after apply ) + replicate = 0 + size = \"10G\" + slot = 0 + ssd = 0 + storage = \"local-zfs\" + storage_type = ( known after apply ) + type = \"scsi\" + volume = ( known after apply ) } + network { + bridge = \"vmbr0\" + firewall = false + link_down = false + macaddr = ( known after apply ) + model = \"virtio\" + queues = ( known after apply ) + rate = ( known after apply ) + tag = - 1 } } Plan : 1 to add , 0 to change , 0 to destroy . Do you want to perform these actions ? Terraform will perform the actions described above . Only 'yes' will be accepted to approve . Enter a value : yes proxmox_vm_qemu.test_server[0 ]: Creating ... proxmox_vm_qemu.test_server[0 ]: Still creating ... [ 10 s elapsed ] proxmox_vm_qemu.test_server[0 ]: Still creating ... [ 20 s elapsed ] proxmox_vm_qemu.test_server[0 ]: Still creating ... [ 30 s elapsed ] proxmox_vm_qemu.test_server[0 ]: Still creating ... [ 40 s elapsed ] proxmox_vm_qemu.test_server[0 ]: Still creating ... [ 50 s elapsed ] proxmox_vm_qemu.test_server[0 ]: Still creating ... [ 1 m 0 s elapsed ] proxmox_vm_qemu.test_server[0 ]: Creation complete after 1 m 9 s [ id = prox- 1 u/qemu/ 142 ] Apply complete ! Resources : 1 added , 0 changed , 0 destroyed . Now go check Proxmox and see if your VM was created: Success! You should now be able to SSH into the new VM with the key you already provided (note: the username will be \u2018ubuntu\u2019, not whatever you had set in your key) .","title":"Execute the Terraform plan and watch the VMs appear!"},{"location":"DevOps/Terraform/Proxmox/#removing-the-test-vm","text":"I just set the count to 0 for the resource in the main.tf file and apply and the VM is stopped and destroyed. resource \"proxmox_vm_qemu\" \"test_server\" { count = # just want 1 for now, set to 0 and apply to destroy }","title":"Removing the test VM"},{"location":"DevOps/Terraform/Proxmox/#conclusion","text":"This felt like a quick-n-dirty tutorial for how to use Terraform to deploy virtual machines in Proxmox but looking back, there is a decent amount of detail. It took me quite a while to work through permission issues, hostnames being invalid (turns out you can\u2019t have underscores (_) in hostnames, duh, that took an hour to find), assigning roles to users vs the associated API keys, etc. but I\u2019m glad I worked through everything and can pass it along. Check back soon for my next post on using Terraform to deploy a full set of Kubernetes machines to a Proxmox cluster (and thrilling sequel to that post, Using Ansible to bootstrap a Kubernetes Cluster)!","title":"Conclusion"},{"location":"DevOps/Terraform/Proxmox/#references","text":"https://austinsnerdythings.com/2021/09/01/how-to-deploy-vms-in-proxmox-with-terraform/ https://registry.terraform.io/providers/Telmate/proxmox/latest/docs https://vectops.com/2020/05/provision-proxmox-vms-with-terraform-quick-and-easy/ https://norocketscience.at/provision-proxmox-virtual-machines-with-terraform/","title":"References"},{"location":"Linux/Gentoo/Base%20system/","text":"Installing base system \u00b6 Selecting mirrors \u00b6 In order to download source code quickly it is recommended to select a fast mirror. Portage will look in the make.conf file for the GENTOO_MIRRORS variable and use the mirrors listed therein. root #mirrorselect -i -o >> /mnt/gentoo/etc/portage/make.conf Gentoo ebuild repository \u00b6 A second important step in selecting mirrors is to configure the Gentoo ebuild repository via the /etc/portage/repos.conf/gentoo.conf file. This file contains the sync information needed to update the package repository (the collection of ebuilds and related files containing all the information Portage needs to download and install software packages). root #mkdir --parents /mnt/gentoo/etc/portage/repos.conf root #cp /mnt/gentoo/usr/share/portage/config/repos.conf /mnt/gentoo/etc/portage/repos.conf/gentoo.conf # /mnt/gentoo/etc/portage/repos.conf/gentoo.conf [DEFAULT] main-repo = gentoo [gentoo] location = /var/db/repos/gentoo sync-type = rsync sync-uri = rsync://rsync.gentoo.org/gentoo-portage auto-sync = yes sync-rsync-verify-jobs = 1 sync-rsync-verify-metamanifest = yes sync-rsync-verify-max-age = 24 sync-openpgp-key-path = /usr/share/openpgp-keys/gentoo-release.asc sync-openpgp-key-refresh-retry-count = 40 sync-openpgp-key-refresh-retry-overall-timeout = 1200 sync-openpgp-key-refresh-retry-delay-exp-base = 2 sync-openpgp-key-refresh-retry-delay-max = 60 sync-openpgp-key-refresh-retry-delay-mult = 4 DNS info \u00b6 root #cp --dereference /etc/resolv.conf /mnt/gentoo/etc/ Mounting the necessary filesystmes \u00b6 In a few moments, the Linux root will be changed towards the new location. The filesystems that need to be made available are: Filesystem Description /proc/ a pseudo-filesystem. It looks like regular files, but is generated on-the-fly by the Linux kernel /sys/ is a pseudo-filesystem, like /proc/ which it was once meant to replace, and is more structured than /proc/ /dev/ is a regular file system which contains all device. It is partially managed by the Linux device manager (usually udev) /run/ is a temporary file system used for files generated at runtime, such as PID files or locks The /proc/ location will be mounted on /mnt/gentoo/proc/ whereas the others are bind-mounted. The latter means that, for instance, /mnt/gentoo/sys/ will actually be /sys/ (it is just a second entry point to the same filesystem) whereas /mnt/gentoo/proc/ is a new mount (instance so to speak) of the filesystem. root #mount --types proc /proc /mnt/gentoo/proc root #mount --rbind /sys /mnt/gentoo/sys root #mount --make-rslave /mnt/gentoo/sys root #mount --rbind /dev /mnt/gentoo/dev root #mount --make-rslave /mnt/gentoo/dev root #mount --bind /run /mnt/gentoo/run root #mount --make-slave /mnt/gentoo/run The --make-rslave operations are needed for systemd support later in the installation. Entering the new environment \u00b6 Now that all partitions are initialized and the base environment installed, it is time to enter the new installation environment by chrooting into it. This means that the session will change its root (most top-level location that can be accessed) from the current installation environment (installation CD or other installation medium) to the installation system (namely the initialized partitions). Hence the name, change root or chroot. This chrooting is done in three steps: The root location is changed from / (on the installation medium) to /mnt/gentoo/ (on the partitions) using chroot Some settings (those in /etc/profile) are reloaded in memory using the source command The primary prompt is changed to help us remember that this session is inside a chroot environment. root #chroot /mnt/gentoo /bin/bash root #source /etc/profile root #export PS1=\"(chroot) ${PS1}\" Mounting the boot partition \u00b6 root #mount /dev/sda1 /boot Congiuring Portage \u00b6 Installing a Gentoo ebuild repository snapshot from the web","title":"Base system"},{"location":"Linux/Gentoo/Base%20system/#installing-base-system","text":"","title":"Installing base system"},{"location":"Linux/Gentoo/Base%20system/#selecting-mirrors","text":"In order to download source code quickly it is recommended to select a fast mirror. Portage will look in the make.conf file for the GENTOO_MIRRORS variable and use the mirrors listed therein. root #mirrorselect -i -o >> /mnt/gentoo/etc/portage/make.conf","title":"Selecting mirrors"},{"location":"Linux/Gentoo/Base%20system/#gentoo-ebuild-repository","text":"A second important step in selecting mirrors is to configure the Gentoo ebuild repository via the /etc/portage/repos.conf/gentoo.conf file. This file contains the sync information needed to update the package repository (the collection of ebuilds and related files containing all the information Portage needs to download and install software packages). root #mkdir --parents /mnt/gentoo/etc/portage/repos.conf root #cp /mnt/gentoo/usr/share/portage/config/repos.conf /mnt/gentoo/etc/portage/repos.conf/gentoo.conf # /mnt/gentoo/etc/portage/repos.conf/gentoo.conf [DEFAULT] main-repo = gentoo [gentoo] location = /var/db/repos/gentoo sync-type = rsync sync-uri = rsync://rsync.gentoo.org/gentoo-portage auto-sync = yes sync-rsync-verify-jobs = 1 sync-rsync-verify-metamanifest = yes sync-rsync-verify-max-age = 24 sync-openpgp-key-path = /usr/share/openpgp-keys/gentoo-release.asc sync-openpgp-key-refresh-retry-count = 40 sync-openpgp-key-refresh-retry-overall-timeout = 1200 sync-openpgp-key-refresh-retry-delay-exp-base = 2 sync-openpgp-key-refresh-retry-delay-max = 60 sync-openpgp-key-refresh-retry-delay-mult = 4","title":"Gentoo ebuild repository"},{"location":"Linux/Gentoo/Base%20system/#dns-info","text":"root #cp --dereference /etc/resolv.conf /mnt/gentoo/etc/","title":"DNS info"},{"location":"Linux/Gentoo/Base%20system/#mounting-the-necessary-filesystmes","text":"In a few moments, the Linux root will be changed towards the new location. The filesystems that need to be made available are: Filesystem Description /proc/ a pseudo-filesystem. It looks like regular files, but is generated on-the-fly by the Linux kernel /sys/ is a pseudo-filesystem, like /proc/ which it was once meant to replace, and is more structured than /proc/ /dev/ is a regular file system which contains all device. It is partially managed by the Linux device manager (usually udev) /run/ is a temporary file system used for files generated at runtime, such as PID files or locks The /proc/ location will be mounted on /mnt/gentoo/proc/ whereas the others are bind-mounted. The latter means that, for instance, /mnt/gentoo/sys/ will actually be /sys/ (it is just a second entry point to the same filesystem) whereas /mnt/gentoo/proc/ is a new mount (instance so to speak) of the filesystem. root #mount --types proc /proc /mnt/gentoo/proc root #mount --rbind /sys /mnt/gentoo/sys root #mount --make-rslave /mnt/gentoo/sys root #mount --rbind /dev /mnt/gentoo/dev root #mount --make-rslave /mnt/gentoo/dev root #mount --bind /run /mnt/gentoo/run root #mount --make-slave /mnt/gentoo/run The --make-rslave operations are needed for systemd support later in the installation.","title":"Mounting the necessary filesystmes"},{"location":"Linux/Gentoo/Base%20system/#entering-the-new-environment","text":"Now that all partitions are initialized and the base environment installed, it is time to enter the new installation environment by chrooting into it. This means that the session will change its root (most top-level location that can be accessed) from the current installation environment (installation CD or other installation medium) to the installation system (namely the initialized partitions). Hence the name, change root or chroot. This chrooting is done in three steps: The root location is changed from / (on the installation medium) to /mnt/gentoo/ (on the partitions) using chroot Some settings (those in /etc/profile) are reloaded in memory using the source command The primary prompt is changed to help us remember that this session is inside a chroot environment. root #chroot /mnt/gentoo /bin/bash root #source /etc/profile root #export PS1=\"(chroot) ${PS1}\"","title":"Entering the new environment"},{"location":"Linux/Gentoo/Base%20system/#mounting-the-boot-partition","text":"root #mount /dev/sda1 /boot","title":"Mounting the boot partition"},{"location":"Linux/Gentoo/Base%20system/#congiuring-portage","text":"Installing a Gentoo ebuild repository snapshot from the web","title":"Congiuring Portage"},{"location":"Linux/Gentoo/Bootloader/","text":"Configuring the bootloader \u00b6 Selecting a boot loader \u00b6 With the Linux kernel configured, system tools installed and configuration files edited, it is time to install the last important piece of a Linux installation: the boot loader. The boot loader is responsible for firing up the Linux kernel upon boot - without it, the system would not know how to proceed when the power button has been pressed. For amd64, we document how to configure either GRUB or LILO for BIOS based systems, and GRUB or efibootmgr for UEFI systems. In this section of the Handbook a delineation has been made between emerging the boot loader's package and installing a boot loader to a system disk. Here the term emerge will be used to ask Portage to make the software package available to the system. The term install will signify the boot loader copying files or physically modifying appropriate sections of the system's disk drive in order to render the boot loader activated and ready to operate on the next power cycle. Default: GRUB \u00b6 By default, the majority of Gentoo systems now rely upon GRUB (found in the sys-boot/grub package), which is the direct successor to GRUB Legacy . With no additional configuration, GRUB gladly supports older BIOS (\"pc\") systems. With a small amount of configuration, necessary before build time, GRUB can support more than a half a dozen additional platforms. For more information, consult the Prerequisites section of the GRUB article. Emerge \u00b6 Warning When using an older BIOS system supporting only MBR partition tables, no additional configuration is needed in order to emerge GRUB: root #emerge --ask --verbose sys-boot/grub Tip UEFI users : running the above command will output the enabled GRUB_PLATFORMS values before emerging. When using UEFI capable systems, users will need to ensure GRUB_PLATFORMS=\"efi-64\" is enabled (as it is the case by default). If that is not the case for the setup, GRUB_PLATFORMS=\"efi-64\" will need to be added to the /etc/portage/make.conf file before emerging GRUB so that the package will be built with EFI functionality: root #echo 'GRUB_PLATFORMS=\"efi-64\"' >> /etc/portage/make.conf root #emerge --ask sys-boot/grub If GRUB was somehow emerged without enabling GRUB_PLATFORMS=\"efi-64\", the line (as shown above) can be added to make.conf and then dependencies for the world package set can be re-calculated by passing the --update --newuse options to emerge: root #emerge --ask --update --newuse --verbose sys-boot/grub Install \u00b6 Next, install the necessary GRUB files to the /boot/grub/ directory via the grub-install command. Presuming the first disk (the one where the system boots from) is /dev/sda, one of the following commands will do: BIOS: root #grub-install /dev/sda UEFI: Make sure the EFI system partition has been mounted before running grub-install. It is possible for grub-install to install the GRUB EFI file (grubx64.efi) into the wrong directory without providing any indication the wrong directory was used. root #grub-install --target=x86_64-efi --efi-directory=/boot Modify the --efi-directory option to the root of the EFI System Partition. This is necessary if the /boot partition was not formatted as a FAT variant. Warning If grub-install returns an error like Could not prepare Boot variable: Read-only file system, it may be necessary to remount the efivars special mount as read-write in order to succeed: root #mount -o remount,rw,nosuid,nodev,noexec --types efivarfs efivarfs /sys/firmware/efi/efivars Some motherboard manufacturers seem to only support the /efi/boot/ directory location for the .EFI file in the EFI System Partition (ESP). The GRUB installer can perform this operation automatically with the --removable option. Verify the ESP is mounted before running the following commands. Presuming the ESP is mounted at /boot (as suggested earlier), execute: root #grub-install --target=x86_64-efi --efi-directory=/boot --removable This creates the default directory defined by the UEFI specification, and then copies the grubx64.efi file to the 'default' EFI file location defined by the same specification. Configure \u00b6 Next, generate the GRUB configuration based on the user configuration specified in the /etc/default/grub file and /etc/grub.d scripts. In most cases, no configuration is needed by users as GRUB will automatically detect which kernel to boot (the highest one available in /boot/) and what the root file system is. It is also possible to append kernel parameters in /etc/default/grub using the GRUB_CMDLINE_LINUX variable. To generate the final GRUB configuration, run the grub-mkconfig command: root #grub-mkconfig -o /boot/grub/grub.cfg Generating grub.cfg ... Found linux image: /boot/vmlinuz-5.15.52-gentoo Found initrd image: /boot/initramfs-genkernel-amd64-5.15.52-gentoo done The output of the command must mention that at least one Linux image is found, as those are needed to boot the system. If an initramfs is used or genkernel was used to build the kernel, the correct initrd image should be detected as well. If this is not the case, go to /boot/ and check the contents using the ls command. If the files are indeed missing, go back to the kernel configuration and installation instructions. Tip The os-prober utility can be used in conjunction with GRUB to detect other operating systems from attached drives. Windows 7, 8.1, 10, and other distributions of Linux are detectable. Those desiring dual boot systems should emerge the sys-boot/os-prober package then re-run the grub-mkconfig command (as seen above). If detection problems are encountered be sure to read the GRUB article in its entirety before asking the Gentoo community for support. efibootmgr \u00b6 On UEFI based systems, the UEFI firmware on the system (in other words the primary bootloader), can be directly manipulated to look for UEFI boot entries. Such systems do not need to have additional (also known as secondary) bootloaders like GRUB in order to help boot the system. With that being said, the reason EFI-based bootloaders such as GRUB exist is to extend the functionality of UEFI systems during the boot process. Using efibootmgr is really for those who desire to take a minimalist (although more rigid) approach to booting their system; using GRUB (see above) is easier for the majority of users because it offers a flexible approach when booting UEFI systems. Remember sys-boot/efibootmgr application is not a bootloader; it is a tool to interact with the UEFI firmware and update its settings, so that the Linux kernel that was previously installed can be booted with additional options (if necessary), or to allow multiple boot entries. This interaction is done through the EFI variables (hence the need for kernel support of EFI vars). Be sure to read through the EFI stub kernel article before continuing. The kernel must have specific options enabled to be directly bootable by the system's UEFI firmware. It might be necessary to recompile the kernel. It is also a good idea to take a look at the efibootmgr article. To reiterate, efibootmgr is not a requirement to boot an UEFI system. The Linux kernel itself can be booted immediately, and additional kernel command-line options can be built-in to the Linux kernel (there is a kernel configuration option called CONFIG_CMDLINE that allows the user to specify boot parameters as command-line options. Even an initramfs can be 'built-in' to the kernel. root #emerge --ask sys-boot/efibootmgr Then, create the /boot/efi/boot/ location, and then copy the kernel into this location, calling it bootx64.efi: root #mkdir -p /boot/efi/boot root #cp /boot/vmlinuz-* /boot/efi/boot/bootx64.efi Next, tell the UEFI firmware that a boot entry called \"Gentoo\" is to be created, which has the freshly compiled EFI stub kernel: root #efibootmgr --create --disk /dev/sda --part 2 --label \"Gentoo\" --loader \"\\efi\\boot\\bootx64.efi\" If an initial RAM file system (initramfs) is used, add the proper boot option to it: root #efibootmgr -c -d /dev/sda -p 2 -L \"Gentoo\" -l \"\\efi\\boot\\bootx64.efi\" initrd='\\initramfs-genkernel-amd64-5.15.52-gentoo' The use of as directory separator is mandatory when using UEFI definitions. Rebooting the system \u00b6 Exit the chrooted environment and unmount all mounted partitions. Then type in that one magical command that initiates the final, true test: reboot root #exit cdimage ~#cd cdimage ~#umount -l /mnt/gentoo/dev { /shm,/pts, } cdimage ~#umount -R /mnt/gentoo cdimage ~#reboot","title":"Bootloader"},{"location":"Linux/Gentoo/Bootloader/#configuring-the-bootloader","text":"","title":"Configuring the bootloader"},{"location":"Linux/Gentoo/Bootloader/#selecting-a-boot-loader","text":"With the Linux kernel configured, system tools installed and configuration files edited, it is time to install the last important piece of a Linux installation: the boot loader. The boot loader is responsible for firing up the Linux kernel upon boot - without it, the system would not know how to proceed when the power button has been pressed. For amd64, we document how to configure either GRUB or LILO for BIOS based systems, and GRUB or efibootmgr for UEFI systems. In this section of the Handbook a delineation has been made between emerging the boot loader's package and installing a boot loader to a system disk. Here the term emerge will be used to ask Portage to make the software package available to the system. The term install will signify the boot loader copying files or physically modifying appropriate sections of the system's disk drive in order to render the boot loader activated and ready to operate on the next power cycle.","title":"Selecting a boot loader"},{"location":"Linux/Gentoo/Bootloader/#default-grub","text":"By default, the majority of Gentoo systems now rely upon GRUB (found in the sys-boot/grub package), which is the direct successor to GRUB Legacy . With no additional configuration, GRUB gladly supports older BIOS (\"pc\") systems. With a small amount of configuration, necessary before build time, GRUB can support more than a half a dozen additional platforms. For more information, consult the Prerequisites section of the GRUB article.","title":"Default: GRUB"},{"location":"Linux/Gentoo/Bootloader/#emerge","text":"Warning When using an older BIOS system supporting only MBR partition tables, no additional configuration is needed in order to emerge GRUB: root #emerge --ask --verbose sys-boot/grub Tip UEFI users : running the above command will output the enabled GRUB_PLATFORMS values before emerging. When using UEFI capable systems, users will need to ensure GRUB_PLATFORMS=\"efi-64\" is enabled (as it is the case by default). If that is not the case for the setup, GRUB_PLATFORMS=\"efi-64\" will need to be added to the /etc/portage/make.conf file before emerging GRUB so that the package will be built with EFI functionality: root #echo 'GRUB_PLATFORMS=\"efi-64\"' >> /etc/portage/make.conf root #emerge --ask sys-boot/grub If GRUB was somehow emerged without enabling GRUB_PLATFORMS=\"efi-64\", the line (as shown above) can be added to make.conf and then dependencies for the world package set can be re-calculated by passing the --update --newuse options to emerge: root #emerge --ask --update --newuse --verbose sys-boot/grub","title":"Emerge"},{"location":"Linux/Gentoo/Bootloader/#install","text":"Next, install the necessary GRUB files to the /boot/grub/ directory via the grub-install command. Presuming the first disk (the one where the system boots from) is /dev/sda, one of the following commands will do: BIOS: root #grub-install /dev/sda UEFI: Make sure the EFI system partition has been mounted before running grub-install. It is possible for grub-install to install the GRUB EFI file (grubx64.efi) into the wrong directory without providing any indication the wrong directory was used. root #grub-install --target=x86_64-efi --efi-directory=/boot Modify the --efi-directory option to the root of the EFI System Partition. This is necessary if the /boot partition was not formatted as a FAT variant. Warning If grub-install returns an error like Could not prepare Boot variable: Read-only file system, it may be necessary to remount the efivars special mount as read-write in order to succeed: root #mount -o remount,rw,nosuid,nodev,noexec --types efivarfs efivarfs /sys/firmware/efi/efivars Some motherboard manufacturers seem to only support the /efi/boot/ directory location for the .EFI file in the EFI System Partition (ESP). The GRUB installer can perform this operation automatically with the --removable option. Verify the ESP is mounted before running the following commands. Presuming the ESP is mounted at /boot (as suggested earlier), execute: root #grub-install --target=x86_64-efi --efi-directory=/boot --removable This creates the default directory defined by the UEFI specification, and then copies the grubx64.efi file to the 'default' EFI file location defined by the same specification.","title":"Install"},{"location":"Linux/Gentoo/Bootloader/#configure","text":"Next, generate the GRUB configuration based on the user configuration specified in the /etc/default/grub file and /etc/grub.d scripts. In most cases, no configuration is needed by users as GRUB will automatically detect which kernel to boot (the highest one available in /boot/) and what the root file system is. It is also possible to append kernel parameters in /etc/default/grub using the GRUB_CMDLINE_LINUX variable. To generate the final GRUB configuration, run the grub-mkconfig command: root #grub-mkconfig -o /boot/grub/grub.cfg Generating grub.cfg ... Found linux image: /boot/vmlinuz-5.15.52-gentoo Found initrd image: /boot/initramfs-genkernel-amd64-5.15.52-gentoo done The output of the command must mention that at least one Linux image is found, as those are needed to boot the system. If an initramfs is used or genkernel was used to build the kernel, the correct initrd image should be detected as well. If this is not the case, go to /boot/ and check the contents using the ls command. If the files are indeed missing, go back to the kernel configuration and installation instructions. Tip The os-prober utility can be used in conjunction with GRUB to detect other operating systems from attached drives. Windows 7, 8.1, 10, and other distributions of Linux are detectable. Those desiring dual boot systems should emerge the sys-boot/os-prober package then re-run the grub-mkconfig command (as seen above). If detection problems are encountered be sure to read the GRUB article in its entirety before asking the Gentoo community for support.","title":"Configure"},{"location":"Linux/Gentoo/Bootloader/#efibootmgr","text":"On UEFI based systems, the UEFI firmware on the system (in other words the primary bootloader), can be directly manipulated to look for UEFI boot entries. Such systems do not need to have additional (also known as secondary) bootloaders like GRUB in order to help boot the system. With that being said, the reason EFI-based bootloaders such as GRUB exist is to extend the functionality of UEFI systems during the boot process. Using efibootmgr is really for those who desire to take a minimalist (although more rigid) approach to booting their system; using GRUB (see above) is easier for the majority of users because it offers a flexible approach when booting UEFI systems. Remember sys-boot/efibootmgr application is not a bootloader; it is a tool to interact with the UEFI firmware and update its settings, so that the Linux kernel that was previously installed can be booted with additional options (if necessary), or to allow multiple boot entries. This interaction is done through the EFI variables (hence the need for kernel support of EFI vars). Be sure to read through the EFI stub kernel article before continuing. The kernel must have specific options enabled to be directly bootable by the system's UEFI firmware. It might be necessary to recompile the kernel. It is also a good idea to take a look at the efibootmgr article. To reiterate, efibootmgr is not a requirement to boot an UEFI system. The Linux kernel itself can be booted immediately, and additional kernel command-line options can be built-in to the Linux kernel (there is a kernel configuration option called CONFIG_CMDLINE that allows the user to specify boot parameters as command-line options. Even an initramfs can be 'built-in' to the kernel. root #emerge --ask sys-boot/efibootmgr Then, create the /boot/efi/boot/ location, and then copy the kernel into this location, calling it bootx64.efi: root #mkdir -p /boot/efi/boot root #cp /boot/vmlinuz-* /boot/efi/boot/bootx64.efi Next, tell the UEFI firmware that a boot entry called \"Gentoo\" is to be created, which has the freshly compiled EFI stub kernel: root #efibootmgr --create --disk /dev/sda --part 2 --label \"Gentoo\" --loader \"\\efi\\boot\\bootx64.efi\" If an initial RAM file system (initramfs) is used, add the proper boot option to it: root #efibootmgr -c -d /dev/sda -p 2 -L \"Gentoo\" -l \"\\efi\\boot\\bootx64.efi\" initrd='\\initramfs-genkernel-amd64-5.15.52-gentoo' The use of as directory separator is mandatory when using UEFI definitions.","title":"efibootmgr"},{"location":"Linux/Gentoo/Bootloader/#rebooting-the-system","text":"Exit the chrooted environment and unmount all mounted partitions. Then type in that one magical command that initiates the final, true test: reboot root #exit cdimage ~#cd cdimage ~#umount -l /mnt/gentoo/dev { /shm,/pts, } cdimage ~#umount -R /mnt/gentoo cdimage ~#reboot","title":"Rebooting the system"},{"location":"Linux/Gentoo/Finalizing/","text":"Finalizing \u00b6 User administration \u00b6 Adding a user for daily use \u00b6 Group Description audio Be able to access the audio devices. cdrom Be able to directly access optical devices. floppy Be able to directly access floppy devices. games Be able to play games. portage Be able to access portage restricted resources. usb Be able to access USB devices. video Be able to access video capturing hardware and doing hardware acceleration. wheel Be able to use su. For instance, to create a user called larry who is member of the wheel, users, and audio groups, log in as root first (only root can create users) and run useradd: Login:root Password: ( Enter the root password ) root #useradd -m -G users,wheel,audio -s /bin/bash larry root #passwd larry Password: ( Enter the password for larry ) Re-enter password: ( Re-enter the password to verify ) Disk cleanup \u00b6 Removing tarballs \u00b6 With the Gentoo installation finished and the system rebooted, if everything has gone well, we can now remove the downloaded stage3 tarball from the hard disk. Remember that they were downloaded to the / directory. root #rm /stage3-*.tar.*","title":"Finalizing"},{"location":"Linux/Gentoo/Finalizing/#finalizing","text":"","title":"Finalizing"},{"location":"Linux/Gentoo/Finalizing/#user-administration","text":"","title":"User administration"},{"location":"Linux/Gentoo/Finalizing/#adding-a-user-for-daily-use","text":"Group Description audio Be able to access the audio devices. cdrom Be able to directly access optical devices. floppy Be able to directly access floppy devices. games Be able to play games. portage Be able to access portage restricted resources. usb Be able to access USB devices. video Be able to access video capturing hardware and doing hardware acceleration. wheel Be able to use su. For instance, to create a user called larry who is member of the wheel, users, and audio groups, log in as root first (only root can create users) and run useradd: Login:root Password: ( Enter the root password ) root #useradd -m -G users,wheel,audio -s /bin/bash larry root #passwd larry Password: ( Enter the password for larry ) Re-enter password: ( Re-enter the password to verify )","title":"Adding a user for daily use"},{"location":"Linux/Gentoo/Finalizing/#disk-cleanup","text":"","title":"Disk cleanup"},{"location":"Linux/Gentoo/Finalizing/#removing-tarballs","text":"With the Gentoo installation finished and the system rebooted, if everything has gone well, we can now remove the downloaded stage3 tarball from the hard disk. Remember that they were downloaded to the / directory. root #rm /stage3-*.tar.*","title":"Removing tarballs"},{"location":"Linux/Gentoo/Introduction/","text":"How to install Gentoo with EFI boot loader and OpenRC sysControl \u00b6 AMD64 Handbook \u00b6 How the installation is structured \u00b6 The Gentoo Installation can be seen as a 10-step procedure. Each step results in a certain state: Step Step Result 1 The user is in a working environment ready to install Gentoo. 2 The Internet connection is ready to install Gentoo. 3 The hard disks are initialized to host the Gentoo installation. 4 The installation environment is prepared and the user is ready to chroot into the new environment. 5 Core packages, which are the same on all Gentoo installations, are installed. 6 The Linux kernel is installed. 7 Most of the Gentoo system configuration files are created. 8 The necessary system tools are installed. 9 The proper boot loader has been installed and configured. 10 The freshly installed Gentoo Linux environment is ready to be explored.","title":"Introduction"},{"location":"Linux/Gentoo/Introduction/#how-to-install-gentoo-with-efi-boot-loader-and-openrc-syscontrol","text":"","title":"How to install Gentoo with EFI boot loader and OpenRC sysControl"},{"location":"Linux/Gentoo/Introduction/#amd64-handbook","text":"","title":"AMD64 Handbook"},{"location":"Linux/Gentoo/Introduction/#how-the-installation-is-structured","text":"The Gentoo Installation can be seen as a 10-step procedure. Each step results in a certain state: Step Step Result 1 The user is in a working environment ready to install Gentoo. 2 The Internet connection is ready to install Gentoo. 3 The hard disks are initialized to host the Gentoo installation. 4 The installation environment is prepared and the user is ready to chroot into the new environment. 5 Core packages, which are the same on all Gentoo installations, are installed. 6 The Linux kernel is installed. 7 Most of the Gentoo system configuration files are created. 8 The necessary system tools are installed. 9 The proper boot loader has been installed and configured. 10 The freshly installed Gentoo Linux environment is ready to be explored.","title":"How the installation is structured"},{"location":"Linux/Gentoo/Kernel/","text":"Configuring the kernel \u00b6 Installing firmware and/or microcode \u00b6 Firmware \u00b6 Before getting to configuring kernel sections, it is beneficial to be aware that some hardware devices require additional, sometimes non-FOSS compliant, firmware to be installed on the system before they will operate correctly. This is often the case for wireless network interfaces commonly found in both desktop and laptop computers. Modern video chips from vendors like AMD, Nvidia, and Intel, often also require external firmware files to be fully functional. Most firmware for modern hardware devices can be found within the sys-kernel/linux-firmware package. This is the recommendation to have to be installed before the initial system reboot. root #emerge --ask sys-kernel/linux-firmware Installing certain firmware packages often requires accepting the associated firmware licenses. If necessary, visit the license handling section of the Handbook for help on accepting licenses. Microcode \u00b6 In addition to discrete graphics hardware and network interfaces, CPUs also can require firmware updates. Typically this kind of firmware is referred to as microcode. Newer revisions of microcode are sometimes necessary to patch instability, security concerns, or other miscellaneous bugs in CPU hardware. Microcode updates for AMD CPUs are distributed within the aforementioned sys-kernel/linux-firmware package. Microcode for Intel CPUs can be found within the sys-firmware/intel-microcode package, which will need to be installed separately. See the Microcode article for more information on how to apply microcode updates. Kernel configuration and compilation \u00b6 Full automation approach: Distribution kernels Hybrid approach: Genkernel Full manual approach Distribution kernels Distribution kernels \u00b6 Distribution Kernels are ebuilds that cover the complete process of unpacking, configuring, compiling, and installing the kernel. The primary advantage of this method is that the kernels are updated to new versions by the package manager as part of @world upgrade. This requires no more involvement than running an emerge command. Distribution kernels default to a configuration supporting the majority of hardware, however two mechanisms are offered for customization: savedconfig and config snippets. See the project page for more details on configuration. Installing the correct installkernel package \u00b6 # systemd-boot as the bootloader root #emerge --ask sys-kernel/installkernel-systemd-boot # Grub,LILO,etc traditional layout root #emerge --ask sys-kernel/installkernel-gentoo Install a distribution kernel \u00b6 # build a kernel with Gentoo patches form source root #emerge --ask sys-kernel/gentoo-kernel Upgrading and cleaning up \u00b6 Once the kernel is installed, the package manager will automatically update it to newer versions. The previous versions will be kept until the package manager is requested to clean up stale packages. To reclaim disk space, stale packages can be trimmed by periodically running emerge with the --depclean option: root #emerge --depclean # clean up old kernel versions as specifically emerge --prune sys-kernel/gentoo-kernel sys-kernel/gentoo-kernel-bin Post-install/upgrade tasks \u00b6 Distribution kernels are capable of rebuilding kernel modules installed by other packages. linux-mod.eclass provides the dist-kernel USE flag which controls a subslot dependency on virtual/dist-kernel . Enabling this USE flag on packages like sys-fs/zfs and sys-fs/zfs-kmod allows them to automatically be rebuilt against a newly updated kernel and, if applicable, will re-generate the initramfs accordingly. Manually rebuilding the initramfs If required, manually trigger such rebuilds by, after a kernel upgrade, executing: root #emerge --ask @module-rebuild If any kernel modules (e.g. ZFS) are needed at early boot, rebuild the initramfs afterward via: root #emerge --config sys-kernel/gentoo-kernel root #emerge --config sys-kernel/gentoo-kernel-bin Installing the kernel sources \u00b6 This section is only relevant when using the following genkernel (hybrid) or manual kernel management approach. When installing and compiling the kernel for amd64-based systems, Gentoo recommends the sys-kernel/gentoo-sources package. Choose an appropriate kernel source and install it using emerge: root #emerge --ask sys-kernel/gentoo-sources This will install the Linux kernel sources in /usr/src/ using the specific kernel version in the path. It will not create a symbolic link by itself without USE=symlink being enabled on the chosen kernel sources package. It is conventional for a /usr/src/linux symlink to be maintained, such that it refers to whichever sources correspond with the currently running kernel. However, this symbolic link will not be created by default. An easy way to create the symbolic link is to utilize eselect's kernel module. For further information regarding the purpose of the symlink, and how to manage it, please refer to Kernel/Upgrade. First, list all installed kernels: root #eselect kernel list Available kernel symlink targets: [ 1 ] linux-5.15.52-gentoo In order to create a symbolic link called linux, use: eselect kernel set 1 root #ls -l /usr/src/linux lrwxrwxrwx 1 root root 12 Oct 13 11 :04 /usr/src/linux -> linux-5.15.52-gentoo Genkernel \u00b6 If an entirely manual configuration looks too daunting, system administrators should consider using genkernel as a hybrid approach to kernel maintenance. Genkernel provides a generic kernel configuration file, automatically generates the kernel, initramfs, and associated modules, and then installs the resulting binaries to the appropriate locations. This results in minimal and generic hardware support for the system's first boot, and allows for additional update control and customization of the kernel's configuration in the future. While using genkernel to maintain the kernel provides system administrators with more update control over the system's kernel, initramfs, and other options, it will require a time and effort commitment to perform future kernel updates as new sources are released. Those looking for a hands-off approach to kernel maintenance should use a distribution kernel. Misconception to believe genkernel automatically generates a custom kernel configuration for the hardware on which it is run; It uses a predetermined kernel configuration that supports most generic hardware and automatically handles the make commands necessary to assemble and install the kernel, the associate modules, and the initramfs file. As a prerequisite, due to the firwmare USE flag being enabled by default for the sys-kernel/genkernel package, the package manager will also attempt to pull in the sys-kernel/linux-firmware package. The binary redistributable software licenses are required to be accepted before the linux-firmware will install. This license group can be accepted system-wide for any package by adding the @BINARY-REDISTRIBUTABLE as an ACCEPT_LICENSE value in the /etc/portage/make.conf file. It can be exclusively accepted for the linux-firmware package by adding a specific inclusion via a /etc/portage/package.license/linux-firmware file. If necessary, review the methods of accepting software licenses available in the Installing the base system chapter of the handbook, then make some changes for acceptable software licenses. If in analysis paralysis, the following will do the trick: root #mkdir /etc/portage/package.license /etc/portage/package.license/linux-firmware Accept binary redistributable licenses for the linux-firmware package sys-kernel/linux-firmware @BINARY-REDISTRIBUTABLE Installation \u00b6 Explanations and prerequisites aside, install the sys-kernel/genkernel package: Generation \u00b6 Compile the kernel sources by running genkernel all. Be aware though, as genkernel compiles a kernel that supports a wide array of hardware for differing computer architectures, this compilation may take quite a while to finish. If the root partition/volume uses a filesystem other than ext4, it may be necessary to manually configure the kernel using genkernel --menuconfig all to add built-in kernel support for the particular filesystem(s) (i.e. not building the filesystem as a module). Users of LVM2 should add --lvm as an argument to the genkernel command below. root #genkernel --mountboot --install all Once genkernel completes, a kernel and an initial ram filesystem (initramfs) will be generated and installed into the /boot directory. Associated modules will be installed into the /lib/modules directory. The initramfs will be started immediately after loading the kernel to perform hardware auto-detection (just like in the live disk image environments). root #ls /boot/vmlinu* /boot/initramfs* root #ls /lib/modules Manual configuration \u00b6 Introduction \u00b6 Manually configuring a kernel is often seen as the most difficult procedure a Linux user ever has to perform. Nothing is less true - after configuring a couple of kernels no one remembers that it was difficult! However, one thing is true: it is vital to know the system when a kernel is configured manually. Most information can be gathered by emerging sys-apps/pciutils which contains the lspci command: root #emerge --ask sys-apps/pciutils Inside the chroot, it is safe to ignore any pcilib warnings (like pcilib: cannot open /sys/bus/pci/devices ) that lspci might throw out. Another source of system information is to run lsmod to see what kernel modules the installation CD uses as it might provide a nice hint on what to enable. Now go to the kernel source directory and execute make menuconfig. This will fire up menu-driven configuration screen. root #cd /usr/src/linux root #make menuconfig Gentoo kernel configuration guide The Linux kernel configuration has many, many sections. Let's first list some options that must be activated (otherwise Gentoo will not function, or not function properly without additional tweaks). We also have a Gentoo kernel configuration guide on the Gentoo wiki that might help out further. Enabling required options \u00b6 When using sys-kernel/gentoo-sources, it is strongly recommend the Gentoo-specific configuration options be enabled. These ensure that a minimum of kernel features required for proper functioning is available: Enable Gentoo-specific options KERNEL Gentoo Linux ---> Generic Driver Options ---> [*] Gentoo Linux support [*] Linux dynamic and persistent device naming (userspace devfs) support [*] Select options required by Portage features Support for init systems, system and service manager ---> [*] OpenRC, runit and other script based systems and managers. [*] systemd Naturally the choice in the last two lines depends on the selected init system ( OpenRC vs. systemd ). It does not hurt to have support for both init systems enabled. Enabling support for typical system components \u00b6 Make sure that every driver that is vital to the booting of the system (such as SATA controllers, NVMe block device support, filesystem support, etc.) is compiled in the kernel and not as a module, otherwise the system may not be able to boot completely. Next select the exact processor type. It is also recommended to enable MCE features (if available) so that users are able to be notified of any hardware problems. On some architectures (such as x86_64), these errors are not printed to dmesg , but to /dev/mcelog . This requires the app-admin/mcelog package. Also select Maintain a devtmpfs file system to mount at /dev so that critical device files are already available early in the boot process (CONFIG_DEVTMPFS and CONFIG_DEVTMPFS_MOUNT) : Enable devtmpfs support(_CONFIG_DEVTMPFS) KERNEL Device Driver ---> Generic Driver Options ---> [*] Maintain a devtmpfs filesystem to mount at /dev [*] Automount devtmpfs at /dev, after the kernel mounted the rootfs Verify SCSI disk support has been activated \u0013(CONFIG_BLK_DEV_SD) : Enabling SCSI disk support (CONFIG_SCSI, CONFIG_BLK_DEV_SD) KERNEL Device Driver ---> SCSI device support ---> <*> SCSI device support <*> SCSI disk support Enabling basic SATA and PATA support (CONFIG_ATA_ACPI, CONFIG_SATA_PMP, CONFIG_SATA_AHCI, CONFIG_ATA_BMDMA, CONFIG_ATA_SFF, CONFIG_ATA_PIIX) KERNEL Device Driver ---> <*> Serial ATA and Parallel ATA drivers (libata) ---> [*] ATA ACPI Support [*] SATA Poart Multiplier support <*> AHCI SATA support (achi) [*] ATA BMDMA support [*] ATA SFF support (for legacy IDE and PATA) <*> Intel ESB, ICH, PIIX3, PIIX4 PATA/SATA support (ata_piix) Verify basic NVMe support has been enabled: Enable basic NVMe support for Linux 4.4.x (CONFIG_BLK_DEV_NVME) KERNEL Device Driver ---> <*> NVM Express block device Enable basic NVMe support for Linux 5.x.x (CONFIG_DEVTMPFS) KERNEL Device Driver ---> NVME support ---> <*> NVM Express block device It does not hurt to enable the following additional NVMe support: Enabling additional NVMe support (CONFIG_NVME_MULTIPATH, CONFIG_NVME_MULTIPATH, CONFIG_NVME_HWMON, CONFIG_NVME_FC, CONFIG_NVME_TCP, CONFIG_NVME_TARGET, CONFIG_NVME_TARGET_PASSTHRU, CONFIG_NVME_TARGET_LOOP, CONFIG_NVME_TARGET_FC, CONFIG_NVME_TARGET_FCLOOP, CONFIG_NVME_TARGET_TCP KERNEL [*] NVMe multipath support [*] NVME hardware monitoring <M> NVM Express over Fabrics FC host driver <M> NVM Express over Fabrics TCP host driver <M> NVM Traget support [*] NVMe Target Passthrough support <M> NVMe loopback device support <M> NVMe over Fabrics FC target driver < > NVMe over Fabrics FC Transport Loopback Test driver (NEW) <M> NVMe over Fabrics TCP target driver Now go to File Systems and select support for the filesystems that will be used by the system. Do not compile the file system that is used for the root filesystem as module, otherwise the system may not be able to mount the partition. Also select Virtual memory and /proc file system. Select one or more of the following options as needed by the system: Enable file system support (CONFIG_EXT2_FS, CONFIG_EXT3_FS, CONFIG_EXT4_FS, CONFIG_BTRFS_FS, CONFIG_MSDOS_FS, CONFIG_VFAT_FS, CONFIG_PROC_FS, and CONFIG_TMPFS) KERNEL File systems ---> <*> Second extended fs support <*> The Extended 3 (ext3) filesystem <*> The Extended 4 (ext3) filesystem <*> Btrfs filesystem support DOS/FAT/NT Filesystems ---> <*> MSDOS fs support <*> VFAT (Windows-95) fs support Pseudo Filesystems ---> [*] /porc file system support [*] Tmpfs virtual memory file system support (former shm fs) Most systems also have multiple cores at their disposal, so it is important to activate Symmetric multi-processing support (CONFIG_SMP): Activating SMP support (CONFIG_SMP) KERNEL Processor type and features ---> [*] Symmetric multi-processing support If USB input devices (like keyboard or mouse) or other USB devices will be used, do not forget to enable those as well: Enable USB and human input device support (CONFIG_HID_GENERIC, CONFIG_USB_HID, CONFIG_USB_SUPPORT, CONFIG_USB_XHCI_HCD, CONFIG_USB_EHCI_HCD, CONFIG_USB_OHCI_HCD, (CONFIG_HID_GENERIC, CONFIG_USB_HID, CONFIG_USB_SUPPORT, CONFIG_USB_XHCI_HCD, CONFIG_USB_EHCI_HCD, CONFIG_USB_OHCI_HCD, CONFIG_USB4) KERNEL Device Drivers ---> HID support ---> -*- HID bus support <*> Generic HID driver [*] Battery level reporting for HID devices USB HID suport ---> <*> USB HID transport layer [*] USB support ---> <*> xHCI HCD (USB 3.0) support <*> EHCI HCD (USB 2.0) support <*> OHCI HCD (USB 1.1) support <*> Unified support for USB4 and Thunderbolt ---> If PPPoE is used to connect to the Internet, or a dial-up modem, then enable the following options (CONFIG_PPP, CONFIG_PPP_ASYNC, and CONFIG_PPP_SYNC_TTY) : Enabling PPPoE support (PPPoE, CONFIG_PPPOE, CONFIG_PPP_ASYNC, CONFIG_PPP_SYNC_TTY KERNEL Device Driver ---> Network device support ---> <*> PPP (point-to-point protocol) support <*> PPP over Ethernet <*> PPP support for async serial ports <*> PPP support for sync tty point Architecture specific kernel configuration \u00b6 Enable GPT partition label support if that was used previously when partitioning the disk (CONFIG_PARTITION_ADVANCED and CONFIG_EFI_PARTITION): Enable support for GPT KERNEL _*_ Enable the block layer ---> Partition Types ---> [*] Advanced partition selection [*] EFI GUID Partition support Enable EFI stub support, EFI variables and EFI Framebuffer in the Linux kernel if UEFI is used to boot the system (CONFIG_EFI, CONFIG_EFI_STUB, CONFIG_EFI_MIXED, CONFIG_EFI_VARS, and CONFIG_FB_EFI) : Enable support for UEFI KERNEL Processor type and features ---> [*] EFI runtime service support [*] EFI stub support [*] EFI mixed-mode support Device Drivers Firmware Drivers ---> EFI (Extension Firmware Interface) Support ---> <*> EFI Veriable Support via sysfs Graphics support ---> Frame buffer Devices ---> <*> Support for frame buffer devices ---> [*] EFI-based Framebuffer Support Compiling and installing \u00b6 With the configuration now done, it is time to compile and install the kernel. Exit the configuration and start the compilation process: root #make && make modules_install It is possible to enable parallel builds using make -jX with X being an integer number of parallel tasks that the build process is allowed to launch. This is similar to the instructions about /etc/portage/make.conf earlier, with the MAKEOPTS variable. When the kernel has finished compiling, copy the kernel image to /boot/. This is handled by the make install command: root #make install Optional: Building an initramfs \u00b6 In certain cases it is necessary to build an initramfs - an initial ram-based file system. The most common reason is when important file system locations (like /usr/ or /var/) are on separate partitions. With an initramfs, these partitions can be mounted using the tools available inside the initramfs. Without an initramfs, there is a risk that the system will not boot properly as the tools that are responsible for mounting the file systems require information that resides on unmounted file systems. An initramfs will pull in the necessary files into an archive which is used right after the kernel boots, but before the control is handed over to the init tool. Scripts on the initramfs will then make sure that the partitions are properly mounted before the system continues booting. Important If using genkernel, it should be used for both building the kernel and the initramfs. When using genkernel only for generating an initramfs, it is crucial to pass --kernel-config=/path/to/kernel.config to genkernel or the generated initramfs may not work with a manually built kernel. Note that manually built kernels go beyond the scope of support for the handbook. See the kernel configuration article for more information. To install an initramfs, install sys-kernel/dracut first, then have it generate an initramfs: root #emerge --ask sys-kernel/dracut root #dracut --kver=5.15.52-gentoo The initramfs will be stored in /boot/. The resulting file can be found by simply listing the files starting with initramfs: root #ls /boot/initramfs* Kernel modules \u00b6 Listing available kernel modules \u00b6 Hardware modules are optional to be listed manually. udev will normally load all hardware modules that are detected to be connected in most cases. However, it is not harmful for modules that will be automatically loaded to be listed. Modules cannot be loaded twice; they are either loaded or unloaded. Sometimes exotic hardware requires help to load their drivers. The modules that need to be loaded during each boot in can be added to /etc/modules-load.d/ .conf files in the format of one module per line. When extra options are needed for the modules, they should be set in /etc/modprobe.d/ .conf files instead. To view all modules available for a specific kernel version, issue the following find command. Do not forget to substitute \" \" with the appropriate version of the kernel to search: root #find /lib/modules/<kernel version>/ -type f -iname '*.o' -or -iname '*.ko' | less Force loading particular kernel modules \u00b6 To force load the kernel to load the 3c59x.ko module (which is the driver for a specific 3Com network card family), edit the /etc/modules-load.d/network.conf file and enter the module name within it. root #mkdir -p /etc/modules-load.d root #nano -w /etc/modules-load.d/network.conf Note that the module's .ko file suffix is insignificant to the loading mechanism and left out of the configuration file: Force loading 3c59x module FILE /etc/modules-load.d/network.conf 3c59x","title":"Kernel"},{"location":"Linux/Gentoo/Kernel/#configuring-the-kernel","text":"","title":"Configuring the kernel"},{"location":"Linux/Gentoo/Kernel/#installing-firmware-andor-microcode","text":"","title":"Installing firmware and/or microcode"},{"location":"Linux/Gentoo/Kernel/#firmware","text":"Before getting to configuring kernel sections, it is beneficial to be aware that some hardware devices require additional, sometimes non-FOSS compliant, firmware to be installed on the system before they will operate correctly. This is often the case for wireless network interfaces commonly found in both desktop and laptop computers. Modern video chips from vendors like AMD, Nvidia, and Intel, often also require external firmware files to be fully functional. Most firmware for modern hardware devices can be found within the sys-kernel/linux-firmware package. This is the recommendation to have to be installed before the initial system reboot. root #emerge --ask sys-kernel/linux-firmware Installing certain firmware packages often requires accepting the associated firmware licenses. If necessary, visit the license handling section of the Handbook for help on accepting licenses.","title":"Firmware"},{"location":"Linux/Gentoo/Kernel/#microcode","text":"In addition to discrete graphics hardware and network interfaces, CPUs also can require firmware updates. Typically this kind of firmware is referred to as microcode. Newer revisions of microcode are sometimes necessary to patch instability, security concerns, or other miscellaneous bugs in CPU hardware. Microcode updates for AMD CPUs are distributed within the aforementioned sys-kernel/linux-firmware package. Microcode for Intel CPUs can be found within the sys-firmware/intel-microcode package, which will need to be installed separately. See the Microcode article for more information on how to apply microcode updates.","title":"Microcode"},{"location":"Linux/Gentoo/Kernel/#kernel-configuration-and-compilation","text":"Full automation approach: Distribution kernels Hybrid approach: Genkernel Full manual approach","title":"Kernel configuration and compilation"},{"location":"Linux/Gentoo/Kernel/#distribution-kernels-distribution-kernels","text":"Distribution Kernels are ebuilds that cover the complete process of unpacking, configuring, compiling, and installing the kernel. The primary advantage of this method is that the kernels are updated to new versions by the package manager as part of @world upgrade. This requires no more involvement than running an emerge command. Distribution kernels default to a configuration supporting the majority of hardware, however two mechanisms are offered for customization: savedconfig and config snippets. See the project page for more details on configuration.","title":"Distribution kernels Distribution kernels"},{"location":"Linux/Gentoo/Kernel/#installing-the-correct-installkernel-package","text":"# systemd-boot as the bootloader root #emerge --ask sys-kernel/installkernel-systemd-boot # Grub,LILO,etc traditional layout root #emerge --ask sys-kernel/installkernel-gentoo","title":"Installing the correct installkernel package"},{"location":"Linux/Gentoo/Kernel/#install-a-distribution-kernel","text":"# build a kernel with Gentoo patches form source root #emerge --ask sys-kernel/gentoo-kernel","title":"Install a distribution kernel"},{"location":"Linux/Gentoo/Kernel/#upgrading-and-cleaning-up","text":"Once the kernel is installed, the package manager will automatically update it to newer versions. The previous versions will be kept until the package manager is requested to clean up stale packages. To reclaim disk space, stale packages can be trimmed by periodically running emerge with the --depclean option: root #emerge --depclean # clean up old kernel versions as specifically emerge --prune sys-kernel/gentoo-kernel sys-kernel/gentoo-kernel-bin","title":"Upgrading and cleaning up"},{"location":"Linux/Gentoo/Kernel/#post-installupgrade-tasks","text":"Distribution kernels are capable of rebuilding kernel modules installed by other packages. linux-mod.eclass provides the dist-kernel USE flag which controls a subslot dependency on virtual/dist-kernel . Enabling this USE flag on packages like sys-fs/zfs and sys-fs/zfs-kmod allows them to automatically be rebuilt against a newly updated kernel and, if applicable, will re-generate the initramfs accordingly. Manually rebuilding the initramfs If required, manually trigger such rebuilds by, after a kernel upgrade, executing: root #emerge --ask @module-rebuild If any kernel modules (e.g. ZFS) are needed at early boot, rebuild the initramfs afterward via: root #emerge --config sys-kernel/gentoo-kernel root #emerge --config sys-kernel/gentoo-kernel-bin","title":"Post-install/upgrade tasks"},{"location":"Linux/Gentoo/Kernel/#installing-the-kernel-sources","text":"This section is only relevant when using the following genkernel (hybrid) or manual kernel management approach. When installing and compiling the kernel for amd64-based systems, Gentoo recommends the sys-kernel/gentoo-sources package. Choose an appropriate kernel source and install it using emerge: root #emerge --ask sys-kernel/gentoo-sources This will install the Linux kernel sources in /usr/src/ using the specific kernel version in the path. It will not create a symbolic link by itself without USE=symlink being enabled on the chosen kernel sources package. It is conventional for a /usr/src/linux symlink to be maintained, such that it refers to whichever sources correspond with the currently running kernel. However, this symbolic link will not be created by default. An easy way to create the symbolic link is to utilize eselect's kernel module. For further information regarding the purpose of the symlink, and how to manage it, please refer to Kernel/Upgrade. First, list all installed kernels: root #eselect kernel list Available kernel symlink targets: [ 1 ] linux-5.15.52-gentoo In order to create a symbolic link called linux, use: eselect kernel set 1 root #ls -l /usr/src/linux lrwxrwxrwx 1 root root 12 Oct 13 11 :04 /usr/src/linux -> linux-5.15.52-gentoo","title":"Installing the kernel sources"},{"location":"Linux/Gentoo/Kernel/#genkernel","text":"If an entirely manual configuration looks too daunting, system administrators should consider using genkernel as a hybrid approach to kernel maintenance. Genkernel provides a generic kernel configuration file, automatically generates the kernel, initramfs, and associated modules, and then installs the resulting binaries to the appropriate locations. This results in minimal and generic hardware support for the system's first boot, and allows for additional update control and customization of the kernel's configuration in the future. While using genkernel to maintain the kernel provides system administrators with more update control over the system's kernel, initramfs, and other options, it will require a time and effort commitment to perform future kernel updates as new sources are released. Those looking for a hands-off approach to kernel maintenance should use a distribution kernel. Misconception to believe genkernel automatically generates a custom kernel configuration for the hardware on which it is run; It uses a predetermined kernel configuration that supports most generic hardware and automatically handles the make commands necessary to assemble and install the kernel, the associate modules, and the initramfs file. As a prerequisite, due to the firwmare USE flag being enabled by default for the sys-kernel/genkernel package, the package manager will also attempt to pull in the sys-kernel/linux-firmware package. The binary redistributable software licenses are required to be accepted before the linux-firmware will install. This license group can be accepted system-wide for any package by adding the @BINARY-REDISTRIBUTABLE as an ACCEPT_LICENSE value in the /etc/portage/make.conf file. It can be exclusively accepted for the linux-firmware package by adding a specific inclusion via a /etc/portage/package.license/linux-firmware file. If necessary, review the methods of accepting software licenses available in the Installing the base system chapter of the handbook, then make some changes for acceptable software licenses. If in analysis paralysis, the following will do the trick: root #mkdir /etc/portage/package.license /etc/portage/package.license/linux-firmware Accept binary redistributable licenses for the linux-firmware package sys-kernel/linux-firmware @BINARY-REDISTRIBUTABLE","title":"Genkernel"},{"location":"Linux/Gentoo/Kernel/#installation","text":"Explanations and prerequisites aside, install the sys-kernel/genkernel package:","title":"Installation"},{"location":"Linux/Gentoo/Kernel/#generation","text":"Compile the kernel sources by running genkernel all. Be aware though, as genkernel compiles a kernel that supports a wide array of hardware for differing computer architectures, this compilation may take quite a while to finish. If the root partition/volume uses a filesystem other than ext4, it may be necessary to manually configure the kernel using genkernel --menuconfig all to add built-in kernel support for the particular filesystem(s) (i.e. not building the filesystem as a module). Users of LVM2 should add --lvm as an argument to the genkernel command below. root #genkernel --mountboot --install all Once genkernel completes, a kernel and an initial ram filesystem (initramfs) will be generated and installed into the /boot directory. Associated modules will be installed into the /lib/modules directory. The initramfs will be started immediately after loading the kernel to perform hardware auto-detection (just like in the live disk image environments). root #ls /boot/vmlinu* /boot/initramfs* root #ls /lib/modules","title":"Generation"},{"location":"Linux/Gentoo/Kernel/#manual-configuration","text":"","title":"Manual configuration"},{"location":"Linux/Gentoo/Kernel/#introduction","text":"Manually configuring a kernel is often seen as the most difficult procedure a Linux user ever has to perform. Nothing is less true - after configuring a couple of kernels no one remembers that it was difficult! However, one thing is true: it is vital to know the system when a kernel is configured manually. Most information can be gathered by emerging sys-apps/pciutils which contains the lspci command: root #emerge --ask sys-apps/pciutils Inside the chroot, it is safe to ignore any pcilib warnings (like pcilib: cannot open /sys/bus/pci/devices ) that lspci might throw out. Another source of system information is to run lsmod to see what kernel modules the installation CD uses as it might provide a nice hint on what to enable. Now go to the kernel source directory and execute make menuconfig. This will fire up menu-driven configuration screen. root #cd /usr/src/linux root #make menuconfig Gentoo kernel configuration guide The Linux kernel configuration has many, many sections. Let's first list some options that must be activated (otherwise Gentoo will not function, or not function properly without additional tweaks). We also have a Gentoo kernel configuration guide on the Gentoo wiki that might help out further.","title":"Introduction"},{"location":"Linux/Gentoo/Kernel/#enabling-required-options","text":"When using sys-kernel/gentoo-sources, it is strongly recommend the Gentoo-specific configuration options be enabled. These ensure that a minimum of kernel features required for proper functioning is available: Enable Gentoo-specific options KERNEL Gentoo Linux ---> Generic Driver Options ---> [*] Gentoo Linux support [*] Linux dynamic and persistent device naming (userspace devfs) support [*] Select options required by Portage features Support for init systems, system and service manager ---> [*] OpenRC, runit and other script based systems and managers. [*] systemd Naturally the choice in the last two lines depends on the selected init system ( OpenRC vs. systemd ). It does not hurt to have support for both init systems enabled.","title":"Enabling required options"},{"location":"Linux/Gentoo/Kernel/#enabling-support-for-typical-system-components","text":"Make sure that every driver that is vital to the booting of the system (such as SATA controllers, NVMe block device support, filesystem support, etc.) is compiled in the kernel and not as a module, otherwise the system may not be able to boot completely. Next select the exact processor type. It is also recommended to enable MCE features (if available) so that users are able to be notified of any hardware problems. On some architectures (such as x86_64), these errors are not printed to dmesg , but to /dev/mcelog . This requires the app-admin/mcelog package. Also select Maintain a devtmpfs file system to mount at /dev so that critical device files are already available early in the boot process (CONFIG_DEVTMPFS and CONFIG_DEVTMPFS_MOUNT) : Enable devtmpfs support(_CONFIG_DEVTMPFS) KERNEL Device Driver ---> Generic Driver Options ---> [*] Maintain a devtmpfs filesystem to mount at /dev [*] Automount devtmpfs at /dev, after the kernel mounted the rootfs Verify SCSI disk support has been activated \u0013(CONFIG_BLK_DEV_SD) : Enabling SCSI disk support (CONFIG_SCSI, CONFIG_BLK_DEV_SD) KERNEL Device Driver ---> SCSI device support ---> <*> SCSI device support <*> SCSI disk support Enabling basic SATA and PATA support (CONFIG_ATA_ACPI, CONFIG_SATA_PMP, CONFIG_SATA_AHCI, CONFIG_ATA_BMDMA, CONFIG_ATA_SFF, CONFIG_ATA_PIIX) KERNEL Device Driver ---> <*> Serial ATA and Parallel ATA drivers (libata) ---> [*] ATA ACPI Support [*] SATA Poart Multiplier support <*> AHCI SATA support (achi) [*] ATA BMDMA support [*] ATA SFF support (for legacy IDE and PATA) <*> Intel ESB, ICH, PIIX3, PIIX4 PATA/SATA support (ata_piix) Verify basic NVMe support has been enabled: Enable basic NVMe support for Linux 4.4.x (CONFIG_BLK_DEV_NVME) KERNEL Device Driver ---> <*> NVM Express block device Enable basic NVMe support for Linux 5.x.x (CONFIG_DEVTMPFS) KERNEL Device Driver ---> NVME support ---> <*> NVM Express block device It does not hurt to enable the following additional NVMe support: Enabling additional NVMe support (CONFIG_NVME_MULTIPATH, CONFIG_NVME_MULTIPATH, CONFIG_NVME_HWMON, CONFIG_NVME_FC, CONFIG_NVME_TCP, CONFIG_NVME_TARGET, CONFIG_NVME_TARGET_PASSTHRU, CONFIG_NVME_TARGET_LOOP, CONFIG_NVME_TARGET_FC, CONFIG_NVME_TARGET_FCLOOP, CONFIG_NVME_TARGET_TCP KERNEL [*] NVMe multipath support [*] NVME hardware monitoring <M> NVM Express over Fabrics FC host driver <M> NVM Express over Fabrics TCP host driver <M> NVM Traget support [*] NVMe Target Passthrough support <M> NVMe loopback device support <M> NVMe over Fabrics FC target driver < > NVMe over Fabrics FC Transport Loopback Test driver (NEW) <M> NVMe over Fabrics TCP target driver Now go to File Systems and select support for the filesystems that will be used by the system. Do not compile the file system that is used for the root filesystem as module, otherwise the system may not be able to mount the partition. Also select Virtual memory and /proc file system. Select one or more of the following options as needed by the system: Enable file system support (CONFIG_EXT2_FS, CONFIG_EXT3_FS, CONFIG_EXT4_FS, CONFIG_BTRFS_FS, CONFIG_MSDOS_FS, CONFIG_VFAT_FS, CONFIG_PROC_FS, and CONFIG_TMPFS) KERNEL File systems ---> <*> Second extended fs support <*> The Extended 3 (ext3) filesystem <*> The Extended 4 (ext3) filesystem <*> Btrfs filesystem support DOS/FAT/NT Filesystems ---> <*> MSDOS fs support <*> VFAT (Windows-95) fs support Pseudo Filesystems ---> [*] /porc file system support [*] Tmpfs virtual memory file system support (former shm fs) Most systems also have multiple cores at their disposal, so it is important to activate Symmetric multi-processing support (CONFIG_SMP): Activating SMP support (CONFIG_SMP) KERNEL Processor type and features ---> [*] Symmetric multi-processing support If USB input devices (like keyboard or mouse) or other USB devices will be used, do not forget to enable those as well: Enable USB and human input device support (CONFIG_HID_GENERIC, CONFIG_USB_HID, CONFIG_USB_SUPPORT, CONFIG_USB_XHCI_HCD, CONFIG_USB_EHCI_HCD, CONFIG_USB_OHCI_HCD, (CONFIG_HID_GENERIC, CONFIG_USB_HID, CONFIG_USB_SUPPORT, CONFIG_USB_XHCI_HCD, CONFIG_USB_EHCI_HCD, CONFIG_USB_OHCI_HCD, CONFIG_USB4) KERNEL Device Drivers ---> HID support ---> -*- HID bus support <*> Generic HID driver [*] Battery level reporting for HID devices USB HID suport ---> <*> USB HID transport layer [*] USB support ---> <*> xHCI HCD (USB 3.0) support <*> EHCI HCD (USB 2.0) support <*> OHCI HCD (USB 1.1) support <*> Unified support for USB4 and Thunderbolt ---> If PPPoE is used to connect to the Internet, or a dial-up modem, then enable the following options (CONFIG_PPP, CONFIG_PPP_ASYNC, and CONFIG_PPP_SYNC_TTY) : Enabling PPPoE support (PPPoE, CONFIG_PPPOE, CONFIG_PPP_ASYNC, CONFIG_PPP_SYNC_TTY KERNEL Device Driver ---> Network device support ---> <*> PPP (point-to-point protocol) support <*> PPP over Ethernet <*> PPP support for async serial ports <*> PPP support for sync tty point","title":"Enabling support for typical system components"},{"location":"Linux/Gentoo/Kernel/#architecture-specific-kernel-configuration","text":"Enable GPT partition label support if that was used previously when partitioning the disk (CONFIG_PARTITION_ADVANCED and CONFIG_EFI_PARTITION): Enable support for GPT KERNEL _*_ Enable the block layer ---> Partition Types ---> [*] Advanced partition selection [*] EFI GUID Partition support Enable EFI stub support, EFI variables and EFI Framebuffer in the Linux kernel if UEFI is used to boot the system (CONFIG_EFI, CONFIG_EFI_STUB, CONFIG_EFI_MIXED, CONFIG_EFI_VARS, and CONFIG_FB_EFI) : Enable support for UEFI KERNEL Processor type and features ---> [*] EFI runtime service support [*] EFI stub support [*] EFI mixed-mode support Device Drivers Firmware Drivers ---> EFI (Extension Firmware Interface) Support ---> <*> EFI Veriable Support via sysfs Graphics support ---> Frame buffer Devices ---> <*> Support for frame buffer devices ---> [*] EFI-based Framebuffer Support","title":"Architecture specific kernel configuration"},{"location":"Linux/Gentoo/Kernel/#compiling-and-installing","text":"With the configuration now done, it is time to compile and install the kernel. Exit the configuration and start the compilation process: root #make && make modules_install It is possible to enable parallel builds using make -jX with X being an integer number of parallel tasks that the build process is allowed to launch. This is similar to the instructions about /etc/portage/make.conf earlier, with the MAKEOPTS variable. When the kernel has finished compiling, copy the kernel image to /boot/. This is handled by the make install command: root #make install","title":"Compiling and installing"},{"location":"Linux/Gentoo/Kernel/#optional-building-an-initramfs","text":"In certain cases it is necessary to build an initramfs - an initial ram-based file system. The most common reason is when important file system locations (like /usr/ or /var/) are on separate partitions. With an initramfs, these partitions can be mounted using the tools available inside the initramfs. Without an initramfs, there is a risk that the system will not boot properly as the tools that are responsible for mounting the file systems require information that resides on unmounted file systems. An initramfs will pull in the necessary files into an archive which is used right after the kernel boots, but before the control is handed over to the init tool. Scripts on the initramfs will then make sure that the partitions are properly mounted before the system continues booting. Important If using genkernel, it should be used for both building the kernel and the initramfs. When using genkernel only for generating an initramfs, it is crucial to pass --kernel-config=/path/to/kernel.config to genkernel or the generated initramfs may not work with a manually built kernel. Note that manually built kernels go beyond the scope of support for the handbook. See the kernel configuration article for more information. To install an initramfs, install sys-kernel/dracut first, then have it generate an initramfs: root #emerge --ask sys-kernel/dracut root #dracut --kver=5.15.52-gentoo The initramfs will be stored in /boot/. The resulting file can be found by simply listing the files starting with initramfs: root #ls /boot/initramfs*","title":"Optional: Building an initramfs"},{"location":"Linux/Gentoo/Kernel/#kernel-modules","text":"","title":"Kernel modules"},{"location":"Linux/Gentoo/Kernel/#listing-available-kernel-modules","text":"Hardware modules are optional to be listed manually. udev will normally load all hardware modules that are detected to be connected in most cases. However, it is not harmful for modules that will be automatically loaded to be listed. Modules cannot be loaded twice; they are either loaded or unloaded. Sometimes exotic hardware requires help to load their drivers. The modules that need to be loaded during each boot in can be added to /etc/modules-load.d/ .conf files in the format of one module per line. When extra options are needed for the modules, they should be set in /etc/modprobe.d/ .conf files instead. To view all modules available for a specific kernel version, issue the following find command. Do not forget to substitute \" \" with the appropriate version of the kernel to search: root #find /lib/modules/<kernel version>/ -type f -iname '*.o' -or -iname '*.ko' | less","title":"Listing available kernel modules"},{"location":"Linux/Gentoo/Kernel/#force-loading-particular-kernel-modules","text":"To force load the kernel to load the 3c59x.ko module (which is the driver for a specific 3Com network card family), edit the /etc/modules-load.d/network.conf file and enter the module name within it. root #mkdir -p /etc/modules-load.d root #nano -w /etc/modules-load.d/network.conf Note that the module's .ko file suffix is insignificant to the loading mechanism and left out of the configuration file: Force loading 3c59x module FILE /etc/modules-load.d/network.conf 3c59x","title":"Force loading particular kernel modules"},{"location":"Linux/Gentoo/Media/","text":"Choosing the media \u00b6 Hardware requirements \u00b6 AMD64 livedisk hardware requirements Minimal CD LiveDVD CPU Any x86-64 CPU, both AMD64 and Intel 64 Memory 2 GB Disk space 8 GB(excluding swap space) Swap space At least 2 GB Gentoo Linux installation Media \u00b6 Minimal installation \u00b6 The Gentoo minimal installation CD is a bootable image: a self-contained Gentoo environment. It allows the user to boot Linux from the CD or other installation media. During the boot process the hardware is detected and the appropriate drivers are loaded. The image is maintained by Gentoo developers and allows anyone to install Gentoo if an active Internet connection is available. The Minimal Installation CD is called install-amd64-minimal- .iso. The occasional Gentoo LiveDVD \u00b6 Occasionally, a special DVD image is crafted which can be used to install Gentoo. The instructions in this chapter target the Minimal Installation CD, so things might be a bit different when booting from the LiveDVD. However, the LiveDVD (or any other official Gentoo Linux environment) supports getting a root prompt by just invoking sudo su - or sudo -i in a terminal. What are stages then? \u00b6 A stage3 tarball is an archive containing a profile specific minimal Gentoo environment. Stage3 tarballs are suitable to continue the Gentoo installation using the instructions in this handbook. Previously, the handbook described the installation using one of three stage tarballs . Gentoo does not offer stage1 and stage2 tarballs for download any more since these are mostly for internal use and for bootstrapping Gentoo on new architectures. Stage3 tarballs can be downloaded from releases/amd64/autobuilds/ on any of the official Gentoo mirrors . Stage files update frequently and are not included in official installation images. A .CONTENTS file which is a text file listing all files available on the installation media. This file can be useful to verify if particular firmware or drivers are available on the installation media before downloading it. A .DIGESTS file which contains the hash of the ISO file itself, in various hashing formats/algorithms. This file can be used to verify if the downloaded ISO file is corrupt or not. A .asc file which is a cryptographic signature of the ISO file. This can be used to both verify if the downloaded ISO file is corrupt or not, as well as verify that the download is indeed provided by the Gentoo Release Engineering team and has not been tampered with. Using dd to write the ISO image to a USB driver \u00b6 Warning The dd command will wipe all data from the destination drive. Always backup all important data. root #dd if=/path/to/image.iso of=/dev/sdc bs=8192k; sync Creating bootable LiveUSB drives from Linux systems \u00b6 Automatic drive-wide installation script \u00b6 Warning This script will erase all data from the USB drive. Make sure to backup any pre-existing data first, and always backup all important data. Note #!/bin/bash set -e image = ${ 1 :?Supply the .iso image of a Gentoo installation medium } target = ${ 2 :?Supply the target device } echo Checking for the necessary tools presence... which syslinux which sfdisk which mkfs.vfat echo Mounting Gentoo CD image... cdmountpoint = /mnt/gentoo-cd mkdir -p \" $cdmountpoint \" trap 'echo Unmounting Gentoo CD image...; umount \"$cdmountpoint\"' EXIT mount -o loop,ro \" $image \" \" $cdmountpoint \" echo Creating a disk-wide EFI FAT partition on \" $target \" ... echo ',,U,*' | sfdisk --wipe always \" $target \" echo Installing syslinux MBR on \" $target \" ... dd if = /usr/share/syslinux/mbr.bin of = \" $target \" sleep 1 echo Creating file system on \" $target \" 1 ... mkfs.vfat \" $target \" 1 -n GENTOO echo Mounting file system... mountpoint = /mnt/gentoo-usb mkdir -p \" $mountpoint \" mount \" $target \" 1 \" $mountpoint \" echo Copying files... cp -r \" $cdmountpoint \" /* \" $mountpoint \" / mv \" $mountpoint \" /isolinux/* \" $mountpoint \" mv \" $mountpoint \" /isolinux.cfg \" $mountpoint \" /syslinux.cfg rm -rf \" $mountpoint \" /isolinux* mv \" $mountpoint \" /memtest86 \" $mountpoint \" /memtest sed -i -e \"s:cdroot:cdroot slowusb:\" -e \"s:kernel memtest86:kernel memtest:\" \" $mountpoint \" /syslinux.cfg echo Unmounting file system... umount \" $mountpoint \" echo Installing syslinux on \" $target \" 1 syslinux \" $target \" 1 echo Syncing... sync echo 'Done!'","title":"Media"},{"location":"Linux/Gentoo/Media/#choosing-the-media","text":"","title":"Choosing the media"},{"location":"Linux/Gentoo/Media/#hardware-requirements","text":"AMD64 livedisk hardware requirements Minimal CD LiveDVD CPU Any x86-64 CPU, both AMD64 and Intel 64 Memory 2 GB Disk space 8 GB(excluding swap space) Swap space At least 2 GB","title":"Hardware requirements"},{"location":"Linux/Gentoo/Media/#gentoo-linux-installation-media","text":"","title":"Gentoo Linux installation Media"},{"location":"Linux/Gentoo/Media/#minimal-installation","text":"The Gentoo minimal installation CD is a bootable image: a self-contained Gentoo environment. It allows the user to boot Linux from the CD or other installation media. During the boot process the hardware is detected and the appropriate drivers are loaded. The image is maintained by Gentoo developers and allows anyone to install Gentoo if an active Internet connection is available. The Minimal Installation CD is called install-amd64-minimal- .iso.","title":"Minimal installation"},{"location":"Linux/Gentoo/Media/#the-occasional-gentoo-livedvd","text":"Occasionally, a special DVD image is crafted which can be used to install Gentoo. The instructions in this chapter target the Minimal Installation CD, so things might be a bit different when booting from the LiveDVD. However, the LiveDVD (or any other official Gentoo Linux environment) supports getting a root prompt by just invoking sudo su - or sudo -i in a terminal.","title":"The occasional Gentoo LiveDVD"},{"location":"Linux/Gentoo/Media/#what-are-stages-then","text":"A stage3 tarball is an archive containing a profile specific minimal Gentoo environment. Stage3 tarballs are suitable to continue the Gentoo installation using the instructions in this handbook. Previously, the handbook described the installation using one of three stage tarballs . Gentoo does not offer stage1 and stage2 tarballs for download any more since these are mostly for internal use and for bootstrapping Gentoo on new architectures. Stage3 tarballs can be downloaded from releases/amd64/autobuilds/ on any of the official Gentoo mirrors . Stage files update frequently and are not included in official installation images. A .CONTENTS file which is a text file listing all files available on the installation media. This file can be useful to verify if particular firmware or drivers are available on the installation media before downloading it. A .DIGESTS file which contains the hash of the ISO file itself, in various hashing formats/algorithms. This file can be used to verify if the downloaded ISO file is corrupt or not. A .asc file which is a cryptographic signature of the ISO file. This can be used to both verify if the downloaded ISO file is corrupt or not, as well as verify that the download is indeed provided by the Gentoo Release Engineering team and has not been tampered with.","title":"What are stages then?"},{"location":"Linux/Gentoo/Media/#using-dd-to-write-the-iso-image-to-a-usb-driver","text":"Warning The dd command will wipe all data from the destination drive. Always backup all important data. root #dd if=/path/to/image.iso of=/dev/sdc bs=8192k; sync","title":"Using dd to write the ISO image to a USB driver"},{"location":"Linux/Gentoo/Media/#creating-bootable-liveusb-drives-from-linux-systems","text":"","title":"Creating bootable LiveUSB drives from Linux systems"},{"location":"Linux/Gentoo/Media/#automatic-drive-wide-installation-script","text":"Warning This script will erase all data from the USB drive. Make sure to backup any pre-existing data first, and always backup all important data. Note #!/bin/bash set -e image = ${ 1 :?Supply the .iso image of a Gentoo installation medium } target = ${ 2 :?Supply the target device } echo Checking for the necessary tools presence... which syslinux which sfdisk which mkfs.vfat echo Mounting Gentoo CD image... cdmountpoint = /mnt/gentoo-cd mkdir -p \" $cdmountpoint \" trap 'echo Unmounting Gentoo CD image...; umount \"$cdmountpoint\"' EXIT mount -o loop,ro \" $image \" \" $cdmountpoint \" echo Creating a disk-wide EFI FAT partition on \" $target \" ... echo ',,U,*' | sfdisk --wipe always \" $target \" echo Installing syslinux MBR on \" $target \" ... dd if = /usr/share/syslinux/mbr.bin of = \" $target \" sleep 1 echo Creating file system on \" $target \" 1 ... mkfs.vfat \" $target \" 1 -n GENTOO echo Mounting file system... mountpoint = /mnt/gentoo-usb mkdir -p \" $mountpoint \" mount \" $target \" 1 \" $mountpoint \" echo Copying files... cp -r \" $cdmountpoint \" /* \" $mountpoint \" / mv \" $mountpoint \" /isolinux/* \" $mountpoint \" mv \" $mountpoint \" /isolinux.cfg \" $mountpoint \" /syslinux.cfg rm -rf \" $mountpoint \" /isolinux* mv \" $mountpoint \" /memtest86 \" $mountpoint \" /memtest sed -i -e \"s:cdroot:cdroot slowusb:\" -e \"s:kernel memtest86:kernel memtest:\" \" $mountpoint \" /syslinux.cfg echo Unmounting file system... umount \" $mountpoint \" echo Installing syslinux on \" $target \" 1 syslinux \" $target \" 1 echo Syncing... sync echo 'Done!'","title":"Automatic drive-wide installation script"},{"location":"Linux/Gentoo/Network/","text":"Network \u00b6 Determine interface names \u00b6 root #ifconfig eth0 Link encap:Ethernet HWaddr 00 :50:BA:8F:61:7A inet addr:192.168.0.2 Bcast:192.168.0.255 Mask:255.255.255.0 inet6 addr: fe80::50:ba8f:617a/10 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:1498792 errors:0 dropped:0 overruns:0 frame:0 TX packets:1284980 errors:0 dropped:0 overruns:0 carrier:0 collisions:1984 txqueuelen:100 RX bytes:485691215 ( 463 .1 Mb ) TX bytes:123951388 ( 118 .2 Mb ) Interrupt:11 Base address:0xe800 Configure the proxies \u00b6 #http_proxy root #export http_proxy=\"http://proxy.gentoo.org:8080\" #ftp_proxy root #export ftp_proxy=\"ftp://proxy.gentoo.org:8080\" #rsync_proxy root #export RSYNC_PROXY=\"proxy.gentoo.org:8080\" Netowork automatic config Tool \u00b6 root #net-setup eth0 DHCP \u00b6 root #dhcpcd eth0 Preparing the wireless access \u00b6 root #iw dev wlp9s0 info Interface wlp9s0 ifindex 3 wdev 0x1 addr 00 :00:00:00:00:00 type managed wiphy 0 channel 11 ( 2462 MHz ) , width: 20 MHz ( no HT ) , center1: 2462 MHz txpower 30 .00 dBm root #iw dev wlp9s0 link Not connected. root #ip link set dev wlp9s0 up If the wireless network is set up with WPA or WPA2, then wpa_supplicant needs to be used. For more information on configuring wireless networking in Gentoo Linux, please read the Wireless networking chapter in the Gentoo Handbook.","title":"Network"},{"location":"Linux/Gentoo/Network/#network","text":"","title":"Network"},{"location":"Linux/Gentoo/Network/#determine-interface-names","text":"root #ifconfig eth0 Link encap:Ethernet HWaddr 00 :50:BA:8F:61:7A inet addr:192.168.0.2 Bcast:192.168.0.255 Mask:255.255.255.0 inet6 addr: fe80::50:ba8f:617a/10 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:1498792 errors:0 dropped:0 overruns:0 frame:0 TX packets:1284980 errors:0 dropped:0 overruns:0 carrier:0 collisions:1984 txqueuelen:100 RX bytes:485691215 ( 463 .1 Mb ) TX bytes:123951388 ( 118 .2 Mb ) Interrupt:11 Base address:0xe800","title":"Determine interface names"},{"location":"Linux/Gentoo/Network/#configure-the-proxies","text":"#http_proxy root #export http_proxy=\"http://proxy.gentoo.org:8080\" #ftp_proxy root #export ftp_proxy=\"ftp://proxy.gentoo.org:8080\" #rsync_proxy root #export RSYNC_PROXY=\"proxy.gentoo.org:8080\"","title":"Configure the proxies"},{"location":"Linux/Gentoo/Network/#netowork-automatic-config-tool","text":"root #net-setup eth0","title":"Netowork automatic config Tool"},{"location":"Linux/Gentoo/Network/#dhcp","text":"root #dhcpcd eth0","title":"DHCP"},{"location":"Linux/Gentoo/Network/#preparing-the-wireless-access","text":"root #iw dev wlp9s0 info Interface wlp9s0 ifindex 3 wdev 0x1 addr 00 :00:00:00:00:00 type managed wiphy 0 channel 11 ( 2462 MHz ) , width: 20 MHz ( no HT ) , center1: 2462 MHz txpower 30 .00 dBm root #iw dev wlp9s0 link Not connected. root #ip link set dev wlp9s0 up If the wireless network is set up with WPA or WPA2, then wpa_supplicant needs to be used. For more information on configuring wireless networking in Gentoo Linux, please read the Wireless networking chapter in the Gentoo Handbook.","title":"Preparing the wireless access"},{"location":"Linux/Gentoo/Prepare%20the%20disks/","text":"Prepare the disks \u00b6 GUID Partition Table - he GUID Partition Table (GPT) setup (also called GPT disklabel) uses 64-bit identifiers for the partitions. The location in which it stores the partition information is much bigger than the 512 bytes of the MBR partition table (DOS disklabel), which means there is practically no limit on the amount of partitions for a GPT disk. Also the size of a partition is bounded by a much greater limit (almost 8 ZiB - yes, zebibytes). Default partitioning scheme \u00b6 Partition Filesystem Size Description /dev/sda1 fat32 (UEFI) or ext4 (BIOS - aka Legacy boot) 256M Boot/EFI system partition /dev/sda2 swap RAM size *2 Swap partition /dev/sda3 ext4 Rest of the disk Root partition Filesystems \u00b6 btrfs A next generation filesystem that provides many advanced features such as snapshotting, self-healing through checksums, transparent compression, subvolumes, and integrated RAID. Kernels prior to 5.4.y are not guaranteed to be safe to use with btrfs in production because fixes for serious issues are only present in the more recent releases of the LTS kernel branches. Filesystem corruption issues are common on older kernel branches, with anything older than 4.4.y being especially unsafe and prone to corruption. Corruption is more likely on older kernels (than 5.4.y) when compression is enabled. RAID \u215a and quota groups unsafe on all versions of btrfs. Furthermore, btrfs can counter-intuitively fail filesystem operations with ENOSPC when df reports free space due to internal fragmentation (free space pinned by DATA + SYSTEM chunks, but needed in METADATA chunks). Additionally, a single 4K reference to a 128M extent inside btrfs can cause free space to be present, but unavailable for allocations. This can also cause btrfs to return ENOSPC when free space is reported by df . Installing sys-fs/btrfsmaintenance and configuring the scripts to run periodically can help to reduce the possibility of ENOSPC issues by rebalancing btrfs, but it will not eliminate the risk of ENOSPC when free space is present. Some workloads will never hit ENOSPC while others will. If the risk of ENOSPC in production is unacceptable, you should use something else. If using btrfs, be certain to avoid configurations known to have issues. With the exception of ENOSPC, information on the issues present in btrfs in the latest kernel branches is available at the btrfs status page . Filesystem Creation command On minimal CD\uff1f Package btrfs mkfs.btrfs Yes sys-fs/btrfs-progs ext4 mkfs.ext4 Yes sys-fs/e2fsprogs EFI system partition partition (/dev/sda1) as FAT32 and the root partition (/dev/sda3) as ext4 as used in the example partition structure root #mkfs.vfat -F 32 /dev/sda1 root #mkfs.ext4 /dev/sda3 root #mkswap /dev/sda2 To activate the swap partition root #swapon /dev/sda2 Mounting the root partition root #mkdir --parents /mnt/gentoo root #mount /dev/sda3 /mnt/gentoo","title":"Prepare the disks"},{"location":"Linux/Gentoo/Prepare%20the%20disks/#prepare-the-disks","text":"GUID Partition Table - he GUID Partition Table (GPT) setup (also called GPT disklabel) uses 64-bit identifiers for the partitions. The location in which it stores the partition information is much bigger than the 512 bytes of the MBR partition table (DOS disklabel), which means there is practically no limit on the amount of partitions for a GPT disk. Also the size of a partition is bounded by a much greater limit (almost 8 ZiB - yes, zebibytes).","title":"Prepare the disks"},{"location":"Linux/Gentoo/Prepare%20the%20disks/#default-partitioning-scheme","text":"Partition Filesystem Size Description /dev/sda1 fat32 (UEFI) or ext4 (BIOS - aka Legacy boot) 256M Boot/EFI system partition /dev/sda2 swap RAM size *2 Swap partition /dev/sda3 ext4 Rest of the disk Root partition","title":"Default partitioning scheme"},{"location":"Linux/Gentoo/Prepare%20the%20disks/#filesystems","text":"btrfs A next generation filesystem that provides many advanced features such as snapshotting, self-healing through checksums, transparent compression, subvolumes, and integrated RAID. Kernels prior to 5.4.y are not guaranteed to be safe to use with btrfs in production because fixes for serious issues are only present in the more recent releases of the LTS kernel branches. Filesystem corruption issues are common on older kernel branches, with anything older than 4.4.y being especially unsafe and prone to corruption. Corruption is more likely on older kernels (than 5.4.y) when compression is enabled. RAID \u215a and quota groups unsafe on all versions of btrfs. Furthermore, btrfs can counter-intuitively fail filesystem operations with ENOSPC when df reports free space due to internal fragmentation (free space pinned by DATA + SYSTEM chunks, but needed in METADATA chunks). Additionally, a single 4K reference to a 128M extent inside btrfs can cause free space to be present, but unavailable for allocations. This can also cause btrfs to return ENOSPC when free space is reported by df . Installing sys-fs/btrfsmaintenance and configuring the scripts to run periodically can help to reduce the possibility of ENOSPC issues by rebalancing btrfs, but it will not eliminate the risk of ENOSPC when free space is present. Some workloads will never hit ENOSPC while others will. If the risk of ENOSPC in production is unacceptable, you should use something else. If using btrfs, be certain to avoid configurations known to have issues. With the exception of ENOSPC, information on the issues present in btrfs in the latest kernel branches is available at the btrfs status page . Filesystem Creation command On minimal CD\uff1f Package btrfs mkfs.btrfs Yes sys-fs/btrfs-progs ext4 mkfs.ext4 Yes sys-fs/e2fsprogs EFI system partition partition (/dev/sda1) as FAT32 and the root partition (/dev/sda3) as ext4 as used in the example partition structure root #mkfs.vfat -F 32 /dev/sda1 root #mkfs.ext4 /dev/sda3 root #mkswap /dev/sda2 To activate the swap partition root #swapon /dev/sda2 Mounting the root partition root #mkdir --parents /mnt/gentoo root #mount /dev/sda3 /mnt/gentoo","title":"Filesystems"},{"location":"Linux/Gentoo/Stage3/","text":"Installing stage3 \u00b6 Installing a stage tarball \u00b6 Verify the current date and time by running the date command: \u00b6 root #date Mon Oct 3 13 :16:22 PDT 2021 Official Gentoo live environments include the chronyd command (available through the net-misc/chrony package) and a configuration file pointing to ntp.org time servers. It can be used to automatically sync the system clock to UTC time using a time server. Using this method requires a working network configuration and may not be available on all architectures. root #chronyd -q Choosing a stage tarball \u00b6 Multilib (32 and 64-bit) Multilib Choosing a base tarball for the system can save a considerable amount of time later on in the installation process, specifically when it is time to choose a system profile. The selection of a stage tarball will directly impact future system configuration and can save a headache or two later on down the line. The multilib tarball uses 64-bit libraries when possible, and only falls back to the 32-bit versions when necessary for compatibility. This is an excellent option for the majority of installations because it provides a great amount of flexibility for customization in the future. Those who desire their systems to be capable of easily switching profiles should download the multilib tarball option for their respective processor architecture. Most users should not use the 'advanced' tarballs options; they are for specific software or hardware configurations. No-multilib(pure 64-bit) No-Multilib Selecting a no-multilib tarball to be the base of the system provides a complete 64-bit operating system environment. This effectively renders the ability to switch to multilib profiles improbable, although still technically possible. Warning Readers who are just starting out with Gentoo should not choose a no-multilib tarball unless it is absolutely necessary. Migrating from a no-multilib to a multilib system requires an extremely well-working knowledge of Gentoo and the lower-level toolchain (it may even cause our Toolchain developers to shudder a little). It is not for the faint of heart and is beyond the scope of this guide. OpenRC \u00b6 OpenRC is a dependency-based init system (responsible for starting up system services once the kernel has booted) that maintains compatibility with the system provided init program, normally located in /sbin/init. It is Gentoo's native and original init system, but is also deployed by a few other Linux distributions and BSD systems. OpenRC does not function as a replacement for the /sbin/init file by default and is 100% compatible with Gentoo init scripts. This means a solution can be found to run the dozens of daemons in the Gentoo ebuild repository. systemd \u00b6 systemd is a modern SysV-style init and rc replacement for Linux systems. It is used as the primary init system by a majority of Linux distributions. systemd is fully supported in Gentoo and works for its intended purpose. If something seems lacking in the Handbook for a systemd install path, review the systemd article before asking for support. Donwloading the stage tarball \u00b6 Graphical browsers root #cd /mnt/gentoo root #wget <PASTED_STAGE_URL> Command-line browsers root #links https://www.gentoo.org/downloads/mirrors/ root #links -http-proxy proxy.server.com:8080 https://www.gentoo.org/downloads/mirrors/ root #lynx https://www.gentoo.org/downloads/mirrors/ Verifying and validating root #openssl dgst -r -sha512 stage3-amd64-<release>-<init>.tar.xz root #sha512sum stage3-amd64-<release>-<init>.tar.xz root #openssl dgst -r -whirlpool stage3-amd64-<release>-<init>.tar.xz Unpacking the stage tarball \u00b6 root #tar xpvf stage3-*.tar.xz --xattrs-include='*.*' --numeric-owner Configuring compile options \u00b6 To optimize the system, it is possible to set variables which impact the behavior of Portage, Gentoo's officially supported package manager. All those variables can be set as environment variables (using export) but setting via export is not permanent. root #nano -w /mnt/gentoo/etc/portage/make.conf CFLAGS and CXXFLAGS \u00b6 The CFLAGS and CXXFLAGS variables define the optimization flags for GCC C and C++ compilers respectively. Although those are defined generally here, for maximum performance one would need to optimize these flags for each program separately. The reason for this is because every program is different. However, this is not manageable, hence the definition of these flags in the make.conf file. In make.conf one should define the optimization flags that will make the system the most responsive generally. Don't place experimental settings in this variable; too much optimization can make programs misbehave (crash, or even worse, malfunction). To understand them all, read the GNU Online Manual(s) or the gcc info page (info gcc - only works on a working Linux system). The make.conf.example file itself also contains lots of examples and information; don't forget to read it too. A first setting is the -march= or -mtune= flag, which specifies the name of the target architecture. Possible options are described in the make.conf.example file (as comments). A commonly used value is native as that tells the compiler to select the target architecture of the current system (the one users are installing Gentoo on). A second one is the -O flag (that is a capital O, not a zero), which specifies the gcc optimization class flag. Possible classes are s (for size-optimized), 0 (zero - for no optimizations), 1, 2 or even 3 for more speed-optimization flags (every class has the same flags as the one before, plus some extras). -O2 is the recommended default. -O3 is known to cause problems when used system-wide, so we recommend to stick to -O2. Another popular optimization flag is -pipe (use pipes rather than temporary files for communication between the various stages of compilation). It has no impact on the generated code, but uses more memory. On systems with low memory, gcc might get killed. In that case, do not use this flag. Using -fomit-frame-pointer (which doesn't keep the frame pointer in a register for functions that don't need one) might have serious repercussions on the debugging of applications. When the CFLAGS and CXXFLAGS variables are defined, combine the several optimization flags in one string. The default values contained in the stage3 archive that is unpacked should be good enough. The following one is just an example: # Compiler flags to set for all languages COMMON_FLAGS = \"-march=native -O2 -pipe\" # Use the same settings for both variables CFLAGS = \" ${ COMMON_FLAGS } \" CXXFLAGS = \" ${ COMMON_FLAGS } \" Although the GCC optimization article has more information on how the various compilation options can affect a system, the Safe CFLAGS article may be a more practical place for beginners to start optimizing their systems. MAKEOPTS \u00b6 The MAKEOPTS variable defines how many parallel compilations should occur when installing a package. As of Portage version 3.0.31 1 , if left undefined, Portage's default behavior is to set the MAKEOPTS value to the same number of threads returned by nproc. A good choice is the smaller of: the number of threads the CPU has, or the total amount of system RAM divided by 2 GiB. Warning Using a large number of jobs can significantly impact memory consumption. A good recommendation is to have at least 2 GiB of RAM for every job specified (so, e.g. -j6 requires at least 12 GiB). To avoid running out of memory, lower the number of jobs to fit the available memory. When using parallel emerges (--jobs), the effective number of jobs run can grow exponentially (up to make jobs multiplied by emerge jobs). This can be worked around by running a localhost-only distcc configuration that will limit the number of compiler instances per host.:warning /etc/portage/make.conf If left undefined, Portage's default behavior is to set the MAKEOPTS value to the same number of threads returned by nproc # Example MAKEOPTS declaration MAKEOPTS = \"-j4\" https://gitweb.gentoo.org/proj/portage.git/commit/?id=5d2af567772bb12b073f1671daea6263055cbdc2 \u21a9","title":"Stage3"},{"location":"Linux/Gentoo/Stage3/#installing-stage3","text":"","title":"Installing stage3"},{"location":"Linux/Gentoo/Stage3/#installing-a-stage-tarball","text":"","title":"Installing a stage tarball"},{"location":"Linux/Gentoo/Stage3/#verify-the-current-date-and-time-by-running-the-date-command","text":"root #date Mon Oct 3 13 :16:22 PDT 2021 Official Gentoo live environments include the chronyd command (available through the net-misc/chrony package) and a configuration file pointing to ntp.org time servers. It can be used to automatically sync the system clock to UTC time using a time server. Using this method requires a working network configuration and may not be available on all architectures. root #chronyd -q","title":"Verify the current date and time by running the date command:"},{"location":"Linux/Gentoo/Stage3/#choosing-a-stage-tarball","text":"Multilib (32 and 64-bit) Multilib Choosing a base tarball for the system can save a considerable amount of time later on in the installation process, specifically when it is time to choose a system profile. The selection of a stage tarball will directly impact future system configuration and can save a headache or two later on down the line. The multilib tarball uses 64-bit libraries when possible, and only falls back to the 32-bit versions when necessary for compatibility. This is an excellent option for the majority of installations because it provides a great amount of flexibility for customization in the future. Those who desire their systems to be capable of easily switching profiles should download the multilib tarball option for their respective processor architecture. Most users should not use the 'advanced' tarballs options; they are for specific software or hardware configurations. No-multilib(pure 64-bit) No-Multilib Selecting a no-multilib tarball to be the base of the system provides a complete 64-bit operating system environment. This effectively renders the ability to switch to multilib profiles improbable, although still technically possible. Warning Readers who are just starting out with Gentoo should not choose a no-multilib tarball unless it is absolutely necessary. Migrating from a no-multilib to a multilib system requires an extremely well-working knowledge of Gentoo and the lower-level toolchain (it may even cause our Toolchain developers to shudder a little). It is not for the faint of heart and is beyond the scope of this guide.","title":"Choosing a stage tarball"},{"location":"Linux/Gentoo/Stage3/#openrc","text":"OpenRC is a dependency-based init system (responsible for starting up system services once the kernel has booted) that maintains compatibility with the system provided init program, normally located in /sbin/init. It is Gentoo's native and original init system, but is also deployed by a few other Linux distributions and BSD systems. OpenRC does not function as a replacement for the /sbin/init file by default and is 100% compatible with Gentoo init scripts. This means a solution can be found to run the dozens of daemons in the Gentoo ebuild repository.","title":"OpenRC"},{"location":"Linux/Gentoo/Stage3/#systemd","text":"systemd is a modern SysV-style init and rc replacement for Linux systems. It is used as the primary init system by a majority of Linux distributions. systemd is fully supported in Gentoo and works for its intended purpose. If something seems lacking in the Handbook for a systemd install path, review the systemd article before asking for support.","title":"systemd"},{"location":"Linux/Gentoo/Stage3/#donwloading-the-stage-tarball","text":"Graphical browsers root #cd /mnt/gentoo root #wget <PASTED_STAGE_URL> Command-line browsers root #links https://www.gentoo.org/downloads/mirrors/ root #links -http-proxy proxy.server.com:8080 https://www.gentoo.org/downloads/mirrors/ root #lynx https://www.gentoo.org/downloads/mirrors/ Verifying and validating root #openssl dgst -r -sha512 stage3-amd64-<release>-<init>.tar.xz root #sha512sum stage3-amd64-<release>-<init>.tar.xz root #openssl dgst -r -whirlpool stage3-amd64-<release>-<init>.tar.xz","title":"Donwloading the stage tarball"},{"location":"Linux/Gentoo/Stage3/#unpacking-the-stage-tarball","text":"root #tar xpvf stage3-*.tar.xz --xattrs-include='*.*' --numeric-owner","title":"Unpacking the stage tarball"},{"location":"Linux/Gentoo/Stage3/#configuring-compile-options","text":"To optimize the system, it is possible to set variables which impact the behavior of Portage, Gentoo's officially supported package manager. All those variables can be set as environment variables (using export) but setting via export is not permanent. root #nano -w /mnt/gentoo/etc/portage/make.conf","title":"Configuring compile options"},{"location":"Linux/Gentoo/Stage3/#cflags-and-cxxflags","text":"The CFLAGS and CXXFLAGS variables define the optimization flags for GCC C and C++ compilers respectively. Although those are defined generally here, for maximum performance one would need to optimize these flags for each program separately. The reason for this is because every program is different. However, this is not manageable, hence the definition of these flags in the make.conf file. In make.conf one should define the optimization flags that will make the system the most responsive generally. Don't place experimental settings in this variable; too much optimization can make programs misbehave (crash, or even worse, malfunction). To understand them all, read the GNU Online Manual(s) or the gcc info page (info gcc - only works on a working Linux system). The make.conf.example file itself also contains lots of examples and information; don't forget to read it too. A first setting is the -march= or -mtune= flag, which specifies the name of the target architecture. Possible options are described in the make.conf.example file (as comments). A commonly used value is native as that tells the compiler to select the target architecture of the current system (the one users are installing Gentoo on). A second one is the -O flag (that is a capital O, not a zero), which specifies the gcc optimization class flag. Possible classes are s (for size-optimized), 0 (zero - for no optimizations), 1, 2 or even 3 for more speed-optimization flags (every class has the same flags as the one before, plus some extras). -O2 is the recommended default. -O3 is known to cause problems when used system-wide, so we recommend to stick to -O2. Another popular optimization flag is -pipe (use pipes rather than temporary files for communication between the various stages of compilation). It has no impact on the generated code, but uses more memory. On systems with low memory, gcc might get killed. In that case, do not use this flag. Using -fomit-frame-pointer (which doesn't keep the frame pointer in a register for functions that don't need one) might have serious repercussions on the debugging of applications. When the CFLAGS and CXXFLAGS variables are defined, combine the several optimization flags in one string. The default values contained in the stage3 archive that is unpacked should be good enough. The following one is just an example: # Compiler flags to set for all languages COMMON_FLAGS = \"-march=native -O2 -pipe\" # Use the same settings for both variables CFLAGS = \" ${ COMMON_FLAGS } \" CXXFLAGS = \" ${ COMMON_FLAGS } \" Although the GCC optimization article has more information on how the various compilation options can affect a system, the Safe CFLAGS article may be a more practical place for beginners to start optimizing their systems.","title":"CFLAGS and CXXFLAGS"},{"location":"Linux/Gentoo/Stage3/#makeopts","text":"The MAKEOPTS variable defines how many parallel compilations should occur when installing a package. As of Portage version 3.0.31 1 , if left undefined, Portage's default behavior is to set the MAKEOPTS value to the same number of threads returned by nproc. A good choice is the smaller of: the number of threads the CPU has, or the total amount of system RAM divided by 2 GiB. Warning Using a large number of jobs can significantly impact memory consumption. A good recommendation is to have at least 2 GiB of RAM for every job specified (so, e.g. -j6 requires at least 12 GiB). To avoid running out of memory, lower the number of jobs to fit the available memory. When using parallel emerges (--jobs), the effective number of jobs run can grow exponentially (up to make jobs multiplied by emerge jobs). This can be worked around by running a localhost-only distcc configuration that will limit the number of compiler instances per host.:warning /etc/portage/make.conf If left undefined, Portage's default behavior is to set the MAKEOPTS value to the same number of threads returned by nproc # Example MAKEOPTS declaration MAKEOPTS = \"-j4\" https://gitweb.gentoo.org/proj/portage.git/commit/?id=5d2af567772bb12b073f1671daea6263055cbdc2 \u21a9","title":"MAKEOPTS"},{"location":"Linux/Gentoo/System/","text":"Configuring the system \u00b6 Filesystem information \u00b6 Creating the fstab file \u00b6 The /etc/fstab file uses a table-like syntax. Every line consists of six fields, separated by whitespace (space(s), tabs, or a mixture of the two). Each field has its own meaning: The first field shows the block special device or remote filesystem to be mounted. Several kinds of device identifiers are available for block special device nodes, including paths to device files, filesystem labels and UUIDs, and partition labels and UUIDs. The second field shows the mount point at which the partition should be mounted. The third field shows the type of filesystem used by the partition. The fourth field shows the mount options used by mount when it wants to mount the partition. As every filesystem has its own mount options, so system admins are encouraged to read the mount man page (man mount) for a full listing. Multiple mount options are comma-separated. The fifth field is used by dump to determine if the partition needs to be dumped or not. This can generally be left as 0 (zero). The sixth field is used by fsck to determine the order in which filesystems should be checked if the system wasn't shut down properly. The root filesystem should have 1 while the rest should have 2 (or 0 if a filesystem check is not necessary). root #nano /etc/fstab Filesystem labels and UUIDs \u00b6 Both MBR (BIOS) and GPT include support for filesystem labels and filesystem UUIDs. These attributes can be defined in /etc/fstab as alternatives for the mount command to use when attempting to find and mount block devices. Filesystem labels and UUIDs are identified by the LABEL and UUID prefix and can be viewed with the blkid command: root #blkid Danger If the filesystem inside a partition is wiped, then the filesystem label and the UUID values will be subsequently altered or removed. Because of uniqueness, readers that are using an MBR-style partition table are recommended to use UUIDs over labels to define mountable volumes in /etc/fstab. Info UUIDs of the filesystem on a LVM volume and its LVM snapshots are identical, therefore using UUIDs to mount LVM volumes should be avoided. Partition labels and UUIDs \u00b6 Users who have gone the GPT route have a couple more 'robust' options available to define partitions in /etc/fstab. Partition labels and partition UUIDs can be used to identify the block device's individual partition(s), regardless of what filesystem has been chosen for the partition itself. Partition labels and UUIDs are identified by the PARTLABEL and PARTUUID prefixes respectively and can be viewed nicely in the terminal by running the blkid command: root #blkid Below is a more elaborate example of an /etc/fstab file: A full /etc/fstab example FILE /etc/fstab Adjust any formatting difference and additional partitions created from the Preparing the disks step filesystem dir type options jump pass /dev/sda1 /boot vfat defaults,noatime 0 2 /dev/sda2 none swap sw 0 0 /dev/sda3 / ext4 noatime 0 1 /dev/cdrom /mnt/cdrom auto noauto,user 0 0 When auto is used in the third field, it makes the mount command guess what the filesystem would be. This is recommended for removable media as they can be created with one of many filesystems. The user option in the fourth field makes it possible for non-root users to mount the CD. To improve performance, most users would want to add the noatime mount option, which results in a faster system since access times are not registered (those are not needed generally anyway). This is also recommended for systems with solid state drives (SSDs). Due to degradation in performance, defining the discard mount option in /etc/fstab is not recommended. It is generally better to schedule block discards on a periodic basis using a job scheduler such as cron or a timer (systemd). See Periodic fstrim jobs for more information. Networking information \u00b6 For systems running OpenRC, a more detailed reference for network setup is available in the advanced network configuration section, which is covered near the end of the handbook. Systems with more specific network needs may need to skip ahead, then return here to continue with the rest of the installation. For more specific systemd network setup, please review see the networking portion of the systemd article. Hostname \u00b6 Set the hostname (OpenRC or systemd) \u00b6 root #echo tux > /etc/hostname systemd \u00b6 root #hostnamectl hostname tux Network \u00b6 DHCP via dhcpcd (any init system) \u00b6 root #emerge --ask net-misc/dhcpcd To enable and then start the service on OpenRC systems: root #rc-update add dhcpcd default root #rc-service dhcpcd start To enable and start the service on systmd system: root #systemctl enable --now dhcpcd All networking information is gathered in /etc/conf.d/net. It uses a straightforward - yet perhaps not intuitive - syntax. Do not fear! Everything is explained below. A fully commented example that covers many different configurations is available in /usr/share/doc/netifrc-*/net.example.bz2. First install net-misc/netifrc root #emerge --ask --noreplace net-misc/netifrc If the network connection needs to be configured because of specific DHCP options or because DHCP is not used at all, then open /etc/conf.d/net: root #nano /etc/conf.d/net Set both config_eth0 and routes_eth0 to enter IP address information and routing information: This assumes that the network interface will be called eth0. This is, however, very system dependent. It is recommended to assume that the interface is named the same as the interface name when booted from the installation media if the installation media is sufficiently recent. More information can be found in the Network interface naming section. Static IP definition FILE /etc/conf.d/net config_eth0=\"192.168.0.2 netmask 255.255.255.0 brd 192.168.0.255\" routes_eth0=\"default via 192.168.0.1\" To use DHCP, define config_eth0: DHCP definition FILE /etc/conf.d/net config_eth0=\"dhcp\" Go over /usr/share/doc/netifrc-*/net.example.bz2 for a list of additional configuration options. Be sure to also read up on the DHCP client man page if specific DHCP options need to be set. Automatically start networking at boot \u00b6 root #cd /etc/init.d root #ln -s net.lo net.eth0 root #rc-update add net.eth0 default Infor If the system has several network interfaces, then the appropriate net.* files need to be created just like we did with net.eth0. If, after booting the system, it is discovered the network interface name (which is currently documented as eth0) was wrong, then execute the following steps to rectify: Update the /etc/conf.d/net file with the correct interface name (like enp3s0 or enp5s0, instead of eth0). Create new symbolic link (like /etc/init.d/net.enp3s0). Remove the old symbolic link ( rm /etc/init.d/net.eth0 ). Add the new one to the default runlevel. Remove the old one using rc-update del net.eth0 default . The hosts file \u00b6 root #nano /etc/hosts Filling in the network information FILE /etc/hosts # This defines the current system and must be set 127.0.0.1 tux.homenetwork tux localhost # Optional definition of extra systems on the network 192.168.0.5 jenny.homenetwork jenny 192.168.0.6 benny.homenetwork benny System information \u00b6 Root password \u00b6 root #passwd Init and boot configuration \u00b6 When using OpenRC with Gentoo, it uses /etc/rc.conf to configure the services, startup, and shutdown of a system. Open up /etc/rc.conf and enjoy all the comments in the file. Review the settings and change where needed. OpenRC \u00b6 root #nano /etc/rc.conf Next, open /etc/conf.d/keymaps to handle keyboard configuration. Edit it to configure and select the right keyboard. root #nano /etc/conf.d/keymaps Take special care with the keymap variable. If the wrong keymap is selected, then weird results will come up when typing on the keyboard. Finally, edit /etc/conf.d/hwclock to set the clock options. Edit it according to personal preference. root #nano /etc/conf.d/hwclock If the hardware clock is not using UTC, then it is necessary to set clock=\"local\" in the file. Otherwise the system might show clock skew behavior. systemd systemd \u00b6 First, it is recommended to run systemd-firstboot which will prepare various components of the system are set correctly for the first boot into the new systemd environment. The passing the following options will include a prompt for the user to set a locale, timezone, hostname, root password, and root shell values. It will also assign a random machine ID to the installation: root #systemd-firstboot --prompt --setup-machine-id root #systemctl preset-all --preset-mode=enable-only root #systemctl preset-all","title":"System"},{"location":"Linux/Gentoo/System/#configuring-the-system","text":"","title":"Configuring the system"},{"location":"Linux/Gentoo/System/#filesystem-information","text":"","title":"Filesystem information"},{"location":"Linux/Gentoo/System/#creating-the-fstab-file","text":"The /etc/fstab file uses a table-like syntax. Every line consists of six fields, separated by whitespace (space(s), tabs, or a mixture of the two). Each field has its own meaning: The first field shows the block special device or remote filesystem to be mounted. Several kinds of device identifiers are available for block special device nodes, including paths to device files, filesystem labels and UUIDs, and partition labels and UUIDs. The second field shows the mount point at which the partition should be mounted. The third field shows the type of filesystem used by the partition. The fourth field shows the mount options used by mount when it wants to mount the partition. As every filesystem has its own mount options, so system admins are encouraged to read the mount man page (man mount) for a full listing. Multiple mount options are comma-separated. The fifth field is used by dump to determine if the partition needs to be dumped or not. This can generally be left as 0 (zero). The sixth field is used by fsck to determine the order in which filesystems should be checked if the system wasn't shut down properly. The root filesystem should have 1 while the rest should have 2 (or 0 if a filesystem check is not necessary). root #nano /etc/fstab","title":"Creating the fstab file"},{"location":"Linux/Gentoo/System/#filesystem-labels-and-uuids","text":"Both MBR (BIOS) and GPT include support for filesystem labels and filesystem UUIDs. These attributes can be defined in /etc/fstab as alternatives for the mount command to use when attempting to find and mount block devices. Filesystem labels and UUIDs are identified by the LABEL and UUID prefix and can be viewed with the blkid command: root #blkid Danger If the filesystem inside a partition is wiped, then the filesystem label and the UUID values will be subsequently altered or removed. Because of uniqueness, readers that are using an MBR-style partition table are recommended to use UUIDs over labels to define mountable volumes in /etc/fstab. Info UUIDs of the filesystem on a LVM volume and its LVM snapshots are identical, therefore using UUIDs to mount LVM volumes should be avoided.","title":"Filesystem labels and UUIDs"},{"location":"Linux/Gentoo/System/#partition-labels-and-uuids","text":"Users who have gone the GPT route have a couple more 'robust' options available to define partitions in /etc/fstab. Partition labels and partition UUIDs can be used to identify the block device's individual partition(s), regardless of what filesystem has been chosen for the partition itself. Partition labels and UUIDs are identified by the PARTLABEL and PARTUUID prefixes respectively and can be viewed nicely in the terminal by running the blkid command: root #blkid Below is a more elaborate example of an /etc/fstab file: A full /etc/fstab example FILE /etc/fstab Adjust any formatting difference and additional partitions created from the Preparing the disks step filesystem dir type options jump pass /dev/sda1 /boot vfat defaults,noatime 0 2 /dev/sda2 none swap sw 0 0 /dev/sda3 / ext4 noatime 0 1 /dev/cdrom /mnt/cdrom auto noauto,user 0 0 When auto is used in the third field, it makes the mount command guess what the filesystem would be. This is recommended for removable media as they can be created with one of many filesystems. The user option in the fourth field makes it possible for non-root users to mount the CD. To improve performance, most users would want to add the noatime mount option, which results in a faster system since access times are not registered (those are not needed generally anyway). This is also recommended for systems with solid state drives (SSDs). Due to degradation in performance, defining the discard mount option in /etc/fstab is not recommended. It is generally better to schedule block discards on a periodic basis using a job scheduler such as cron or a timer (systemd). See Periodic fstrim jobs for more information.","title":"Partition labels and UUIDs"},{"location":"Linux/Gentoo/System/#networking-information","text":"For systems running OpenRC, a more detailed reference for network setup is available in the advanced network configuration section, which is covered near the end of the handbook. Systems with more specific network needs may need to skip ahead, then return here to continue with the rest of the installation. For more specific systemd network setup, please review see the networking portion of the systemd article.","title":"Networking information"},{"location":"Linux/Gentoo/System/#hostname","text":"","title":"Hostname"},{"location":"Linux/Gentoo/System/#set-the-hostname-openrc-or-systemd","text":"root #echo tux > /etc/hostname","title":"Set the hostname (OpenRC or systemd)"},{"location":"Linux/Gentoo/System/#systemd","text":"root #hostnamectl hostname tux","title":"systemd"},{"location":"Linux/Gentoo/System/#network","text":"","title":"Network"},{"location":"Linux/Gentoo/System/#dhcp-via-dhcpcd-any-init-system","text":"root #emerge --ask net-misc/dhcpcd To enable and then start the service on OpenRC systems: root #rc-update add dhcpcd default root #rc-service dhcpcd start To enable and start the service on systmd system: root #systemctl enable --now dhcpcd All networking information is gathered in /etc/conf.d/net. It uses a straightforward - yet perhaps not intuitive - syntax. Do not fear! Everything is explained below. A fully commented example that covers many different configurations is available in /usr/share/doc/netifrc-*/net.example.bz2. First install net-misc/netifrc root #emerge --ask --noreplace net-misc/netifrc If the network connection needs to be configured because of specific DHCP options or because DHCP is not used at all, then open /etc/conf.d/net: root #nano /etc/conf.d/net Set both config_eth0 and routes_eth0 to enter IP address information and routing information: This assumes that the network interface will be called eth0. This is, however, very system dependent. It is recommended to assume that the interface is named the same as the interface name when booted from the installation media if the installation media is sufficiently recent. More information can be found in the Network interface naming section. Static IP definition FILE /etc/conf.d/net config_eth0=\"192.168.0.2 netmask 255.255.255.0 brd 192.168.0.255\" routes_eth0=\"default via 192.168.0.1\" To use DHCP, define config_eth0: DHCP definition FILE /etc/conf.d/net config_eth0=\"dhcp\" Go over /usr/share/doc/netifrc-*/net.example.bz2 for a list of additional configuration options. Be sure to also read up on the DHCP client man page if specific DHCP options need to be set.","title":"DHCP via dhcpcd (any init system)"},{"location":"Linux/Gentoo/System/#automatically-start-networking-at-boot","text":"root #cd /etc/init.d root #ln -s net.lo net.eth0 root #rc-update add net.eth0 default Infor If the system has several network interfaces, then the appropriate net.* files need to be created just like we did with net.eth0. If, after booting the system, it is discovered the network interface name (which is currently documented as eth0) was wrong, then execute the following steps to rectify: Update the /etc/conf.d/net file with the correct interface name (like enp3s0 or enp5s0, instead of eth0). Create new symbolic link (like /etc/init.d/net.enp3s0). Remove the old symbolic link ( rm /etc/init.d/net.eth0 ). Add the new one to the default runlevel. Remove the old one using rc-update del net.eth0 default .","title":"Automatically start networking at boot"},{"location":"Linux/Gentoo/System/#the-hosts-file","text":"root #nano /etc/hosts Filling in the network information FILE /etc/hosts # This defines the current system and must be set 127.0.0.1 tux.homenetwork tux localhost # Optional definition of extra systems on the network 192.168.0.5 jenny.homenetwork jenny 192.168.0.6 benny.homenetwork benny","title":"The hosts file"},{"location":"Linux/Gentoo/System/#system-information","text":"","title":"System information"},{"location":"Linux/Gentoo/System/#root-password","text":"root #passwd","title":"Root password"},{"location":"Linux/Gentoo/System/#init-and-boot-configuration","text":"When using OpenRC with Gentoo, it uses /etc/rc.conf to configure the services, startup, and shutdown of a system. Open up /etc/rc.conf and enjoy all the comments in the file. Review the settings and change where needed.","title":"Init and boot configuration"},{"location":"Linux/Gentoo/System/#openrc","text":"root #nano /etc/rc.conf Next, open /etc/conf.d/keymaps to handle keyboard configuration. Edit it to configure and select the right keyboard. root #nano /etc/conf.d/keymaps Take special care with the keymap variable. If the wrong keymap is selected, then weird results will come up when typing on the keyboard. Finally, edit /etc/conf.d/hwclock to set the clock options. Edit it according to personal preference. root #nano /etc/conf.d/hwclock If the hardware clock is not using UTC, then it is necessary to set clock=\"local\" in the file. Otherwise the system might show clock skew behavior. systemd","title":"OpenRC"},{"location":"Linux/Gentoo/System/#systemd_1","text":"First, it is recommended to run systemd-firstboot which will prepare various components of the system are set correctly for the first boot into the new systemd environment. The passing the following options will include a prompt for the user to set a locale, timezone, hostname, root password, and root shell values. It will also assign a random machine ID to the installation: root #systemd-firstboot --prompt --setup-machine-id root #systemctl preset-all --preset-mode=enable-only root #systemctl preset-all","title":"systemd"},{"location":"Linux/Gentoo/Tools/","text":"Installing tools \u00b6 System logger \u00b6 OpenRC \u00b6 Some tools are missing from the stage3 archive because several packages provide the same functionality. It is now up to the user to choose which ones to install. Gentoo offers several system logger utilities. A few of these include: app-admin/sysklogd - Offers the traditional set of system logging daemons. The default logging configuration works well out of the box which makes this package a good option for beginners. app-admin/syslog-ng - An advanced system logger. Requires additional configuration for anything beyond logging to one big file. More advanced users may choose this package based on its logging potential; be aware additional configuration is a necessity for any kind of smart logging. app-admin/metalog - A highly-configurable system logger. There may be other system logging utilities available through the Gentoo ebuild repository as well, since the number of available packages increases on a daily basis. If syslog-ng is going to be used, it is recommended to install and configure logrotate. syslog-ng does not provide any rotation mechanism for the log files. Newer versions (>= 2.0) of sysklogd however handle their own log rotation. To install the system logger of choice, emerge it. On OpenRC, add it to the default runlevel using rc-update. The following example installs and activates app-admin/sysklogd as the system's syslog utility: root #emerge --ask app-admin/sysklogd root #rc-update add sysklogd default systemd \u00b6 While a selection of logging mechanisms are presented for OpenRC-based systems, systemd includes a built-in logger called the systemd-journald service. The systemd-journald service is capable of handling most of the logging functionality outlined in the previous system logger section. That is to say, the majority of installations that will run systemd as the system and service manager can safely skip adding a additional syslog utilities. See man journalctl for more details on using journalctl to query and review the systems logs. For a number of reasons, such as the case of forwarding logs to a central host, it may be important to include redundant system logging mechanisms on a systemd-based system. This is a irregular occurrence for the handbook's typical audience and considered an advanced use case. It is therefore not covered by the handbook. File indexing \u00b6 In order to index the file system to provide faster file location capabilities, install sys-apps/mlocate . Remote shell access \u00b6 opensshd's default configuration does not allow root to login as a remote user. Please create a non-root user and configure it appropriately to allow access post-installation if required, or adjust /etc/ssh/sshd_config to allow root. To be able to access the system remotely after installation, sshd must be configured to start on boot. OpenRC \u00b6 To add the sshd init script to the default runlevel on OpenRC: root #rc-update add sshd default If serial console access is needed (which is possible in case of remote servers), agetty must be configured. Uncomment the serial console section in /etc/inittab: root #nano -w /etc/inittab # SERIAL CONSOLES s0:12345:respawn:/sbin/agetty 9600 ttyS0 vt100 s1:12345:respawn:/sbin/agetty 9600 ttyS1 vt100 systemd \u00b6 To enable the SSH server, run: root #systemctl enable sshd root #systemctl enable getty@tty1.service Time synchronization \u00b6 It is important to use some method of synchronizing the system clock. This is usually done via the NTP protocol and software. Other implementations using the NTP protocol exist, like Chrony . root #emerge --ask net-misc/chrony OpenRC \u00b6 root #rc-update add chronyd default systemd \u00b6 root #systemctl enable chronyd.service root #systemctl enable systemd-timesyncd.service Filesystem tools \u00b6 Depending on the filesystems used, it is necessary to install the required file system utilities (for checking the filesystem integrity, creating additional file systems etc.). Note that tools for managing ext4 filesystems ( sys-fs/e2fsprogs ) are already installed as a part of the @system set . The following table lists the tools to install if a certain filesystem is used: Filesystem Package Ext4 sys-fs/e2fsprogs XFS sys-fs/xfsprogs ReiserFS sys-fs/reisefsprogs JFS sys-fs/jfsutils VFAT(FAT32,...) sys-fs/dosfstools Btrfs sys-fs/btrfs-progs ZFS sys-fs/zfs For more information on filesystems in Gentoo see the filesystem article . Networking tools \u00b6 If networking was previously configured in the Configuring the system step and network setup is complete, then this 'networking tools' section can be safely skipped. In this case, proceed with the section on Configuring a bootloader . Installing a DHCP client \u00b6 Warning Most users will need a DHCP client to connect to their network. If none was installed, then the system might not be able to get on the network thus making it impossible to download a DHCP client afterwards. root #emerge --ask net-misc/dhcpcd Installing a PPPoE client \u00b6 root #emerge --ask net-dialup/ppp Installing wireless networking tools \u00b6 If the system will be connecting to wireless networks, install the net-wireless/iw package for Open or WEP networks and/or the net-wireless/wpa_supplicant package for WPA or WPA2 networks. iw is also a useful basic diagnostic tool for scanning wireless networks. root #emerge --ask net-wireless/iw net-wireless/wpa_supplicant","title":"Tools"},{"location":"Linux/Gentoo/Tools/#installing-tools","text":"","title":"Installing tools"},{"location":"Linux/Gentoo/Tools/#system-logger","text":"","title":"System logger"},{"location":"Linux/Gentoo/Tools/#openrc","text":"Some tools are missing from the stage3 archive because several packages provide the same functionality. It is now up to the user to choose which ones to install. Gentoo offers several system logger utilities. A few of these include: app-admin/sysklogd - Offers the traditional set of system logging daemons. The default logging configuration works well out of the box which makes this package a good option for beginners. app-admin/syslog-ng - An advanced system logger. Requires additional configuration for anything beyond logging to one big file. More advanced users may choose this package based on its logging potential; be aware additional configuration is a necessity for any kind of smart logging. app-admin/metalog - A highly-configurable system logger. There may be other system logging utilities available through the Gentoo ebuild repository as well, since the number of available packages increases on a daily basis. If syslog-ng is going to be used, it is recommended to install and configure logrotate. syslog-ng does not provide any rotation mechanism for the log files. Newer versions (>= 2.0) of sysklogd however handle their own log rotation. To install the system logger of choice, emerge it. On OpenRC, add it to the default runlevel using rc-update. The following example installs and activates app-admin/sysklogd as the system's syslog utility: root #emerge --ask app-admin/sysklogd root #rc-update add sysklogd default","title":"OpenRC"},{"location":"Linux/Gentoo/Tools/#systemd","text":"While a selection of logging mechanisms are presented for OpenRC-based systems, systemd includes a built-in logger called the systemd-journald service. The systemd-journald service is capable of handling most of the logging functionality outlined in the previous system logger section. That is to say, the majority of installations that will run systemd as the system and service manager can safely skip adding a additional syslog utilities. See man journalctl for more details on using journalctl to query and review the systems logs. For a number of reasons, such as the case of forwarding logs to a central host, it may be important to include redundant system logging mechanisms on a systemd-based system. This is a irregular occurrence for the handbook's typical audience and considered an advanced use case. It is therefore not covered by the handbook.","title":"systemd"},{"location":"Linux/Gentoo/Tools/#file-indexing","text":"In order to index the file system to provide faster file location capabilities, install sys-apps/mlocate .","title":"File indexing"},{"location":"Linux/Gentoo/Tools/#remote-shell-access","text":"opensshd's default configuration does not allow root to login as a remote user. Please create a non-root user and configure it appropriately to allow access post-installation if required, or adjust /etc/ssh/sshd_config to allow root. To be able to access the system remotely after installation, sshd must be configured to start on boot.","title":"Remote shell access"},{"location":"Linux/Gentoo/Tools/#openrc_1","text":"To add the sshd init script to the default runlevel on OpenRC: root #rc-update add sshd default If serial console access is needed (which is possible in case of remote servers), agetty must be configured. Uncomment the serial console section in /etc/inittab: root #nano -w /etc/inittab # SERIAL CONSOLES s0:12345:respawn:/sbin/agetty 9600 ttyS0 vt100 s1:12345:respawn:/sbin/agetty 9600 ttyS1 vt100","title":"OpenRC"},{"location":"Linux/Gentoo/Tools/#systemd_1","text":"To enable the SSH server, run: root #systemctl enable sshd root #systemctl enable getty@tty1.service","title":"systemd"},{"location":"Linux/Gentoo/Tools/#time-synchronization","text":"It is important to use some method of synchronizing the system clock. This is usually done via the NTP protocol and software. Other implementations using the NTP protocol exist, like Chrony . root #emerge --ask net-misc/chrony","title":"Time synchronization"},{"location":"Linux/Gentoo/Tools/#openrc_2","text":"root #rc-update add chronyd default","title":"OpenRC"},{"location":"Linux/Gentoo/Tools/#systemd_2","text":"root #systemctl enable chronyd.service root #systemctl enable systemd-timesyncd.service","title":"systemd"},{"location":"Linux/Gentoo/Tools/#filesystem-tools","text":"Depending on the filesystems used, it is necessary to install the required file system utilities (for checking the filesystem integrity, creating additional file systems etc.). Note that tools for managing ext4 filesystems ( sys-fs/e2fsprogs ) are already installed as a part of the @system set . The following table lists the tools to install if a certain filesystem is used: Filesystem Package Ext4 sys-fs/e2fsprogs XFS sys-fs/xfsprogs ReiserFS sys-fs/reisefsprogs JFS sys-fs/jfsutils VFAT(FAT32,...) sys-fs/dosfstools Btrfs sys-fs/btrfs-progs ZFS sys-fs/zfs For more information on filesystems in Gentoo see the filesystem article .","title":"Filesystem tools"},{"location":"Linux/Gentoo/Tools/#networking-tools","text":"If networking was previously configured in the Configuring the system step and network setup is complete, then this 'networking tools' section can be safely skipped. In this case, proceed with the section on Configuring a bootloader .","title":"Networking tools"},{"location":"Linux/Gentoo/Tools/#installing-a-dhcp-client","text":"Warning Most users will need a DHCP client to connect to their network. If none was installed, then the system might not be able to get on the network thus making it impossible to download a DHCP client afterwards. root #emerge --ask net-misc/dhcpcd","title":"Installing a DHCP client"},{"location":"Linux/Gentoo/Tools/#installing-a-pppoe-client","text":"root #emerge --ask net-dialup/ppp","title":"Installing a PPPoE client"},{"location":"Linux/Gentoo/Tools/#installing-wireless-networking-tools","text":"If the system will be connecting to wireless networks, install the net-wireless/iw package for Open or WEP networks and/or the net-wireless/wpa_supplicant package for WPA or WPA2 networks. iw is also a useful basic diagnostic tool for scanning wireless networks. root #emerge --ask net-wireless/iw net-wireless/wpa_supplicant","title":"Installing wireless networking tools"},{"location":"Linux/Network/NetCat/","text":"NetCat \u00b6 Netcat (or nc) is a command-line utility that reads and writes data across network connections, using the TCP or UDP protocols. It is one of the most powerful tools in the network and system administrators arsenal, and it as considered as a Swiss army knife of networking tools. \u00b6 Syntax \u00b6 nc [ option ] host port By default, Netcat will attempt to start a TCP connection to the specified host and port. If you would like toestablish a UDP connection, use the -u option: nc -u host port Example \u00b6 To scan for open ports in the range 20-80 you would use the following command: nc -zv $IP $PORT # Print only the lines with the open ports, filter the results with the grep command. nc -zv $IP $PORT 2 > & 1 | grep succeeded # Output Connection to 10 .10.8.8 22 port [ tcp/ssh ] succeeded! Connection to 10 .10.8.8 80 port [ tcp/http ] succeeded! \u00b6","title":"Network"},{"location":"Linux/Network/NetCat/#netcat","text":"","title":"NetCat"},{"location":"Linux/Network/NetCat/#netcat-or-nc-is-a-command-line-utility-that-reads-and-writes-data-across-network-connections-using-the-tcp-or-udp-protocols-it-is-one-of-the-most-powerful-tools-in-the-network-and-system-administrators-arsenal-and-it-as-considered-as-a-swiss-army-knife-of-networking-tools","text":"","title":"Netcat (or nc) is a command-line utility that reads and writes data across network connections, using the TCP or UDP protocols. It is one of the most powerful tools in the network and system administrators arsenal, and it as considered as a Swiss army knife of networking tools."},{"location":"Linux/Network/NetCat/#syntax","text":"nc [ option ] host port By default, Netcat will attempt to start a TCP connection to the specified host and port. If you would like toestablish a UDP connection, use the -u option: nc -u host port","title":"Syntax"},{"location":"Linux/Network/NetCat/#example","text":"To scan for open ports in the range 20-80 you would use the following command: nc -zv $IP $PORT # Print only the lines with the open ports, filter the results with the grep command. nc -zv $IP $PORT 2 > & 1 | grep succeeded # Output Connection to 10 .10.8.8 22 port [ tcp/ssh ] succeeded! Connection to 10 .10.8.8 80 port [ tcp/http ] succeeded!","title":"Example"},{"location":"Linux/Network/NetCat/#_1","text":"","title":""},{"location":"blog/","text":".md-sidebar--secondary:not([hidden]) { visibility: hidden; } Wd Skype \u00b6 This is the story of the time onboard. Most of in spirite on stay on focus and pay aggressive for the next step! Darmarj M \u00b7 @darmarj November 19, 2021 \u00b7 15 min read \u00b7 [ 7.2.6+insiders-3.0.0][insiders-3.0.0] A daemon for Linux and Windows. It manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments and beyond. Continue reading","title":"Index"},{"location":"blog/#wd-skype","text":"This is the story of the time onboard. Most of in spirite on stay on focus and pay aggressive for the next step! Darmarj M \u00b7 @darmarj November 19, 2021 \u00b7 15 min read \u00b7 [ 7.2.6+insiders-3.0.0][insiders-3.0.0] A daemon for Linux and Windows. It manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments and beyond. Continue reading","title":"Wd Skype"},{"location":"blog/2021/11/CNCF-Containerd/CNCF-Containerd/","text":"Cloud Native-Containerd \u00b6 A daemon for Linux and Windows. It manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments and beyond. Architecture \u00b6","title":"Cloud Native-Containerd"},{"location":"blog/2021/11/CNCF-Containerd/CNCF-Containerd/#cloud-native-containerd","text":"A daemon for Linux and Windows. It manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments and beyond.","title":"Cloud Native-Containerd"},{"location":"blog/2021/11/CNCF-Containerd/CNCF-Containerd/#architecture","text":"","title":"Architecture"},{"location":"blog/2021/12/Proxmox/Proxmox-Source/","text":"Proxmox v8 Source in PandaWorld \u00b6 Usage \u00b6 Note comment the context for disable the ENTERPRISE repo in following file nano /etc/apt/sources.list.d/pve-enterprise.list Domestic ATP resource of ustc \u00b6 # GPG key sync up wget https://mirrors.ustc.edu.cn/proxmox/debian/proxmox-release-bookworm.gpg -O /etc/apt/trusted.gpg.d/proxmox-release-bookworm.gpg # The apt resource for no-subscription echo \"deb https://mirrors.ustc.edu.cn/proxmox/debian bookworm pve-no-subscription\" > /etc/apt/sources.list.d/pve-no-subscription.list # Replace the ustc mirrors on latest Proxmox Debian system sed -i 's|^deb http://ftp.debian.org|deb https://mirrors.ustc.edu.cn|g' /etc/apt/sources.list sed -i 's|^deb http://security.debian.org|deb https://mirrors.ustc.edu.cn/debian-security|g' /etc/apt/sources.list # Replace the Ceph resource echo \"deb https://mirrors.ustc.edu.cn/proxmox/debian/ceph-quincy bookworm no-subscription\" > /etc/apt/sources.list.d/ceph.list # Replace the CT(Container) resource sed -i 's|http://download.proxmox.com|https://mirrors.ustc.edu.cn/proxmox|g' /usr/share/perl5/PVE/APLInfo.pm Proxmox v7 Source in PandaWorld \u00b6 Usage \u00b6 Note Moving into /etc/apt/sources.list.d/pve-enterprise.list and comments the original enterprise sources as below: echo \"#deb https://enterprise.proxmox.com/debian/pve bullseye pve-enterprise\" > /etc/apt/sources.list.d/pve-enterprise.list Domestic APT resource \u00b6 vi /etc/apt/sources.list ```markdown deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-free deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-free deb https://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-free deb-src https://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-free","title":"Proxmox v8 Source in PandaWorld"},{"location":"blog/2021/12/Proxmox/Proxmox-Source/#proxmox-v8-source-in-pandaworld","text":"","title":"Proxmox v8 Source in PandaWorld"},{"location":"blog/2021/12/Proxmox/Proxmox-Source/#usage","text":"Note comment the context for disable the ENTERPRISE repo in following file nano /etc/apt/sources.list.d/pve-enterprise.list","title":"Usage"},{"location":"blog/2021/12/Proxmox/Proxmox-Source/#domestic-atp-resource-of-ustc","text":"# GPG key sync up wget https://mirrors.ustc.edu.cn/proxmox/debian/proxmox-release-bookworm.gpg -O /etc/apt/trusted.gpg.d/proxmox-release-bookworm.gpg # The apt resource for no-subscription echo \"deb https://mirrors.ustc.edu.cn/proxmox/debian bookworm pve-no-subscription\" > /etc/apt/sources.list.d/pve-no-subscription.list # Replace the ustc mirrors on latest Proxmox Debian system sed -i 's|^deb http://ftp.debian.org|deb https://mirrors.ustc.edu.cn|g' /etc/apt/sources.list sed -i 's|^deb http://security.debian.org|deb https://mirrors.ustc.edu.cn/debian-security|g' /etc/apt/sources.list # Replace the Ceph resource echo \"deb https://mirrors.ustc.edu.cn/proxmox/debian/ceph-quincy bookworm no-subscription\" > /etc/apt/sources.list.d/ceph.list # Replace the CT(Container) resource sed -i 's|http://download.proxmox.com|https://mirrors.ustc.edu.cn/proxmox|g' /usr/share/perl5/PVE/APLInfo.pm","title":"Domestic ATP resource of ustc"},{"location":"blog/2021/12/Proxmox/Proxmox-Source/#proxmox-v7-source-in-pandaworld","text":"","title":"Proxmox v7 Source in PandaWorld"},{"location":"blog/2021/12/Proxmox/Proxmox-Source/#usage_1","text":"Note Moving into /etc/apt/sources.list.d/pve-enterprise.list and comments the original enterprise sources as below: echo \"#deb https://enterprise.proxmox.com/debian/pve bullseye pve-enterprise\" > /etc/apt/sources.list.d/pve-enterprise.list","title":"Usage"},{"location":"blog/2021/12/Proxmox/Proxmox-Source/#domestic-apt-resource","text":"vi /etc/apt/sources.list ```markdown deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-free deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-free deb https://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-free deb-src https://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-free","title":"Domestic APT resource"},{"location":"blog/2021/12/Proxmox/Proxmox-Subscript/","text":"Proxmox v7 Subscription \u00b6 Usage \u00b6 Note Move into /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js and chage as following: Code if ( res === null || res === undefined ||! res || res . data . status . toLowerCase () !== 'active' ) { if ( false ) { systemctl restart pveproxy.service reboot","title":"Proxmox v7 Subscription"},{"location":"blog/2021/12/Proxmox/Proxmox-Subscript/#proxmox-v7-subscription","text":"","title":"Proxmox v7 Subscription"},{"location":"blog/2021/12/Proxmox/Proxmox-Subscript/#usage","text":"Note Move into /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js and chage as following: Code if ( res === null || res === undefined ||! res || res . data . status . toLowerCase () !== 'active' ) { if ( false ) { systemctl restart pveproxy.service reboot","title":"Usage"},{"location":"blog/2022/01/Bitwarden/","text":"vaultBitwarden \u00b6 This is a Bitwarden server API implementation written in Rust compatible with upstream Bitwarden clients*, perfect for self-hosted deployment where running the official resource-heavy service might not be ideal. Features \u00b6 Basically full implementation of Bitwarden API is provided including: Basic single user functionality Organizations support Attachments Vault API support Serving the static files for Vault interface Website icons API Authenticator and U2F support YubiKey OTP Installation \u00b6 Work_Dir: mkdir ${ WORK_DIR } Cert: certification openssl req -x509 -newkey rsa:4096 -keyout ${ NAME } .key \\ -out ${ NAME } .crt -days 720 -nodes Container: docker run # Get into the Work_Dir and run following dockers command docker run -d --restart always --name ${ CONTAINER_NAME } -e \\ ROCKET_TLS = '{certs=\"/${SSL_PATH}/${SSL_NAME}.crt\",key=\"/${SSL_PATH}/${SSL_NAME}.key\"}' \\ -v / $HOME /vaultwarden/ssl/:/ssl/ \\ -v / $HOME /vaultwarden/vw-data:/data/ \\ -p 443 :80 vaultwarden/server:latest Warning ${SSL_PATH} should be resided at the same path in Work_Dir when invoke the docker run. Note -e set the environment -v set the volume -p set the port DB Backup : Systemd Bash DB Crontab systemd.service [Unit] Description = bitwarden-watch-for-changes [Service] Type = simple ExecStart = /home/USER/bitwardenrs/watch-for-changes.sh Restart = always WorkingDirectory = /home/USER [Install] WantedBy = multi-user.target systemd.sh sudo sed -i 's/^SELINUX=.*/SELINUX=[disabled|permissive]/g' /etc/selinux/config && sudo sestatus sudo reboot sudo cp /PATH/bitwarden-watch-for-changes.service /etc/systemd/system/multi-user.target.wants sudo systemctl enable/start/status bitwarden-watch-for-changes.service sudo journalctl //troubleshoot sudo chmod +x watch-for-changes.shd watcher.sh #!/usr/bin/env bash export PATH = /usr/local/inotify-tools/bin/ path_to_watch = /home/USER/bitwardenrs/bw-data inotifywait -m \" $path_to_watch \" -e create -e moved_to -e modify | while read path action file ; do if [[ $file = ~ ( wal | config.json ) ]] ; then echo '1' > /home/USER/bitwardenrs/bitwarden-folder-updated ; fi done inotify-tools : inotify's features to be used from within shell scripts dnf -y install autoconf automake libtool wget -c https://github.com/inotify-tools/inotify-tools/archive/refs/tags/3.21.9.6.tar.gz tar -zvxf 3 .21.9.6.tar.gz -C /usr/local/src/ cd /usr/local/src/inotify-tools-3.21.9.6/ ./autogen.sh && \\ ./configure --prefix = /usr/local/inotify-tools && \\ make && \\ make install ls /usr/local/inotify-tools/bin/ Github Reference db-backup.sh #!/usr/bin/env bash set -e bitwarden_folder_updated = /home/USER/bitwarden/bitwarden-folder-updated touch $bitwarden_folder_updated if [[ \" $( cat $bitwarden_folder_updated ) \" == \"1\" ]] ; then rm -f /home/USER/bw-bk.tar.gz docker exec bitwarden bash -c 'mkdir -p /data/db-backup && sqlite3 /data/db.sqlite3 \".backup /data/db-backup/backup.sqlite3\"' cd /home/USER/bitwarden/bw-data tar -czvf /home/USER/bw-bk.tar.gz \\ config.json \\ icon_cache \\ db-backup/backup.sqlite3 cd /home/USER/bitwarden/ tar -czvf /home/USER/bw-scripts.tar.gz \\ backup.sh \\ bitwarden-watch-for-changes.service \\ watch-for-changes.sh mv /home/USER/bw-bk.tar.gz /SMB,NFS.AW3.../BitwardenBak/bw-bk- $( date + \"%Y-%m-%d\" ) .tar.gz mv /home/USER/bw-scripts.tar.gz /SMB,NFS.AW3.../BitwardenBak/bw-scripts- $( date + \"%Y-%m-%d\" ) .tar.gz echo \"0\" > /home/USER/bitwarden/bitwarden-folder-updated else echo 'nothing to backup' fi Sqlite3 sqlite3 : curl the source and complie. sqlite3 in docker env: # dnf install make, gcc # tar xvfz sqlite-autoconf-<version>.tar.gz # cd sqlite-autoconf-<version> # ./configure # make # make install # docker exec -it CONTAINER bash -c 'cp /datasource/sqlite3 /usr/local/bin' Example crontab -e 0 3 * * * $PATH /*.sh > /dev/null 2 > & 1 # Rsync on 3:00 AM 0 * * * * $PATH /*.sh > /dev/null 2 > & 1 # Rsync every hour crontab -l # List the schedule","title":"vaultBitwarden"},{"location":"blog/2022/01/Bitwarden/#vaultbitwarden","text":"This is a Bitwarden server API implementation written in Rust compatible with upstream Bitwarden clients*, perfect for self-hosted deployment where running the official resource-heavy service might not be ideal.","title":"vaultBitwarden"},{"location":"blog/2022/01/Bitwarden/#features","text":"Basically full implementation of Bitwarden API is provided including: Basic single user functionality Organizations support Attachments Vault API support Serving the static files for Vault interface Website icons API Authenticator and U2F support YubiKey OTP","title":"Features"},{"location":"blog/2022/01/Bitwarden/#installation","text":"Work_Dir: mkdir ${ WORK_DIR } Cert: certification openssl req -x509 -newkey rsa:4096 -keyout ${ NAME } .key \\ -out ${ NAME } .crt -days 720 -nodes Container: docker run # Get into the Work_Dir and run following dockers command docker run -d --restart always --name ${ CONTAINER_NAME } -e \\ ROCKET_TLS = '{certs=\"/${SSL_PATH}/${SSL_NAME}.crt\",key=\"/${SSL_PATH}/${SSL_NAME}.key\"}' \\ -v / $HOME /vaultwarden/ssl/:/ssl/ \\ -v / $HOME /vaultwarden/vw-data:/data/ \\ -p 443 :80 vaultwarden/server:latest Warning ${SSL_PATH} should be resided at the same path in Work_Dir when invoke the docker run. Note -e set the environment -v set the volume -p set the port DB Backup : Systemd Bash DB Crontab systemd.service [Unit] Description = bitwarden-watch-for-changes [Service] Type = simple ExecStart = /home/USER/bitwardenrs/watch-for-changes.sh Restart = always WorkingDirectory = /home/USER [Install] WantedBy = multi-user.target systemd.sh sudo sed -i 's/^SELINUX=.*/SELINUX=[disabled|permissive]/g' /etc/selinux/config && sudo sestatus sudo reboot sudo cp /PATH/bitwarden-watch-for-changes.service /etc/systemd/system/multi-user.target.wants sudo systemctl enable/start/status bitwarden-watch-for-changes.service sudo journalctl //troubleshoot sudo chmod +x watch-for-changes.shd watcher.sh #!/usr/bin/env bash export PATH = /usr/local/inotify-tools/bin/ path_to_watch = /home/USER/bitwardenrs/bw-data inotifywait -m \" $path_to_watch \" -e create -e moved_to -e modify | while read path action file ; do if [[ $file = ~ ( wal | config.json ) ]] ; then echo '1' > /home/USER/bitwardenrs/bitwarden-folder-updated ; fi done inotify-tools : inotify's features to be used from within shell scripts dnf -y install autoconf automake libtool wget -c https://github.com/inotify-tools/inotify-tools/archive/refs/tags/3.21.9.6.tar.gz tar -zvxf 3 .21.9.6.tar.gz -C /usr/local/src/ cd /usr/local/src/inotify-tools-3.21.9.6/ ./autogen.sh && \\ ./configure --prefix = /usr/local/inotify-tools && \\ make && \\ make install ls /usr/local/inotify-tools/bin/ Github Reference db-backup.sh #!/usr/bin/env bash set -e bitwarden_folder_updated = /home/USER/bitwarden/bitwarden-folder-updated touch $bitwarden_folder_updated if [[ \" $( cat $bitwarden_folder_updated ) \" == \"1\" ]] ; then rm -f /home/USER/bw-bk.tar.gz docker exec bitwarden bash -c 'mkdir -p /data/db-backup && sqlite3 /data/db.sqlite3 \".backup /data/db-backup/backup.sqlite3\"' cd /home/USER/bitwarden/bw-data tar -czvf /home/USER/bw-bk.tar.gz \\ config.json \\ icon_cache \\ db-backup/backup.sqlite3 cd /home/USER/bitwarden/ tar -czvf /home/USER/bw-scripts.tar.gz \\ backup.sh \\ bitwarden-watch-for-changes.service \\ watch-for-changes.sh mv /home/USER/bw-bk.tar.gz /SMB,NFS.AW3.../BitwardenBak/bw-bk- $( date + \"%Y-%m-%d\" ) .tar.gz mv /home/USER/bw-scripts.tar.gz /SMB,NFS.AW3.../BitwardenBak/bw-scripts- $( date + \"%Y-%m-%d\" ) .tar.gz echo \"0\" > /home/USER/bitwarden/bitwarden-folder-updated else echo 'nothing to backup' fi Sqlite3 sqlite3 : curl the source and complie. sqlite3 in docker env: # dnf install make, gcc # tar xvfz sqlite-autoconf-<version>.tar.gz # cd sqlite-autoconf-<version> # ./configure # make # make install # docker exec -it CONTAINER bash -c 'cp /datasource/sqlite3 /usr/local/bin' Example crontab -e 0 3 * * * $PATH /*.sh > /dev/null 2 > & 1 # Rsync on 3:00 AM 0 * * * * $PATH /*.sh > /dev/null 2 > & 1 # Rsync every hour crontab -l # List the schedule","title":"Installation"},{"location":"blog/2022/01/Hello%20Rocky/","text":"Rocky Linux \u00b6 Rocky Linux is an open-source enterprise operating system designed to be 100% bug-for-bug compatible with Red Hat Enterprise Linux\u00ae. It is under intensive development by the community. Mirror Resources \u00b6 Country Mirror Name Categories Bandwidth JP JAIST ftp.jaist.ac.jp 60000 CN eScience Center, Nanjing University mirrors.nju.edu.cn 10000 SG RPMDB.org sg.rpmdb.org 10000 Replace Method : sed -e 's|^mirrorlist=|#mirrorlist=|g' \\ -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://ftp.jaist.ac.jp/rocky|g' -i.bak \\ /etc/yum.repos.d/Rocky-*.repo dnf makecache Revert Method : sed -e 's|^#mirrorlist=|mirrorlist=|g' \\ -e 's|^baseurl=https://mirrors.sjtug.sjtu.edu.cn/rocky|#baseurl=http://dl.rockylinux.org/$contentdir|g' \\ -i.bak \\ /etc/yum.repos.d/Rocky-*.repo dnf makecache Docker \u00b6 dnf config-manager --add-repo = https://download.docker.com/linux/centos/docker-ce.repo && dnf update dnf install -y docker-ce docker-ce-cli containerd.io systemctl enable --now docker systemctl status docker Tip KMS Service: # docker pull mikolatero/vlmcsd # docker run -d -p 1688:1688 --restart=always --name vlmcsd mikolatero/vlmcsd Docker-Compose \u00b6 e.g. #VERSION = 1.29.2 sudo curl -L \"https://github.com/docker/compose/releases/download/#VERSION/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose","title":"Rocky Linux"},{"location":"blog/2022/01/Hello%20Rocky/#rocky-linux","text":"Rocky Linux is an open-source enterprise operating system designed to be 100% bug-for-bug compatible with Red Hat Enterprise Linux\u00ae. It is under intensive development by the community.","title":"Rocky Linux"},{"location":"blog/2022/01/Hello%20Rocky/#mirror-resources","text":"Country Mirror Name Categories Bandwidth JP JAIST ftp.jaist.ac.jp 60000 CN eScience Center, Nanjing University mirrors.nju.edu.cn 10000 SG RPMDB.org sg.rpmdb.org 10000 Replace Method : sed -e 's|^mirrorlist=|#mirrorlist=|g' \\ -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://ftp.jaist.ac.jp/rocky|g' -i.bak \\ /etc/yum.repos.d/Rocky-*.repo dnf makecache Revert Method : sed -e 's|^#mirrorlist=|mirrorlist=|g' \\ -e 's|^baseurl=https://mirrors.sjtug.sjtu.edu.cn/rocky|#baseurl=http://dl.rockylinux.org/$contentdir|g' \\ -i.bak \\ /etc/yum.repos.d/Rocky-*.repo dnf makecache","title":"Mirror Resources"},{"location":"blog/2022/01/Hello%20Rocky/#docker","text":"dnf config-manager --add-repo = https://download.docker.com/linux/centos/docker-ce.repo && dnf update dnf install -y docker-ce docker-ce-cli containerd.io systemctl enable --now docker systemctl status docker Tip KMS Service: # docker pull mikolatero/vlmcsd # docker run -d -p 1688:1688 --restart=always --name vlmcsd mikolatero/vlmcsd","title":"Docker"},{"location":"blog/2022/01/Hello%20Rocky/#docker-compose","text":"e.g. #VERSION = 1.29.2 sudo curl -L \"https://github.com/docker/compose/releases/download/#VERSION/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose","title":"Docker-Compose"},{"location":"blog/2022/02/HomePage/","text":"HomePage on React+Next \u00b6","title":"HomePage on React+Next"},{"location":"blog/2022/02/HomePage/#homepage-on-reactnext","text":"","title":"HomePage on React+Next"},{"location":"blog/2022/03/KVM%20Bridge/","text":"KVM Bridge \u00b6 Linux \u00b6 Use nmtui command to config the Bridge adapter on-premise. NAME nmtui - Text User Interface for controlling NetworkManager Click \"Edit a connection\" option to start walking through. Open the page once click \"Add\" button for \"Bridge\" creation Tips Leave the empty on the slave device for communication automatically by adapter and router. Delete the default wired connection and waiting a while. New Bridge connection will be activated soon later. btctl show bridge name bridge id STP enabled interfaces nm-bridge ID status adapterName","title":"KVM Bridge"},{"location":"blog/2022/03/KVM%20Bridge/#kvm-bridge","text":"","title":"KVM Bridge"},{"location":"blog/2022/03/KVM%20Bridge/#linux","text":"Use nmtui command to config the Bridge adapter on-premise. NAME nmtui - Text User Interface for controlling NetworkManager Click \"Edit a connection\" option to start walking through. Open the page once click \"Add\" button for \"Bridge\" creation Tips Leave the empty on the slave device for communication automatically by adapter and router. Delete the default wired connection and waiting a while. New Bridge connection will be activated soon later. btctl show bridge name bridge id STP enabled interfaces nm-bridge ID status adapterName","title":"Linux"},{"location":"blog/2022/03/Terraform%2BKVM/","text":"Terraform works on KVM for IaaS \u00b6 Terraform is an open-source infrastructure as code software tool that provides a consistent CLI workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files. Summary \u00b6 PopOS libvirt_domain.domain: Error creating libvirt domain : virError(Code=1, Domain=10, Message='internal error: process exited while connecting to monitor: 2019-01-28T02:29:14.861688Z qemu-system-x86_64: -drive file=/var/lib/libvirt/images/volume-0,format=qcow2,if=none,id=drive-virtio-disk0: Could not open '/var/lib/libvirt/images/volume-0': Permission denied') The reason on the problem : On Ubuntu/Debian distros SELinux is enforced by qemu even if it is disabled globally, this might cause unexpected Walkaround : #/etc/libvirt/qemu.conf ~ security_driver = \"none\" ~ sudo systemctl restart libvirtd.service","title":"Terraform works on KVM for IaaS"},{"location":"blog/2022/03/Terraform%2BKVM/#terraform-works-on-kvm-for-iaas","text":"Terraform is an open-source infrastructure as code software tool that provides a consistent CLI workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files.","title":"Terraform works on KVM for IaaS"},{"location":"blog/2022/03/Terraform%2BKVM/#summary","text":"PopOS libvirt_domain.domain: Error creating libvirt domain : virError(Code=1, Domain=10, Message='internal error: process exited while connecting to monitor: 2019-01-28T02:29:14.861688Z qemu-system-x86_64: -drive file=/var/lib/libvirt/images/volume-0,format=qcow2,if=none,id=drive-virtio-disk0: Could not open '/var/lib/libvirt/images/volume-0': Permission denied') The reason on the problem : On Ubuntu/Debian distros SELinux is enforced by qemu even if it is disabled globally, this might cause unexpected Walkaround : #/etc/libvirt/qemu.conf ~ security_driver = \"none\" ~ sudo systemctl restart libvirtd.service","title":"Summary"},{"location":"blog/2022/03/Transparent%20Gnome/","text":"Transparent Gnome Shell \u00b6 To use opacity/transparency occasionally - it is better use xprop command. \u00b6 Reference Link xprop -format _NET_WM_WINDOW_OPACITY 32c -set _NET_WM_WINDOW_OPACITY 0x7FFFFFFF 0x7FFFFFFF - 50% opacity 0xFFFFFFFF - 100% opacity One tiny shell script for reference: \u00b6 Reference Link #!/bin/bash read -p \"Set transparency percentage ? [Enter for 100%]\" mydectrans # only accept 10 to 99, rest is considered 100 [[ \" $mydectrans \" ! = [ 1 -9 ][ 0 -9 ] ]] && mydectrans = 100 # Convert decimal to 32bit hex representation my32bhextrans = $( printf 0x%x $(( 0xffffffff * $(( mydectrans )) / 100 ))) # Execute the action xprop -f _NET_WM_WINDOW_OPACITY 32c -set _NET_WM_WINDOW_OPACITY $my32bhextrans","title":"Transparent Gnome Shell"},{"location":"blog/2022/03/Transparent%20Gnome/#transparent-gnome-shell","text":"","title":"Transparent Gnome Shell"},{"location":"blog/2022/03/Transparent%20Gnome/#to-use-opacitytransparency-occasionally-it-is-better-use-xprop-command","text":"Reference Link xprop -format _NET_WM_WINDOW_OPACITY 32c -set _NET_WM_WINDOW_OPACITY 0x7FFFFFFF 0x7FFFFFFF - 50% opacity 0xFFFFFFFF - 100% opacity","title":"To use opacity/transparency occasionally - it is better use xprop command."},{"location":"blog/2022/03/Transparent%20Gnome/#one-tiny-shell-script-for-reference","text":"Reference Link #!/bin/bash read -p \"Set transparency percentage ? [Enter for 100%]\" mydectrans # only accept 10 to 99, rest is considered 100 [[ \" $mydectrans \" ! = [ 1 -9 ][ 0 -9 ] ]] && mydectrans = 100 # Convert decimal to 32bit hex representation my32bhextrans = $( printf 0x%x $(( 0xffffffff * $(( mydectrans )) / 100 ))) # Execute the action xprop -f _NET_WM_WINDOW_OPACITY 32c -set _NET_WM_WINDOW_OPACITY $my32bhextrans","title":"One tiny shell script for reference:"},{"location":"blog/2022/03/ibus-flypyInput/","text":"iBus \u00b6 IBus (Intelligent Input Bus) is an input method framework, a type of application that allows for easily switching between different keyboard layouts. When combined with an input method editor, it also allows for typing non-Latin characters using a keyboard that does not natively support them. Installation \u00b6 paru/yay ibus-rime paru/yay rime-double-pinyin environment (optional) # /etc/environment GTK_IM_MODULE = ibus QT_IM_MODULE = ibus XMODIFIERS = @im = ibus Rime \u00b6 Create the file of \"default.custom.yaml\" in the $HOME/.config/ibus/rime. # default.custom.yaml # save it to: # ~/.config/ibus/rime (linux) # ~/Library/Rime (macos) # %APPDATA%\\Rime (windows) patch : schema_list : - schema : luna_pinyin # \u6719\u6708\u62fc\u97f3 - schema : luna_pinyin_simp # \u6719\u6708\u62fc\u97f3 \u7b80\u5316\u5b57\u6a21\u5f0f - schema : double_pinyin_flypy # \u5c0f\u9db4\u96d9\u62fc - schema : emoji # emoji \u8868\u60c5 Wizard Configuration \u00b6 Ctrol+ ` AutoStart \u00b6 To launch IBus on user login, create an autostart entry with the following method: profile # /etc/profile.d cat /etc/profile.d/ibus.sh << EOF ibus-daemon -drxR EOF","title":"iBus"},{"location":"blog/2022/03/ibus-flypyInput/#ibus","text":"IBus (Intelligent Input Bus) is an input method framework, a type of application that allows for easily switching between different keyboard layouts. When combined with an input method editor, it also allows for typing non-Latin characters using a keyboard that does not natively support them.","title":"iBus"},{"location":"blog/2022/03/ibus-flypyInput/#installation","text":"paru/yay ibus-rime paru/yay rime-double-pinyin environment (optional) # /etc/environment GTK_IM_MODULE = ibus QT_IM_MODULE = ibus XMODIFIERS = @im = ibus","title":"Installation"},{"location":"blog/2022/03/ibus-flypyInput/#rime","text":"Create the file of \"default.custom.yaml\" in the $HOME/.config/ibus/rime. # default.custom.yaml # save it to: # ~/.config/ibus/rime (linux) # ~/Library/Rime (macos) # %APPDATA%\\Rime (windows) patch : schema_list : - schema : luna_pinyin # \u6719\u6708\u62fc\u97f3 - schema : luna_pinyin_simp # \u6719\u6708\u62fc\u97f3 \u7b80\u5316\u5b57\u6a21\u5f0f - schema : double_pinyin_flypy # \u5c0f\u9db4\u96d9\u62fc - schema : emoji # emoji \u8868\u60c5","title":"Rime"},{"location":"blog/2022/03/ibus-flypyInput/#wizard-configuration","text":"Ctrol+ `","title":"Wizard Configuration"},{"location":"blog/2022/03/ibus-flypyInput/#autostart","text":"To launch IBus on user login, create an autostart entry with the following method: profile # /etc/profile.d cat /etc/profile.d/ibus.sh << EOF ibus-daemon -drxR EOF","title":"AutoStart"},{"location":"blog/2022/06/Groff%20%26%20MOM/","text":"Groff & MOM \u00b6 Groff \u00b6 GNU troff (or groff) is a system for typesetting documents. troff is very flexible and has been used extensively for some thirty years. It is well entrenched in the Unix community. MOM \u00b6 A flexible typesetting and document formatting package that allows you to create high-quality Portable Document Format (.pdf) or PostScript (.ps) files for viewing and printing.) - mom is a flexible typesetting and document formatting package that allows you to create high-quality Portable Document Format (.pdf) or PostScript (.ps) files for viewing and printing.","title":"Groff & MOM"},{"location":"blog/2022/06/Groff%20%26%20MOM/#groff-mom","text":"","title":"Groff &amp; MOM"},{"location":"blog/2022/06/Groff%20%26%20MOM/#groff","text":"GNU troff (or groff) is a system for typesetting documents. troff is very flexible and has been used extensively for some thirty years. It is well entrenched in the Unix community.","title":"Groff"},{"location":"blog/2022/06/Groff%20%26%20MOM/#mom","text":"A flexible typesetting and document formatting package that allows you to create high-quality Portable Document Format (.pdf) or PostScript (.ps) files for viewing and printing.) - mom is a flexible typesetting and document formatting package that allows you to create high-quality Portable Document Format (.pdf) or PostScript (.ps) files for viewing and printing.","title":"MOM"},{"location":"blog/2023/05/OpenWRT/","text":"OpenWRT \u00b6 Parition switch \u00b6 root@lede:~# fw_printenv boot_part boot_part = 1 fw_setenv boot_part 2 reboot","title":"OpenWRT"},{"location":"blog/2023/05/OpenWRT/#openwrt","text":"","title":"OpenWRT"},{"location":"blog/2023/05/OpenWRT/#parition-switch","text":"root@lede:~# fw_printenv boot_part boot_part = 1 fw_setenv boot_part 2 reboot","title":"Parition switch"},{"location":"blog/2023/06/Secret-store/","text":"Secure-store \u00b6 GPG \u00b6 GnuPG is a complete and free implementation of the OpenPGP standard as defined by RFC4880 (also known as PGP). GnuPG allows you to encrypt and sign your data and communications; it features a versatile key management system, along with access modules for all kinds of public key directories. gpg --gen-key # edit expire if needed gpg --edit-key GPG_PUB_KEY # export GPG pub key gpg --output GPG_PUB_KEY --armor --export EMAIL_ID # export GPG pri key gpg --output GPG_PRI_KEY --armor --export-secret-key EMAIL_ID # import keys gpg --import GPG_PRI_KEY gpg --import GPG_PUB_KEY The standard unix password manager pass \u00b6 pass init GPG_PUB_KEY # init git repo for pass secret-store pass git init pass insert github pass generate aws pass show pass git log # set it up for sys env export key = $( pass show KEY_PATH/KEY_ID )","title":"Secure-store"},{"location":"blog/2023/06/Secret-store/#secure-store","text":"","title":"Secure-store"},{"location":"blog/2023/06/Secret-store/#gpg","text":"GnuPG is a complete and free implementation of the OpenPGP standard as defined by RFC4880 (also known as PGP). GnuPG allows you to encrypt and sign your data and communications; it features a versatile key management system, along with access modules for all kinds of public key directories. gpg --gen-key # edit expire if needed gpg --edit-key GPG_PUB_KEY # export GPG pub key gpg --output GPG_PUB_KEY --armor --export EMAIL_ID # export GPG pri key gpg --output GPG_PRI_KEY --armor --export-secret-key EMAIL_ID # import keys gpg --import GPG_PRI_KEY gpg --import GPG_PUB_KEY The standard unix password manager","title":"GPG"},{"location":"blog/2023/06/Secret-store/#pass","text":"pass init GPG_PUB_KEY # init git repo for pass secret-store pass git init pass insert github pass generate aws pass show pass git log # set it up for sys env export key = $( pass show KEY_PATH/KEY_ID )","title":"pass"},{"location":"blog/2023/06/SubEditor/","text":"SublimE (Unlimited) for Linux User \u00b6 bash \"sublime_text\" is in /opt/sublime_text/sublime_text Go to https://hexed.it/ Open and input \"sublime_text\" Search for \"80 78 05 00 0F 94 C1\" Change into \"C6 40 05 01 48 85 C9\" Do export sudo mv /opt/sublime_text/sublime_text /opt/sublime_text/sublime_text.old sudo mv $HOME /Downloads/sublime_text /opt/sublime_text/sublime_text sudo chmod 755 /opt/sublime_text/sublime_text sudo chown root:root /opt/sublime_text/sublime_text","title":"SublimE (Unlimited) for Linux User"},{"location":"blog/2023/06/SubEditor/#sublime-unlimited-for-linux-user","text":"bash \"sublime_text\" is in /opt/sublime_text/sublime_text Go to https://hexed.it/ Open and input \"sublime_text\" Search for \"80 78 05 00 0F 94 C1\" Change into \"C6 40 05 01 48 85 C9\" Do export sudo mv /opt/sublime_text/sublime_text /opt/sublime_text/sublime_text.old sudo mv $HOME /Downloads/sublime_text /opt/sublime_text/sublime_text sudo chmod 755 /opt/sublime_text/sublime_text sudo chown root:root /opt/sublime_text/sublime_text","title":"SublimE (Unlimited) for Linux User"},{"location":"blog/2023/06/TmuxReference/","text":"Tmux \u00b6 \ud83d\udd17 Cheat Sheet & Quick Reference \u00b6 Sessions \u00b6 Description Command Description Command Start a new settion tmux Show all sessions tmux ls tmux new tmux list-sessions tmux new-session Attach to last session tmux a :new tmux at tmux attach tmux attach-session Start a new session with the name mysession tmux new -s mysession kill/delete session mysession tmux kill-ses -t mysession :new -s mysession tmux kill-session -t mysession Attach to a session with the name mysession tmux a -t mysession kill/delete all session but mysession tmux kill-session -a -t mysession tmux at -t mysession kill/delete all session but the current tmux kill-session -a tmux attach -t myssesion tmux attach-session -t mysession Rename session Ctrl + b $ Session and Window Preview Ctrl + b w Detach from session Ctrl + b d Move to previous session Ctrl + b ( Detach others on the session (Maximize window by detch other clients) :attac -d Move to next session Ctrl + b ) Windows \u00b6 Description Command Description Command Start a new session with the name mysession tmux new -s mysession -n mywindow Next window Ctrl + b n Create window Ctrl + b c Switc/select window by number Ctrl + b 0...9 Rename current window Ctrl + b , Toggle last active window Ctrl + b l Close current window Ctrl + b & Reorder windows, swap window number 2(src) and 1(dst) :swap-window -s 2 -t 1 List windows Ctrl + b w Move current window to the left by one position :swap-window -t -1 Previous window Ctrl + b p Panes \u00b6 Description Command Description Command Toggle last active pane Ctrl + b ; Show pane numbers Ctrl + b q Split pane with horizontal layout Ctrl + b % Switch/select pane by number Ctrl + b q 0..9 Split pane with vertical layout Ctrl + b \" Toggle pane zoom Ctrl + b z Move te current pane left Ctrl + b { Convert pane into a window Ctrl + b ! Move the current pane right Ctrl + b } Resize current pane height(holding second key is optional) Ctrl +b KEY Ctrl + b Ctrl + KEY Switch to pane to the direction Ctrl + b KEY Resize current pane width(holding scond key is optional) Ctrl + b KEY Ctrl + b Ctrl + KEY Toggle synchronize-panes(send command to all panes) :setw synchronize-panes Close curent pane Ctrl + b X Switch to next pane Ctrl + b o Copy Mode \u00b6 Description Command Description Command use vi keys in buffer :setw -g mode-keys vi Search forward / Enter copy mode Ctrl +b [ Search backward ? Enter copy mode and scroll one page up Ctrl + b PgUp Next keyword occurance n Quit mode q Previous keyword occurance N Go to top line g Start selection Spacebar Go to bottom line G Clear selection ESC Scroll up KEY Copy selection Enter Scroll down KEY Paste contents of buffer_0i Ctrl + b ] Move cursor left h disaplay buffer_0 contents :show-buffer Move cursor down j copy entire visible contents of pane to a buffer :capture-pane Move cursor up k Show all buffers :list-buffers Move cursor right l Show all buffers and paste selected :choose-buffer Move cursor forward one word at a time w Save buffer contents to buf.txt :save-buffer buf.txt Move cursor backward one word at a time b delete buffer_1 :delete-buffer -b 1/span> Misc \u00b6 Description Command Description Command Enter command mode Ctrl + b : Set OPTION for all windows :setw -g OPTION Set OPTION for all sessions :set -g OPTION Enable mouse mode :set mouse on Help \u00b6 Description Command Description Command tmux list-keys Show every session, window, pane, etc... tmux info List key bindings(shortcuts) Ctrl + b ?","title":"Tmux"},{"location":"blog/2023/06/TmuxReference/#tmux","text":"","title":"Tmux"},{"location":"blog/2023/06/TmuxReference/#cheat-sheet-quick-reference","text":"","title":"\ud83d\udd17 Cheat Sheet &amp; Quick Reference"},{"location":"blog/2023/06/TmuxReference/#sessions","text":"Description Command Description Command Start a new settion tmux Show all sessions tmux ls tmux new tmux list-sessions tmux new-session Attach to last session tmux a :new tmux at tmux attach tmux attach-session Start a new session with the name mysession tmux new -s mysession kill/delete session mysession tmux kill-ses -t mysession :new -s mysession tmux kill-session -t mysession Attach to a session with the name mysession tmux a -t mysession kill/delete all session but mysession tmux kill-session -a -t mysession tmux at -t mysession kill/delete all session but the current tmux kill-session -a tmux attach -t myssesion tmux attach-session -t mysession Rename session Ctrl + b $ Session and Window Preview Ctrl + b w Detach from session Ctrl + b d Move to previous session Ctrl + b ( Detach others on the session (Maximize window by detch other clients) :attac -d Move to next session Ctrl + b )","title":"Sessions"},{"location":"blog/2023/06/TmuxReference/#windows","text":"Description Command Description Command Start a new session with the name mysession tmux new -s mysession -n mywindow Next window Ctrl + b n Create window Ctrl + b c Switc/select window by number Ctrl + b 0...9 Rename current window Ctrl + b , Toggle last active window Ctrl + b l Close current window Ctrl + b & Reorder windows, swap window number 2(src) and 1(dst) :swap-window -s 2 -t 1 List windows Ctrl + b w Move current window to the left by one position :swap-window -t -1 Previous window Ctrl + b p","title":"Windows"},{"location":"blog/2023/06/TmuxReference/#panes","text":"Description Command Description Command Toggle last active pane Ctrl + b ; Show pane numbers Ctrl + b q Split pane with horizontal layout Ctrl + b % Switch/select pane by number Ctrl + b q 0..9 Split pane with vertical layout Ctrl + b \" Toggle pane zoom Ctrl + b z Move te current pane left Ctrl + b { Convert pane into a window Ctrl + b ! Move the current pane right Ctrl + b } Resize current pane height(holding second key is optional) Ctrl +b KEY Ctrl + b Ctrl + KEY Switch to pane to the direction Ctrl + b KEY Resize current pane width(holding scond key is optional) Ctrl + b KEY Ctrl + b Ctrl + KEY Toggle synchronize-panes(send command to all panes) :setw synchronize-panes Close curent pane Ctrl + b X Switch to next pane Ctrl + b o","title":"Panes"},{"location":"blog/2023/06/TmuxReference/#copy-mode","text":"Description Command Description Command use vi keys in buffer :setw -g mode-keys vi Search forward / Enter copy mode Ctrl +b [ Search backward ? Enter copy mode and scroll one page up Ctrl + b PgUp Next keyword occurance n Quit mode q Previous keyword occurance N Go to top line g Start selection Spacebar Go to bottom line G Clear selection ESC Scroll up KEY Copy selection Enter Scroll down KEY Paste contents of buffer_0i Ctrl + b ] Move cursor left h disaplay buffer_0 contents :show-buffer Move cursor down j copy entire visible contents of pane to a buffer :capture-pane Move cursor up k Show all buffers :list-buffers Move cursor right l Show all buffers and paste selected :choose-buffer Move cursor forward one word at a time w Save buffer contents to buf.txt :save-buffer buf.txt Move cursor backward one word at a time b delete buffer_1 :delete-buffer -b 1/span>","title":"Copy Mode"},{"location":"blog/2023/06/TmuxReference/#misc","text":"Description Command Description Command Enter command mode Ctrl + b : Set OPTION for all windows :setw -g OPTION Set OPTION for all sessions :set -g OPTION Enable mouse mode :set mouse on","title":"Misc"},{"location":"blog/2023/06/TmuxReference/#help","text":"Description Command Description Command tmux list-keys Show every session, window, pane, etc... tmux info List key bindings(shortcuts) Ctrl + b ?","title":"Help"},{"location":"blog/2023/09/Vagrant/","text":"Vagrant Plugin \u00b6 Background: \u00b6 Due to the network in China is not good to go on plugin download and upgrade. Hence replace to local source is the better method. Workaround: \u00b6 Rubygems Rubygems-China # Installation # Download the relevant package from upon website # Unpack into a dir and cd to the folder # run following command for Installation $ ruby setup.rb If the permission error message warning up, export ENV for tackle down. $ export GEM_HOME = $HOME /.gem/ruby/3.1.0/ Rubygems install the package \u00b6 $ gem sources --add https://gems.ruby-china.com/ --remove https://rubygems.org/ $ gem sources -l https://gems.ruby-china.com $ gem install vagrant-vbguest","title":"Vagrant Plugin"},{"location":"blog/2023/09/Vagrant/#vagrant-plugin","text":"","title":"Vagrant Plugin"},{"location":"blog/2023/09/Vagrant/#background","text":"Due to the network in China is not good to go on plugin download and upgrade. Hence replace to local source is the better method.","title":"Background:"},{"location":"blog/2023/09/Vagrant/#workaround","text":"Rubygems Rubygems-China # Installation # Download the relevant package from upon website # Unpack into a dir and cd to the folder # run following command for Installation $ ruby setup.rb If the permission error message warning up, export ENV for tackle down. $ export GEM_HOME = $HOME /.gem/ruby/3.1.0/","title":"Workaround:"},{"location":"blog/2023/09/Vagrant/#rubygems-install-the-package","text":"$ gem sources --add https://gems.ruby-china.com/ --remove https://rubygems.org/ $ gem sources -l https://gems.ruby-china.com $ gem install vagrant-vbguest","title":"Rubygems install the package"},{"location":"blog/2023/12/Proxmox-disk-cleanup/","text":"Proxmox disk cleanup \u00b6 # use ncdu command to check the directory usage under root nudc / Linux old kernel cleanup \u00b6 OLD_KERNEL = dpkg -l | awk '{print $2}' | grep -e ^pve-.* $c apt-get remove --purge $OLD_KERNEL","title":"Proxmox disk cleanup"},{"location":"blog/2023/12/Proxmox-disk-cleanup/#proxmox-disk-cleanup","text":"# use ncdu command to check the directory usage under root nudc /","title":"Proxmox disk cleanup"},{"location":"blog/2023/12/Proxmox-disk-cleanup/#linux-old-kernel-cleanup","text":"OLD_KERNEL = dpkg -l | awk '{print $2}' | grep -e ^pve-.* $c apt-get remove --purge $OLD_KERNEL","title":"Linux old kernel cleanup"}]}